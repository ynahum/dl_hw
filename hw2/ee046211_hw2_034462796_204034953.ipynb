{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzV9wsJ5pGhf"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/bubbles/50/000000/mind-map.png\" style=\"height:50px;display:inline\"> EE 046211 - Technion - Deep Learning\n",
        "---\n",
        "\n",
        "## HW2 - Multilayer NNs and Convolutional NNs\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq2c8X93pGhh"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/clouds/96/000000/keyboard.png\" style=\"height:50px;display:inline\"> Keyboard Shortcuts\n",
        "---\n",
        "* Run current cell: **Ctrl + Enter**\n",
        "* Run current cell and move to the next: **Shift + Enter**\n",
        "* Show lines in a code cell: **Esc + L**\n",
        "* View function documentation: **Shift + Tab** inside the parenthesis or `help(name_of_module)`\n",
        "* New cell below: **Esc + B**\n",
        "* Delete cell: **Esc + D, D** (two D's)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZZybn3NpGhh"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Students Information\n",
        "---\n",
        "* Fill in\n",
        "\n",
        "|Name         |Campus Email                     | ID       |\n",
        "|-------------|---------------------------------|----------|\n",
        "|Lior Friedman| liorf@campus.technion.ac.il     | 204034953|\n",
        "|Yair Nahum   | nahum.yair@campus.technion.ac.il| 034462796|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDK5zqhdpGhi"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
        "---\n",
        "* Maximal garde: 100.\n",
        "* Submission only in **pairs**. \n",
        "    * Please make sure you have registered your group in Moodle (there is a group creation component on the Moodle where you need to create your group and assign members).\n",
        "* **No handwritten submissions.** You can choose whether to answer in a Markdown cell in this notebook or attach a PDF with your answers.\n",
        "* <a style='color:red'> SAVE THE NOTEBOOKS WITH THE OUTPUT, CODE CELLS THAT WERE NOT RUN WILL NOT GET ANY POINTS! </a>\n",
        "* What you have to submit:\n",
        "    * If you have answered the questions in the notebook, you should submit this file only, with the name: `ee046211_hw2_id1_id2.ipynb`.\n",
        "    * If you answered the questions in a different file you should submit a `.zip` file with the name `ee046211_hw2_id1_id2.zip` with content:\n",
        "        * `ee046211_hw2_id1_id2.ipynb` - the code tasks\n",
        "        * `ee046211_hw2_id1_id2.pdf` - answers to questions.\n",
        "    * No other file-types (`.py`, `.docx`...) will be accepted.\n",
        "* Submission on the course website (Moodle).\n",
        "* **Latex in Colab** - in some cases, Latex equations may no be rendered. To avoid this, make sure to not use *bullets* in your answers (\"* some text here with Latex equations\" -> \"some text here with Latex equations\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmSj_UufpGhi"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/dusk/64/000000/online.png\" style=\"height:50px;display:inline\"> Working Online and Locally\n",
        "---\n",
        "* You can choose your working environment:\n",
        "    1. `Jupyter Notebook`, **locally** with <a href=\"https://www.anaconda.com/distribution/\">Anaconda</a> or **online** on <a href=\"https://colab.research.google.com/\">Google Colab</a>\n",
        "        * Colab also supports running code on GPU, so if you don't have one, Colab is the way to go. To enable GPU on Colab, in the menu: `Runtime`$\\rightarrow$ `Change Runtime Type` $\\rightarrow$`GPU`.\n",
        "    2. Python IDE such as <a href=\"https://www.jetbrains.com/pycharm/\">PyCharm</a> or <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a>.\n",
        "        * Both allow editing and running Jupyter Notebooks.\n",
        "\n",
        "* Please refer to `Setting Up the Working Environment.pdf` on the Moodle or our GitHub (https://github.com/taldatech/ee046211-deep-learning) to help you get everything installed.\n",
        "* If you need any technical assistance, please go to our Piazza forum (`hw2` folder) and describe your problem (preferably with images)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlp1Fp4ppGhj"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
        "---\n",
        "\n",
        "* [Part 1 - Theory](#-Part-1---Theory)\n",
        "    * [Q1 - Generalization in A Teacher-Student Setup](#-Question-1--Generalization-in-A-Teacher-Student-Setup)\n",
        "    * [Q2 - Backpropagation By Hand](#-Question-2---Backpropagation-By-Hand)\n",
        "    * [Q3 - Deep Double Descent](#-Question-3---Deep-Double-Descent)\n",
        "    * [Q4 - Initialization](#-Question-4---Initialization)\n",
        "    * [Q5 - MLP and Invaraince](#-Question-5---MLP-and-Invaraince)\n",
        "    * [Q6 - VGG Architecture](#-Question-6--VGG-Architecture)\n",
        "* [Part 2 - Code Assignments](#-Part-2---Code-Assignments)\n",
        "    * [Task 1 - The Importance of Activation and Initialization](#-Task-1---The-Importance-of-Activation-and-Initialization)\n",
        "    * [Task 2 - FashionMNIST Deep Classifer](#-Task-2---FashionMNIST-Deep-Classifer)\n",
        "    * [Task 3 - Design a CNN](#-Task-3---Design-a-CNN)\n",
        "* [Credits](#-Credits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKtSiQX_pGhj"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/cute-clipart/64/000000/ball-point-pen.png\" style=\"height:50px;display:inline\"> Part 1 - Theory\n",
        "---\n",
        "* You can choose whether to answser these straight in the notebook (Markdown + Latex) or use another editor (Word, LyX, Latex, Overleaf...) and submit an additional PDF file, **but no handwritten submissions**.\n",
        "* You can attach additional figures (drawings, graphs,...) in a separate PDF file, just make sure to refer to them in your answers.\n",
        "\n",
        "* $\\large\\LaTeX$ <a href=\"https://kapeli.com/cheat_sheets/LaTeX_Math_Symbols.docset/Contents/Resources/Documents/index\">Cheat-Sheet</a> (to write equations)\n",
        "    * <a href=\"http://tug.ctan.org/info/latex-refsheet/LaTeX_RefSheet.pdf\">Another Cheat-Sheet</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsqSFZG1pGhj"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 1 -Generalization in A Teacher-Student Setup\n",
        "---\n",
        "\n",
        "Recall from lecture 4 the Risk $\\mathcal{R}(w)$: $$ \\mathcal{R}(w) \\triangleq \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) } \\left[ ||w^Tx^{(0)} - w_t^Tx^{(0)}||^2 \\right] $$\n",
        "\n",
        "Prove:\n",
        "\n",
        "$$ \\mathcal{R}(w) = ||w-w_t||^2 $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%%md\n"
        },
        "id": "CjAgwlEAOF1I"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/fluency/48/000000/exclamation-mark.png\" style=\"height:50px;display:inline\">  Answer 1 - Generalization in A Teacher-Student Setup \n",
        "---\n",
        "Proof:  \n",
        "$$ \\mathcal{R}(w) \\triangleq \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) } \\left[ ||w^Tx^{(0)} - w_t^Tx^{(0)}||^2 \\right]$$\n",
        "$$= \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) } \\left[ ||{x^{(0)}}^T(w - w_t)||^2 \\right]$$\n",
        "$$\\underbrace{=}_{(1)} \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) } \\left[ (w - w_t)^T x^{(0)} {x^{(0)}}^T(w - w_t) \\right]$$\n",
        "$$\\underbrace{=}_{(2)} (w - w_t)^T \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) } \\left[  x^{(0)} {x^{(0)}}^T \\right] (w - w_t)$$\n",
        "$$\\underbrace{=}_{(3)} (w - w_t)^T I (w - w_t) \\underbrace{=}_{(4)} ||w - w_t||^2$$\n",
        "Where (1) and (4) are due to the L2 norm definition, (2) is due to $(w-w_t)$ is constant relative to the expectation and (3) is due to the fact that the covariance matrix is $I$.  \n",
        "$\\blacksquare$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StkdA5hUOF1K"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 2 - Backpropagation By Hand\n",
        "---\n",
        "Consider the following network:\n",
        "<img src=\"https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/backprop_by_hand_ex1.png\" style=\"height:300px\">\n",
        "\n",
        "We will work with one sample for this example, but it can be extended to mini-batches.\n",
        "\n",
        "* Input: $x = \\begin{bmatrix} 1 \\\\ 4 \\\\ 5 \\end{bmatrix} \\in \\mathbb{R}^3$\n",
        "* Output (target): $ t = \\begin{bmatrix} 0.1 \\\\ 0.05 \\end{bmatrix} \\in \\mathbb{R}^2 $\n",
        "* Number of Hidden Layers: 1\n",
        "* Activation: Sigmoid for both hidden and output layers\n",
        "* Loss Functions: MSE\n",
        "\n",
        "We initialize the weights and biases to random values as follows:\n",
        "<img src=\"https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/backprop_by_hand_ex2.png\" style=\"height:300px\">\n",
        "\n",
        "1. Perform one forward pass and calculate the MSE.\n",
        "2. Perform backpropagation (one backward pass, i.e., calculate the gradients).\n",
        "3. With a learning rate of $\\alpha = 0.01$, what are the new values of the weights after performing the forward pass and backward pass (assume we use SGD)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJZk_9OxOF1M"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/fluency/48/000000/exclamation-mark.png\" style=\"height:50px;display:inline\"> Answer 2 -Backprop by hand\n",
        "\n",
        "1. All calculations rounded to 2 significant digits\n",
        "\n",
        "$$ h_1(x)=S(0.1\\cdot 1+0.3\\cdot 4+0.5\\cdot 5+0.5\\cdot 1)=S(4.3)=0.99$$\n",
        "$$ h_2(x)=S(0.2\\cdot 1+0.4\\cdot 4+0.6\\cdot 5+0.5\\cdot 1)=S(5.3)=1$$\n",
        "$$ o_1(h(x))=S(0.7\\cdot h_1(x)+0.9\\cdot h_2(x)+0.5\\cdot 1)=S(2.093)=0.89$$\n",
        "$$ o_2(h(x))=S(0.8\\cdot h_1(x)+0.1\\cdot h_2(x)+0.5\\cdot 1)=S(1.392)=0.8$$\n",
        "$$ MSE = (0.1-0.89)^2+(0.05-0.8)^2=1.19$$\n",
        "\n",
        "2. \n",
        "\n",
        "$$\\frac{d\\mathcal{L}}{do_1}=2(o_1-t_1)=1.58, \\frac{d\\mathcal{L}}{do_2}=2(o_2-t_2)=1.5$$\n",
        "\n",
        "$$\\frac{d\\mathcal{L}}{dw_7}=\\frac{d\\mathcal{L}}{do_1}\\frac{do_1}{dw_7}=1.58\\cdot S'(o_1(h(x)))h_1(x)=1.58\\cdot 0.206\\cdot 0.99=0.32$$\n",
        "$$\\frac{d\\mathcal{L}}{dw_8}=\\frac{d\\mathcal{L}}{do_2}\\frac{do_2}{dw_8}=1.5\\cdot S'(o_2(h(x)))h_1(x)=1.5\\cdot 0.214\\cdot 0.99=0.32$$\n",
        "$$\\frac{d\\mathcal{L}}{dw_9}=\\frac{d\\mathcal{L}}{do_1}\\frac{do_1}{dw_9}=1.58\\cdot S'(o_1(h(x)))h_2(x)=1.58\\cdot 0.206\\cdot 1=0.33$$\n",
        "$$\\frac{d\\mathcal{L}}{dw_{10}}=\\frac{d\\mathcal{L}}{do_2}\\frac{do_2}{dw_{10}}=1.5\\cdot S'(o_2(h(x)))h_2(x)=1.5\\cdot 0.214\\cdot 1=0.32$$\n",
        "\n",
        "$$\\frac{d\\mathcal{L}}{dh_1}=\\frac{d\\mathcal{L}}{do_1}\\frac{do_1}{dh_1}+\\frac{d\\mathcal{L}}{do_2}\\frac{do_2}{dh_1}=1.58\\cdot S'(o_1(h(x)))w_7+1.5\\cdot S'(o_2(h(x)))w_8\\\\=1.58\\cdot 0.206\\cdot 0.7+1.5\\cdot 0.214\\cdot 0.8 =0.228+0.257=0.48$$\n",
        "\n",
        "$$\\frac{d\\mathcal{L}}{dh_2}=\\frac{d\\mathcal{L}}{do_1}\\frac{do_1}{dh_2}+\\frac{d\\mathcal{L}}{do_2}\\frac{do_2}{dh_2}=1.58\\cdot S'(o_1(h(x)))w_9+1.5\\cdot S'(o_2(h(x)))w_{10}\\\\=1.58\\cdot 0.206\\cdot 0.9+1.5\\cdot 0.214\\cdot 0.1 =0.293+0.032=0.33$$\n",
        "\n",
        "$$\\frac{d\\mathcal{L}}{dw_1}=\\frac{d\\mathcal{L}}{dh_1}\\frac{dh_1}{dw_1}=0.48\\cdot S'(h_1(x))x_1=0.48\\cdot 0.197\\cdot 1=0.09$$\n",
        "$$\\frac{d\\mathcal{L}}{dw_2}=\\frac{d\\mathcal{L}}{dh_2}\\frac{dh_2}{dw_1}=0.33\\cdot S'(h_2(x))x_1=0.33\\cdot 0.197\\cdot 1=0.07$$\n",
        "$$\\frac{d\\mathcal{L}}{dw_3}=\\frac{d\\mathcal{L}}{dh_1}\\frac{dh_1}{dw_3}=0.48\\cdot S'(h_1(x))x_2=0.48\\cdot 0.197\\cdot 4=0.38$$\n",
        "$$\\frac{d\\mathcal{L}}{dw_4}=\\frac{d\\mathcal{L}}{dh_2}\\frac{dh_2}{dw_4}=0.33\\cdot S'(h_2(x))x_2=0.33\\cdot 0.197\\cdot 4=0.26$$\n",
        "$$\\frac{d\\mathcal{L}}{dw_5}=\\frac{d\\mathcal{L}}{dh_1}\\frac{dh_1}{dw_5}=0.48\\cdot S'(h_1(x))x_3=0.48\\cdot 0.197\\cdot 5=0.47$$\n",
        "$$\\frac{d\\mathcal{L}}{dw_6}=\\frac{d\\mathcal{L}}{dh_2}\\frac{dh_2}{dw_6}=0.33\\cdot S'(h_2(x))x_3=0.33\\cdot 0.197\\cdot 5=0.33$$\n",
        "\n",
        "\n",
        "$$\\nabla_w \\mathcal{L}=\\begin{bmatrix} 0.09 \\\\ 0.07 \\\\ 0.38 \\\\ 0.26 \\\\0.47 \\\\0.33 \\\\0.32 \\\\0.32\\\\0.33 \\\\ 0.32\\end{bmatrix}$$\n",
        "\n",
        "3. \n",
        "$$w_{t+1}=w_t-\\alpha \\nabla_w \\mathcal{L}(w_t)=\\begin{bmatrix} 0.1 \\\\ 0.2 \\\\ 0.3 \\\\ 0.4 \\\\0.5 \\\\0.6 \\\\0.7 \\\\0.8 \\\\0.9 \\\\ 0.1\\end{bmatrix}-0.01\\begin{bmatrix} 0.09 \\\\ 0.07 \\\\ 0.38 \\\\ 0.26 \\\\0.47 \\\\0.33 \\\\0.32 \\\\0.32\\\\0.33 \\\\ 0.32\\end{bmatrix}=\\begin{bmatrix} 0.099 \\\\ 0.199 \\\\ 0.296 \\\\ 0.397 \\\\0.495 \\\\0.597 \\\\0.697 \\\\0.797 \\\\0.897 \\\\ 0.097\\end{bmatrix}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjO6xMRAOF1N"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 3 - Deep Double Descent\n",
        "---\n",
        "\n",
        "For the following plots:\n",
        "1. Where is the critical point (the point of transition between the \"Classical Regime\" and \"Modern Regime\") of the deep double descent?\n",
        "2. What type of double descent is shown? Explain.\n",
        "    \n",
        "\n",
        "a. <img src='https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/double_descent_transformer.PNG' style=\"height:300px\">\n",
        "\n",
        "b. <img src='https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/double_descent_resnet.PNG' style=\"height:300px\">\n",
        "\n",
        "c. <img src='https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/double_descent_intermediate.PNG' style=\"height:300px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJvVLhojOF1O"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/fluency/48/000000/exclamation-mark.png\" style=\"height:50px;display:inline\"> Answer 3 -Deep Double Decent\n",
        "\n",
        "a. \n",
        "The critical point is around dimension=200 for the blue (German-English) model and around dimension=250 for the green (English-French) model. This is an example of model-wise double decent as we see that larger models have worse test error until a critical region is reached and the trend reverses. \n",
        "\n",
        "b. The critical point is around width=10 for the least noisy case and around width=13 for the highest noise.  \n",
        "This is an example of model-wise double decent, and we also see that adding label noise causes the critical region to move further away.  \n",
        "As the noise increases, the SNR and thus the optimal step size decreases. This effectively increases the dimensionallity of the data and moves the interpolation threshold location ($\\gamma=\\frac{d}{n}=1$ requires bigger dimensioanlity of the model parameters).\n",
        "\n",
        "c. At the large model at around 100 epochs we see an epoch-wise double descent as increasing the training time reverses the overfitting in the test error.  \n",
        "The small and intermediate models does not exhibit deep double decent. In intermediate sized models, we see the classical U shape of ML that requires early stopping.  \n",
        "In small sized models, we stay in the biased location, where increasing the training time improves the test error.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU-XkJf5OF1P"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 4 - Initialization\n",
        "---\n",
        "\n",
        "Recall that in lecture 5 we were discussing how to calculate the initialization variance, and reached the conclusion that $$ \\sigma_l =\\frac{1}{\\sqrt{d_{l-1}\\mathbb{E}_{z\\sim \\mathcal{N}(0, 1)} \\left[\\varphi^2(z)\\right]}} $$\n",
        "Show that for ReLU activation ($\\varphi(z) = max(0,z)$), the optimal variance satisfies: $$ \\sigma_l = \\sqrt{\\frac{2}{d_{l-1}}}$$\n",
        "\n",
        "All the notations are the same as in the lecture slides."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoOZF5z4OF1Q"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/fluency/48/000000/exclamation-mark.png\" style=\"height:50px;display:inline\"> Answer 4 -Initialization\n",
        "\n",
        "We assume that $\\forall i, \\mathbb{E}\\left [u_{l-1}[i]\\right ]=0, \\text{Var}(u_{l-1}[i])=1$\n",
        "From the lecture we know that under our assumptions for $W_{l-1}$,\n",
        "\n",
        "$$\\text{Var}(u_l[i])=\\sigma_l^2\\sum_{j=1}^{d_{l-1}}\\mathbb{E}\\left [\\varphi^2 (u_{l-1}[i])\\right ]$$\n",
        "\n",
        "For ReLU, $\\varphi^2(z)=\\begin{cases} z^2 & z>0 \\\\ 0 & \\text{otherwise} \\end{cases}$  \n",
        "Moreover, for Relu we don't need to assume/use the central limit theorem for any symetric PDF $f(u_{l-1}[i]) = f(-u_{l-1}[i])$ for example $u_{l-1} \\sim \\mathcal{N}(0,\\sigma^{2}I)$ or $u_{l-1}[i] \\sim \\mathcal{Uniform}(-1,1)$.\n",
        "\n",
        "$$\\mathbb{E}\\left [\\varphi^2 (u_{l-1}[i])\\right ] = \\int \\varphi^2 (u_{l-1}[i])f(u_{l-1}[i])du_{l-1}[i] = \\int_{0}^{\\infty} u_{l-1}[i]^2 f(u_{l-1}[i]) du_{l-1}[i] \\underbrace{=}_{(1)} \\frac{1}{2}\\int_{-\\infty}^{\\infty} u_{l-1}[i]^2 f(u_{l-1}[i]) du_{l-1}[i]=\\frac{1}{2}\\mathbb{E}\\left [u_{l-1}[i]^2\\right ]\\underbrace{=}_{(2)}\\frac{1}{2}\\text{Var}(u_{l-1}[i])\\underbrace{=}_{(3)}\\frac{1}{2}  $$\n",
        "\n",
        "The transitions are explained as follows:  \n",
        "$(1)$ is due to the fact that the integrand is an even/symetric function.  \n",
        "$(2)$ is due to the fact that $\\mathbb{E}\\left [u_{l-1}[i]\\right ]=0 \\Rightarrow  \\text{Var}(u_{l-1}[i]) = \\mathbb{E}\\left [u_{l-1}[i]^2\\right ] - \\mathbb{E}\\left [u_{l-1}[i]\\right ]^2 = \\mathbb{E}\\left [u_{l-1}[i]^2\\right ]$   \n",
        "And $(3)$ since $\\text{Var}(u_{l-1}[i])=1 (by the induction step over layers and inputs)$   \n",
        "\n",
        "From this we get $$\\text{Var}(u_l[i])=\\sigma_l^2\\sum_{j=1}^{d_{l-1}}\\frac{1}{2}=1$$\n",
        "\n",
        "$$\\frac{1}{2}\\sigma_l^2 d_{l-1}=1$$\n",
        "\n",
        "$$\\Rightarrow \\sigma_l = \\sqrt{\\frac{2}{d_{l-1}}}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eurEw1szOF1Q"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 5 - MLP and Invaraince\n",
        "---\n",
        "\n",
        "You have to design an MLP with the following input: DNA sequences of length $d$. The DNA is a sequence of bases, where each base can be one of 4 options: $(C, T, G, A)$. Thus, the input can be described as the following matrix: $$ X \\in \\mathcal{R}^{4 \\times d}, $$ where $X[j,i]$ denotes the measured value of base concentration of the $j^{th}$ base at location $i$. \n",
        "\n",
        "The network should output a **binary** classification $y \\in \\{-1, 1\\}$ for a specific property we wish to find. The network will be trained on samples $\\{X^{(n)}, y^{(n)} \\}_{n=1}^{N}$, with a **logistic loss function**.\n",
        "\n",
        "First, we will examine a network with 1 hidden layerof size $4 \\times d$ and a **LeakyReLU** activation $\\phi$: $$ f_w(X) = \\sum_{r=1}^{4}\\sum_{k=1}^d W_2[r,k]\\phi\\left(\\sum_{j=1}^{4}\\sum_{i=1}^d W_1[r, k,j, i]X[j, i] \\right),$$ where $w=\\{W_1, W_2\\}$ are the layers of the weight **tensors**. After training is done, the classification will be done with $\\text{sign}(f(X))$.\n",
        "\n",
        "1. Which invariances exist in the network's parameters?\n",
        "2. Now, we notice the fact that: the *direction* in which the DNA is scanned is arbitrary. Thus, if for two inputs $X, \\tilde{X}$: $$ \\forall i,j: \\: X[j,i] = \\tilde{X}[j, d-i], $$ then the two inputs are **equivalent** in their meaning. What constraints should we put on the network's parameters to improve the network's classification performance?\n",
        "3. After that, we now recall that the DNA bases come in pairs, and thus if for two inputs $X, \\tilde{X}$: $$ \\forall i,j : \\: X[j,i] = \\tilde{X}[(j+2)\\text{mod}4,i], $$ then the two inputs are **equivalent** in their meaning. What constraints should we put on the network's parameters to improve the network's classification performance?\n",
        "4. We now notice that the measurement process in noisy, each sample $X^{(n)}$ is in arbitrary scale, and thus if for two $X, \\tilde{X}$: $$ \\forall i,j: \\: X[j,i] = c\\tilde{X}[j,i], $$ for some constant $c>0$, then the two inputs are **equivalent** in their meaning.\n",
        "    * (a) For the given network, that **is already trained**, what is the effect of the scale $c$ on the classification result?\n",
        "    * (b) Can the arbitrary scale hurt the training process? Hint: think what happens to the gradient of each sample.\n",
        "    * (c) How can use this information to improve the classifier performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJb5wlHSOF1R"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/fluency/48/000000/exclamation-mark.png\" style=\"height:50px;display:inline\"> Answer 5 - MLP and Invaraince\n",
        "\n",
        "1. The LeakyReLU activation is rescale symmetric, so there is rescale invariance between $z_1=W_1X$ and $\\phi(z_1)$. Meaning, $\\forall c >0$ if we multiply $W_1$ by $c$ and multiply  $W_2$ by $\\frac{1}{c}$ we get the same function.  \n",
        "The MNN is also invariant to permutations on the nuerons with some permutation matrix $P$ and its inverse $P^{-1}=P^T$.  \n",
        "The final layer is also sign symmetric so we can flip the weights $W_2$ and the sign activation and also get the same result (but this is not a proper invariant).\n",
        "\n",
        "2. To achieve this equivalence, we can require that  $\\forall r,k,j$ the $W_1[r,k,j,:]$ is a symetric tensor relative to the last dimension such that $W_1[r,k,j,i]=W_1[r,k,j,d-i], \\forall i\\in \\left [1,\\frac{d}{2}\\right ]$\n",
        "\n",
        "3. Similarly to section 2, we can require $W_1[r,k,0,i]=W_1[r,k,2,i],W_1[r,k,1,i]=W_1[r,k,3,i]$ for all $r,k,i$.\n",
        "\n",
        "4.  \n",
        "    (a) There is no effect on the classification result itself. The output will be scaled based on the input, so $f_w(X)=c\\cdot f_w(\\tilde{X}))$, but the sign is the same.  \n",
        "    (b) Yes, the training loss is logistic loss, which is not scale invariant, and the gradient of each example is proportional to $$\\frac{\\partial L}{\\partial \\hat{Y}} = \\frac{\\partial \\log(1+e^{-\\hat{Y} Y})}{\\partial \\hat{Y}} = -\\frac{Ye^{-\\hat{Y}Y}}{1+e^{-\\hat{Y}Y}} = -\\frac{Y}{1+e^{\\hat{Y}Y}} = -\\frac{Y}{1+e^{f_w(X)Y}}$$\n",
        "    so having an arbitrary scale may cause the gradient to vanish due to numerical approximation issues.\n",
        "    As an example, if the label is $Y=1$ we can scale $f(X)$ by a big contant $c>0$ and have a vanishing gradient when $f(X)>0$.  \n",
        "    (c) If this property is desired, then we can purposly scale the gradient for all incorrectly labeled examples to increase the gradient, resulting in a situation similar to max-margin classification (we artifically increase the cost of mistakes, thereby increasing the gradient only for incorrectly labeled examples).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68ebTszMOF1R"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 6 -VGG Architecture\n",
        "---\n",
        "\n",
        "1. The VGG-11 CNN architecture consists of 11 convolution (CONV)/fully-connected (FC) layers (every CONV layer has the same padding and stride, every MAXPOOL layer is 2×2 and has padding of 0 and stride 2). Fill in the table. You need to **consider the bias**.\n",
        "\n",
        "\n",
        "* CONV$M$-$N$: a convolutional layer with $N$ neurons, each of size $M \\times M \\times D$, where $D$ is the number of filters. $stride=1, padding=1$ \n",
        "* POOL2: $2 \\times 2$ Max Pooling with $stride=2$\n",
        "    * In case the input of the layer is odd, you should round down. For example, if the output of the layer should be $3.5 \\times 3.5 \\times 3$, you should round to $3 \\times 3 \\times 3$ (i.e., ignore the last column of the input image when performing MaxPooling).\n",
        "* FC-N: a fully connected layer with $N$ neurons.\n",
        "\n",
        "\n",
        "| Layer  | Output Dimension  | Number of Parameters (Weights) |\n",
        "|---|---|---|\n",
        "| INPUT  |  224x224x3 | 0  |\n",
        "|  CONV3-64 | -  | -  | \n",
        "| ReLU |  - | -  |\n",
        "| POOL2|  - | -  |\n",
        "|CONV3-128 | - | -|\n",
        "|ReLU | - | -|\n",
        "| POOL2|  - | -  |\n",
        "|CONV3-256 | - | -|\n",
        "|ReLU | - | -|\n",
        "|CONV3-256 | - | -|\n",
        "|ReLU | - | -|\n",
        "| POOL2|  - | -  |\n",
        "|CONV3-512 | - | -|\n",
        "|ReLU | - | -|\n",
        "|CONV3-512 | - | -|\n",
        "|ReLU | - | -|\n",
        "| POOL2|  - | -  |\n",
        "|CONV3-512 | - | -|\n",
        "|ReLU | - | -|\n",
        "|CONV3-512 | - | -|\n",
        "|ReLU | - | -|\n",
        "| POOL2|  - | -  |\n",
        "| FC-4096|  - | -  |\n",
        "| FC-4096|  - | -  |\n",
        "| FC-1000|  - | -  |\n",
        "| SOFTMAX|  - | -  |\n",
        "\n",
        "2. What is the total number of parameters? (use a calculator for this one)\n",
        "3. What percentage of the weights are found in the fully-connected layers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpLC9zRMOF1T"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/fluency/48/000000/exclamation-mark.png\" style=\"height:50px;display:inline\"> Answer 6 -VGG architecture\n",
        "\n",
        "1. Filled in the table  \n",
        "\n",
        "| Layer  | Output Dimension  | Number of Parameters (Weights) |\n",
        "|---|---|---|\n",
        "| INPUT  |  224x224x3 | 0  |\n",
        "|  CONV3-64 | 224x224x64  | 64x(3x3x3+1)=1792  | \n",
        "| ReLU |  224x224x64 | 0  |\n",
        "| POOL2|  112x112x64 | 0  |\n",
        "|CONV3-128 | 112x112x128 | 128x(3x3x64+1)=73,856  |\n",
        "|ReLU | 112x112x128 | 0|\n",
        "| POOL2|  56x56x128 | 0|\n",
        "|CONV3-256 | 56x56x256 | 256x(3x3x128+1)=295,168|\n",
        "|ReLU | 56x56x256 | 0|\n",
        "|CONV3-256 | 56x56x256 | 256x(3x3x256+1)=590,080|\n",
        "|ReLU | 56x56x256 | 0|\n",
        "| POOL2|  28x28x256 | 0  |\n",
        "|CONV3-512 | 28x28x512 | 512x(3x3x256+1)=1,180,160|\n",
        "|ReLU | 28x28x512 | 0|\n",
        "|CONV3-512 | 28x28x512 | 512x(3x3x512+1)=2,359,808|\n",
        "|ReLU | 28x28x512 | 0|\n",
        "| POOL2|  14x14x512 | 0  |\n",
        "|CONV3-512 | 14x14x512 | 512x(3x3x512+1)=2,359,808|\n",
        "|ReLU | 14x14x512 | 0|\n",
        "|CONV3-512 | 14x14x512 | 512x(3x3x512+1)=2,359,808|\n",
        "|ReLU | 14x14x512 | 0|\n",
        "| POOL2|  7x7x512 | 0  |\n",
        "| FC-4096|  4096 | (7x7x512+1)x4096=102,764,544  |\n",
        "| FC-4096|  4096 | (4096+1)x4096=16,781,312  |\n",
        "| FC-1000|  1000 | (4096+1)x1000=4,097,000  |\n",
        "| SOFTMAX|  1000 | 0  |\n",
        "\n",
        "2. convolution layers weights : 9,220,480  \n",
        "   FC layers weights : 123,642,856  \n",
        "   Total: 132,863,336  \n",
        "3. 100 x 123,642,856 / 132,863,336 = 93.06% of the total weights are in the FC layers' weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D-14iM7pGhm"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/officel/80/000000/code.png\" style=\"height:50px;display:inline\"> Part 2 - Code Assignments\n",
        "---\n",
        "* You must write your code in this notebook and save it with the output of all of the code cells.\n",
        "* Additional text can be added in Markdown cells.\n",
        "* You can use any other IDE you like (PyCharm, VSCode...) to write/debug your code, but for the submission you must copy it to this notebook, run the code and save the notebook with the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMvyKarhOF1Y"
      },
      "source": [
        "#### Tips\n",
        "---\n",
        "1. Uniformly distributed tensors - `torch.Tensor(dim1, dim2, ...,dimN).uniform_(-1, 1)`\n",
        "2. Separation to **validation set** in PyTorch - <a href=\"https://gist.github.com/MattKleinsmith/5226a94bad5dd12ed0b871aed98cb123\">See example here</a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Jj_VZlEOF1Y",
        "outputId": "3df5f76b-9d46-4ba5-c478-708f5d6fe080"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f4de079cab0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# imports for the practice (you can add more if you need)\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "# %matplotlib notebook\n",
        "%matplotlib inline\n",
        "\n",
        "seed = 211\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g6fMD9VOF1c"
      },
      "source": [
        "\n",
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 1 - The Importance of Activation and Initialization\n",
        "---\n",
        "In this task, we are going to use $x \\in \\mathcal{R}^{512}$ and simple neural network that outputs $f(x) \\in \\mathcal{R}^{512}$. The network will have 100 layers with 512 units in each layer.\n",
        "\n",
        "1. We initialize the weights from a unit normal distribution. Run the following code cell and explain what happens. Add a short piece of code that locates when it happens (hint: use `torch.isnan()`). **Print** the layer number.\n",
        "2. We can demonstrate that at a given layer, the matrix product of inputs $x$ and weight matrix $a$ that is initialized from a standard normal distribution will, on average, have a standard deviation very close to the square root of the number of input connections. For our example, with 512 dimensions, show that for 10,000 multiplications of $a$ and $x$, the empirical standard deviation is similar to the square root of the number of input connections. Use the unbiased version: $$ \\hat{std} = \\sqrt{\\frac{\\sum_{i=1}^{10000}\\frac{1}{N}\\sum_{j=1}^N y^2}{10000}}, $$ where $y=ax$ and $N$ is the number of input connections. **Print** the mean, std and the square root of the number of input connections.\n",
        "3. For the code from 1, normalize the weight initialization by the square root of the input connections. How does that change the outcome? **Print** the mean and std after the modification.\n",
        "4. Add a `tanh()` activation after each layer for the code from 1. **Print** the mean and std after the modification. Explain the result.\n",
        "5. Xavier initialization sets a layer’s weights to values chosen from a random uniform distribution that’s bounded between $$\\pm \\sqrt{\\frac{6}{n_i + n_{i+1}}}$$ where $n_i$ is the number of incoming network connections, or “fan-in,” to the layer, and $ n_{i+1}$ is the number of outgoing network connections from that layer, also known as the “fan-out”. Glorot and Bengio believed that Xavier weight initialization would maintain the variance of activations and back-propagated gradients all the way up or down the layers of a network and demonstrated that networks initialized with Xavier achieved substantially quicker convergence and higher accuracy. Implement **Xavier Uniform** as `xavier_init(fan_in, fan_out)`, a function that returns a tensor initialized according to **Xavier Uniform**. Use it on the simple network from 1 with `tanh` activation. **Print** the mean and std after the modification.\n",
        "6. If you try to replace the `tanh` activation with `relu` activation in section 5, you will see very different results. Xavier strives to acheive activation outputs of each layer to have a mean of 0 and a standard deviation around 1, on average. When using a ReLU activation, a single layer will, on average have standard deviation that’s very close to the square root of the number of input connections, **divided by the square root of two** ($\\sqrt{\\frac{512}{2}}$ in our example). **Kaiming He et. al.** proposed an initialization scheme that’s tailored for deep neural nets that use these kinds of asymmetric, non-linear activations. Implement **Kaiming Normal** as `kaiming_init(fan_in, fan_out)`, a function that returns a tensor initialized according to **Kaiming Normal** (use `fan_in` mode). Use it on the simple network from 1 with `relu` activation. **Print** the mean and std after the modification. What happens when you use Xavier with RelU activation?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZL2A1iIQOF1e",
        "outputId": "9f79dec5-f6a2-4738-9003-9a6300757ebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(nan) tensor(nan)\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(512)\n",
        "for i in range(100):\n",
        "    a = torch.randn(512, 512)\n",
        "    x = a @ x\n",
        "print(x.mean(), x.std())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G29XbXyoOF1e"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 1 Code and Answers - The Importance of Activation and InitializationYour answers here\n",
        "\n",
        "1. At some point we hit numerical problems as the vector x components explode beyond the CPU max floating point percision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3lQL4qjOF1f",
        "outputId": "7b091d70-814c-4913-f63c-6a2a4842c54a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x tensot is all NaN at layer 29\n",
            "tensor(nan) tensor(nan)\n"
          ]
        }
      ],
      "source": [
        "#1.1\n",
        "N=512\n",
        "x = torch.randn(N)\n",
        "for i in range(100):\n",
        "    a = torch.randn(N, N)\n",
        "    x = a @ x\n",
        "    #print(x.numpy())\n",
        "    if torch.isnan(x).all():\n",
        "        print(f\"x tensot is all NaN at layer {i}\")\n",
        "        #print(x.numpy())\n",
        "        break\n",
        "print(x.mean(), x.std())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IqmmQZrOF1f"
      },
      "source": [
        "2. We get that with 10000 calculations of empirical std, the mean (estimator) is about the same as the square root on the input connections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzh8yVu8OF1f",
        "outputId": "d3b111e2-68e6-4008-e13a-3e4a881f314f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean of stds:22.610655394172667\n",
            "std of stds:1.0025907195653077\n",
            "number of input connections square root:22.627416997969522\n"
          ]
        }
      ],
      "source": [
        "#1.2\n",
        "N=512\n",
        "num_of_runs=10000\n",
        "arr_of_stds=np.zeros((num_of_runs,))\n",
        "for i in range(num_of_runs):\n",
        "    x = torch.randn(N)\n",
        "    a = torch.randn(N, N)\n",
        "    y = a @ x\n",
        "    arr_of_stds[i] = torch.sqrt(torch.sum(y**2)/N).numpy()\n",
        "mean_of_stds = np.mean(arr_of_stds)\n",
        "std_of_stds = np.std(arr_of_stds)\n",
        "squar_root_of_N = np.sqrt(N)\n",
        "print(f\"mean of stds:{mean_of_stds}\")\n",
        "print(f\"std of stds:{std_of_stds}\")\n",
        "print(f\"number of input connections square root:{squar_root_of_N}\")\n",
        "                      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6H43XYiOF1g"
      },
      "source": [
        "3. As we saw in the lecture, once we divide each layer by the square root of the layer's input width, we can maintain the input mean and variance $u_{l-1} \\sim \\mathcal{N}(0,I)$ on the output of that layer $u_l$. Thus, the outputs of the NN don't explode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1ezxJzWOF1g",
        "outputId": "4c29e1cf-acf4-4e2f-d593-5313f7c3fa97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x mean:-0.012621086090803146\n",
            "x std:0.7621493339538574\n"
          ]
        }
      ],
      "source": [
        "#1.3\n",
        "N=512\n",
        "x = torch.randn(N)\n",
        "for i in range(100):\n",
        "    a = torch.randn(N, N) / np.sqrt(N)\n",
        "    x = a @ x\n",
        "print(f\"x mean:{x.mean()}\\nx std:{x.std()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVutw1CaOF1h"
      },
      "source": [
        "4. Since tanh activation output is bounded by (-1,1), the output values can't explode and even w/o the normalization we maintain zero mean and variance one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyXIzRT8OF1k",
        "outputId": "6b6e497e-debb-45a1-8ec4-4e31fbe78f05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x mean:-0.03518223389983177\n",
            "x std:0.9775998592376709\n"
          ]
        }
      ],
      "source": [
        "#1.4\n",
        "N=512\n",
        "x = torch.randn(N)\n",
        "for i in range(100):\n",
        "    a = torch.randn(N, N) \n",
        "    x = torch.tanh(a @ x)\n",
        "print(f\"x mean:{x.mean()}\\nx std:{x.std()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihmZB4R1OF1k"
      },
      "source": [
        "5. With Xavier init We can see the mean is closer to 0 compared to previous section w/o it. The variance has changed and got smaller though, which can cause vanishing values at the last layers of the NN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjTXcstZOF1m",
        "outputId": "0a37152d-cb3f-4f80-f786-290445b28d0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x mean:-0.00306722242385149\n",
            "x std:0.06797510385513306\n"
          ]
        }
      ],
      "source": [
        "#1.5\n",
        "def xavier_init(fan_in, fan_out):\n",
        "    xavier_coef = np.sqrt(6/(fan_in + fan_out))\n",
        "    a = torch.rand(fan_out, fan_in)\n",
        "    a = xavier_coef * (2 * a - 1) \n",
        "    return a\n",
        "\n",
        "N=512\n",
        "x = torch.randn(N)\n",
        "for i in range(100):\n",
        "    a = xavier_init(N, N) \n",
        "    x = torch.tanh(a @ x)\n",
        "print(f\"x mean:{x.mean()}\\nx std:{x.std()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCgdVwvUOF1m"
      },
      "source": [
        "6. Using Relu activation instead of tanh with Xavier initialization, the mean and std are zeroed almost completely, thus the values at output will be effectively 0.  \n",
        "  With Kaiming He init, as we saw in the dry part Q4, for the Relu activation we can use any symmetric distribution and when we enforce the weights std to be  $\\sqrt{\\frac{2}{d_{l-1}}}$ we preserve the input mean and variance on output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8HI86pVOF1n",
        "outputId": "03b37fc9-76a4-405e-9bc6-b60d71470a7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relu with xavier init mean:3.6174410778934776e-16\n",
            "relu with xavier init std:5.593861699600155e-16\n",
            "relu with kaiming init mean:0.5281035900115967\n",
            "relu with kaiming init std:0.8080969452857971\n"
          ]
        }
      ],
      "source": [
        "#1.6\n",
        "N=512\n",
        "x = torch.randn(N)\n",
        "for i in range(100):\n",
        "    a = xavier_init(N, N) \n",
        "    x = torch.relu(a @ x)\n",
        "print(f\"relu with xavier init mean:{x.mean()}\\nrelu with xavier init std:{x.std()}\")\n",
        "\n",
        "def kaiming_init(fan_in, fan_out):\n",
        "    return torch.randn(fan_out, fan_in) * np.sqrt(2/fan_in)\n",
        "\n",
        "N=512\n",
        "x = torch.randn(N)\n",
        "num_of_layers = 100\n",
        "#arr_of_x=np.zeros((N,num_of_runs))\n",
        "for i in range(num_of_layers):\n",
        "    a = kaiming_init(N,N) \n",
        "    x = torch.relu(a @ x)\n",
        "    #arr_of_x[:,i] = x.numpy()\n",
        "\n",
        "print(f\"relu with kaiming init mean:{x.mean()}\\nrelu with kaiming init std:{x.std()}\")\n",
        "#print(f\"relu with kaiming init mean:{arr_of_x.mean()}\\nrelu with kaiming init std:{arr_of_x.std()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlSATRm2OF1n"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 2 - FashionMNIST Deep Classifer\n",
        "---\n",
        "In this task you are going to design and train your first neural network for classification.\n",
        "1. Load the FashionMNIST dataset `torchvision.datasets.FashionMNIST` and display 6 images with their labels from the dataset.\n",
        "2. Design a **MLP** to classify images from the FashionMNIST dataset. **You need to reach at least 85% accuracy on the test set, and 89% for a full grade**.\n",
        "    * You have a free choice of architecture, optimizer, learning scheduler, initialization, regularization and activations.\n",
        "    * In a Markdown block, write down the chosen architectures and all the hyper-parameters.\n",
        "    * **Plot** the loss curves (and any oter statistic you want) as a function of epochs/iterations.\n",
        "    * **Print** the test accuracy.\n",
        "3. Change the initialization of the linear layers and re-train the model. You can pick an initialization of your choosing from : https://pytorch.org/docs/stable/nn.init.html . See example below how to use. **Print** the change in accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBsEvB25OF1n"
      },
      "outputs": [],
      "source": [
        "# example of weight initialization\n",
        "import torch.nn as nn\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, parmaeters):\n",
        "        super(MyModel, self).__init__()\n",
        "        # model definitions/blocks\n",
        "        # ...\n",
        "        # custom initialization\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                # pick initialzation: https://pytorch.org/docs/stable/nn.init.html\n",
        "                # examples\n",
        "                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                # nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu', a=math.sqrt(5))\n",
        "                # nn.init.normal_(m.weight, 0, 0.005)\n",
        "                # don't forget the bias term (m.bias)\n",
        "                pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ops on x\n",
        "        # ...\n",
        "        # output = f(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 2 Code and Answers - FashionMNIST Deep Classifer"
      ],
      "metadata": {
        "id": "h4mTzjp_CTmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load and display of some 6 randome samples from the dataset:"
      ],
      "metadata": {
        "id": "Hoi5cXxF7FSI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "MQwBVxNAOF1o",
        "outputId": "0148e21a-ba61-47c2-d5be-244202739b60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running calculations on:  cuda:0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x1080 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#2.1\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# check if there is gpu avilable, if there is, use it\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.current_device()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"running calculations on: \", device)\n",
        "\n",
        "seed = 211\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "normalize = transforms.Normalize((0.5,), (0.5,))\n",
        "\n",
        "# define transforms\n",
        "valid_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "train_transform = transforms.Compose([\n",
        "    #transforms.RandomCrop(28, padding=4),\n",
        "    #transforms.RandomAffine(0, translate=(0.7,0.7)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "# load the dataset\n",
        "data_dir = './datasets/'\n",
        "train_dataset = datasets.FashionMNIST(\n",
        "    root=data_dir, train=True, transform=train_transform, download=True)\n",
        "valid_dataset = datasets.FashionMNIST(\n",
        "    root=data_dir, train=True, transform=valid_transform, download=True)\n",
        "test_dataset = datasets.FashionMNIST(\n",
        "    root=data_dir, train=False, transform=valid_transform, download=True)\n",
        "\n",
        "# visualize some images\n",
        "train_dataset_no_transform = datasets.FashionMNIST(\n",
        "    root=data_dir, train=True, transform=transforms.ToTensor(), download=True)\n",
        "n_samples = 6\n",
        "sample_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset_no_transform, batch_size=n_samples, shuffle=True)\n",
        "data_iter = iter(sample_loader)\n",
        "sampled_images, sampled_labels = data_iter.next()\n",
        "sampled_imgs_np = sampled_images.numpy()\n",
        "sampled_labels_np = sampled_labels.numpy()\n",
        "\n",
        "def plot_samples(samples, labels, predictions=None):\n",
        "  fig = plt.figure(figsize=(15 ,15))\n",
        "  _, axes = plt.subplots(2, int(len(samples)/2))\n",
        "\n",
        "  for i, ax in enumerate(axes.flat):\n",
        "      ax.imshow(samples[i, 0, :, :], interpolation='none', cmap='gray')\n",
        "      xlabel = str(labels[i])\n",
        "      title = f\"GT: {xlabel}\"\n",
        "      if np.any(predictions) != None:\n",
        "        xpred = str(np.argmax(predictions[i]))\n",
        "        title += f\", P: {xpred}\"\n",
        "      ax.set_title(title)    \n",
        "      ax.set_xlabel(xlabel)\n",
        "      ax.set_xticks([])\n",
        "      ax.set_yticks([])\n",
        "      ax.set_axis_off()\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "plot_samples(sampled_imgs_np, sampled_labels_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. The architechture and training hyper parameters:   \n",
        "  * The architechture:  \n",
        "Linear hidden layers with the following sizes = [512, 256, 128, 64, 10]  (initlally tried without the first hidden layer).  \n",
        "The activation layers are leaky relu (tried also relu and tanh).  \n",
        "The initializtion of the linear layer is the default.  \n",
        "Dropout was enabled for better generalization with 0.2 factor.  \n",
        "Batchnorm was enabled as well between the layers.  \n",
        "See also torchviz output of the network below.  \n",
        "  * The hyper parameters:  \n",
        "Initial learning rate=0.01  \n",
        "Learning Rate scheduler  ReduceLROnPlateau that works on the accuracy (maximize) with patience=2 and factor 0.5.  \n",
        "The optimizer is SGD with momentum parameter=0.9 and with nestrov method (played also with Adam)  \n",
        "Batch size = 128  \n",
        "Number of Epochs = 50 (saving the model with best accuracy along the way)  \n",
        "L2 Regularization with wieght decay factor = 1e-5  \n",
        "Validation split factor = 0.2  \n",
        "Tried to use some augmentation on the data, but it seems the data distribution was too big for the network to learn on future test samples. When removing crop/translation, the generalization on future test samples was better maybe due to hard definition of how the objects are showed to the calssifier.  \n",
        "We used horizontal flip transformation though, to increase data size.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xb7eWpaG-NVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2.2\n",
        "\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "\n",
        "class MyMLP(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(MyMLP, self).__init__()\n",
        "        self.params = copy.deepcopy(params)\n",
        "        self.hidden_layer_sizes = self.params['layers_sizes'][:]\n",
        "        self.num_of_hidden_layers = len(self.hidden_layer_sizes)\n",
        "        assert self.num_of_hidden_layers > 0, \"number of hidden layers should be at least 1\"\n",
        "        self.input_size = self.params['input_size']\n",
        "\n",
        "        self.fc = nn.ModuleList()\n",
        "        prev_layer_size = self.input_size\n",
        "        for layer_index, current_hidden_layer_size in enumerate(self.hidden_layer_sizes):\n",
        "          #print(f\"Adding FC Linear layer {prev_layer_size}x{current_hidden_layer_size}\")\n",
        "          self.fc.append(nn.Linear(prev_layer_size, current_hidden_layer_size))\n",
        "          prev_layer_size = current_hidden_layer_size\n",
        "\n",
        "        if self.params['enable_dropout']:\n",
        "          print(f\"Dropout is enabled with {self.params['dropout']}\")\n",
        "          self.dropout = nn.Dropout(self.params['dropout'])\n",
        "        if self.params['weights_init'] != 'default':\n",
        "          self.init_weights(self.params['weights_init'])\n",
        "\n",
        "    def init_weights(self, init_type):\n",
        "        print('custom init')\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                # pick initialzation: https://pytorch.org/docs/stable/nn.init.html\n",
        "                # examples\n",
        "                if self.params['weights_init'] == 'kaiming_normal_fan_out_relu':\n",
        "                  print('kaiming_normal_fan_out_relu')\n",
        "                  nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if self.params['weights_init'] == 'kaiming_normal_fan_in_lrelu':\n",
        "                  print('kaiming_normal_fan_in_lrelu')\n",
        "                  nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu', a=np.sqrt(5))\n",
        "                # nn.init.normal_(m.weight, 0, 0.005)\n",
        "                # don't forget the bias term (m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        #prev_layer_size = self.input_size\n",
        "        for layer_index, current_hidden_layer_size in enumerate(self.hidden_layer_sizes):\n",
        "          #print(f\"Apply FC layer {layer_index}: {prev_layer_size}x{current_hidden_layer_size}\")\n",
        "          #prev_layer_size = current_hidden_layer_size\n",
        "          x = self.fc[layer_index](x)\n",
        "          if self.params['enable_batchnorm']:\n",
        "            nn.BatchNorm1d(current_hidden_layer_size)\n",
        "          if (layer_index < (len(self.hidden_layer_sizes)-1)):\n",
        "            if self.params['activation'] == 'relu':\n",
        "              #print(f\"Apply relu activation\")\n",
        "              x = nn.functional.relu(x)\n",
        "            elif self.params['activation'] == 'tanh':\n",
        "              x = torch.tanh(x)\n",
        "            elif self.params['activation'] == 'lrelu':\n",
        "              x = nn.functional.leaky_relu(x)\n",
        "            elif self.params['activation'] == 'elu':\n",
        "              x = nn.functional.elu(x)\n",
        "            elif self.params['activation'] == 'gelu':\n",
        "              x = nn.functional.gelu(x)\n",
        "            else:\n",
        "              pass\n",
        "            if self.params['enable_dropout']:\n",
        "              #print(f\"Apply dropout activation\")\n",
        "              x = self.dropout(x)\n",
        "        #output = nn.functional.softmax(x, dim=1)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "qeDx-BEK4wg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the training and checking test accuracy + validation loss at every epoch"
      ],
      "metadata": {
        "id": "iWnEVe-MgxGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training\n",
        "def train(model_hyper_params, train_hyper_params, save_model_with_best_accuracy=False):\n",
        "\n",
        "  num_train = len(train_dataset)\n",
        "  indices = list(range(num_train))\n",
        "  split = int(np.floor(train_hyper_params['VALID_SIZE'] * num_train))\n",
        "\n",
        "  np.random.shuffle(indices)\n",
        "  train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "  train_sampler = SubsetRandomSampler(train_idx)\n",
        "  valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      train_dataset, batch_size=train_hyper_params['BATCH_SIZE'],\n",
        "      sampler=train_sampler)\n",
        "\n",
        "  valid_loader = torch.utils.data.DataLoader(\n",
        "      valid_dataset, batch_size=train_hyper_params['BATCH_SIZE'],\n",
        "      sampler=valid_sampler)\n",
        "\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      test_dataset, batch_size=train_hyper_params['BATCH_SIZE'])\n",
        "\n",
        "  # loss criterion\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  # model\n",
        "  model = MyMLP(model_hyper_params).to(device)\n",
        "  #model(images[0].to(device))\n",
        "  # optimizer\n",
        "  if train_hyper_params['OPTIMIZER'] == 'adam':\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=train_hyper_params['LEARNING_RATE'])\n",
        "  elif train_hyper_params['OPTIMIZER'] == 'sgd':\n",
        "    optimizer = torch.optim.SGD(\n",
        "        model.parameters(),\n",
        "        lr=train_hyper_params['LEARNING_RATE'],\n",
        "        momentum=0.9, nesterov=True, \n",
        "        weight_decay=train_hyper_params['OPTIMIZER_WEIGHT_DECAY'])\n",
        "  else:\n",
        "    assert False, \"optimizer was not defined\"\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  scheduler = ReduceLROnPlateau(\n",
        "      optimizer, mode='max', factor=train_hyper_params['SCHED_FACTOR'],\n",
        "      patience=train_hyper_params['SCHED_PAT'], verbose=True)\n",
        "\n",
        "  epochs_train_losses = []\n",
        "  epochs_valid_losses = []\n",
        "  epochs_test_accuracy = []\n",
        "  max_accuracy = 0\n",
        "\n",
        "  for epoch in range(train_hyper_params['NUM_OF_EPOCHS']):\n",
        "    print(f'epoch {epoch}')\n",
        "\n",
        "    model.train()\n",
        "    epoch_train_losses = []\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "      images = images.view(-1, input_size).to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # forward pass\n",
        "      predictions = model(images)\n",
        "\n",
        "      # loss\n",
        "      loss = criterion(predictions, labels)\n",
        "\n",
        "      # backward pass\n",
        "      optimizer.zero_grad() # clean the gradients from previous iteration\n",
        "      loss.backward() # autograd backward to calculate gradients\n",
        "      optimizer.step() # apply update to the weights\n",
        "      \n",
        "      epoch_train_losses.append(loss.item())\n",
        "\n",
        "    model.eval()\n",
        "    epoch_valid_losses = []\n",
        "\n",
        "    for images, labels in valid_loader:\n",
        "      images = images.view(-1, input_size).to(device)\n",
        "      labels = labels.to(device)\n",
        "      predictions = model(images)\n",
        "      loss = criterion(predictions, labels)\n",
        "      epoch_valid_losses.append(loss.item())\n",
        "\n",
        "    # Calculate Accuracy\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # Iterate through test dataset\n",
        "    for images, labels in test_loader:\n",
        "      # Send images and labels to device\n",
        "      images = images.view(-1, input_size).to(device)\n",
        "      labels = labels.to(device)\n",
        "      # Forward pass only to get logits/output\n",
        "      outputs = model(images)\n",
        "      # Get predictions from the maximum value\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      predicted = predicted.to(device)\n",
        "      # Total number of labels\n",
        "      total += labels.size(0)\n",
        "      # Total correct predictions\n",
        "      # Without .item(), it is a uint8 tensor which will \n",
        "      # not work when you pass this number to the scheduler\n",
        "      correct += (predicted == labels).sum().item()\n",
        "      accuracy = 100 * correct / total\n",
        "\n",
        "    epochs_train_losses.append(np.mean(epoch_train_losses))\n",
        "    epochs_valid_losses.append(np.mean(epoch_valid_losses))\n",
        "    epochs_test_accuracy.append(accuracy)\n",
        "\n",
        "    # Decay Learning Rate, pass validation accuracy for tracking at every epoch\n",
        "    print(f'epoch: {epoch} train_loss: {epochs_train_losses[-1]:.4f} '\n",
        "      f'validation loss:{epochs_valid_losses[-1]:.4f} test accuracy: {accuracy:.2f}%')\n",
        "\n",
        "    scheduler.step(accuracy) # accuracy is used to track down a plateau\n",
        "\n",
        "    if accuracy > max_accuracy:\n",
        "      if save_model_with_best_accuracy:\n",
        "        if max_accuracy != 0:\n",
        "          old_filename = f'accuracy_{max_accuracy}_checkpoint.pth'\n",
        "          !rm {old_filename}\n",
        "        checkpoint = {'model_hyper_params': model_hyper_params,\n",
        "                      'train_hyper_params': train_hyper_params,\n",
        "                      'state_dict': model.state_dict()}\n",
        "        filename = f'accuracy_{accuracy}_checkpoint.pth'\n",
        "        torch.save(checkpoint, filename)\n",
        "      max_accuracy = accuracy\n",
        "\n",
        "  return max_accuracy, epochs_train_losses, epochs_valid_losses, epochs_test_accuracy\n",
        "\n",
        "seed = 211\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# model arch hyper params\n",
        "model_hyper_params = {}\n",
        "hidden_layer_sizes = [512, 256, 128, 64, 10]\n",
        "input_size = 28*28\n",
        "model_hyper_params['input_size'] = input_size \n",
        "model_hyper_params['layers_sizes'] = hidden_layer_sizes\n",
        "model_hyper_params['activation'] = 'lrelu'\n",
        "model_hyper_params['weights_init'] = 'default'\n",
        "model_hyper_params['enable_dropout'] = True \n",
        "model_hyper_params['dropout'] = 0.2\n",
        "model_hyper_params['enable_batchnorm'] = True\n",
        "\n",
        "\n",
        "#training hyper parameters\n",
        "train_hyper_params = {}\n",
        "train_hyper_params['VALID_SIZE'] = 0.2\n",
        "train_hyper_params['BATCH_SIZE'] = 128\n",
        "train_hyper_params['LEARNING_RATE'] = 0.01\n",
        "train_hyper_params['SCHED_FACTOR'] = 0.5\n",
        "train_hyper_params['SCHED_PAT'] = 2\n",
        "train_hyper_params['NUM_OF_EPOCHS'] = 50\n",
        "train_hyper_params['OPTIMIZER'] = 'sgd'\n",
        "train_hyper_params['OPTIMIZER_WEIGHT_DECAY'] = 1e-5\n",
        "\n",
        "best_accuracy, train_losses, valid_losses, test_accuracies = train(\n",
        "    model_hyper_params, train_hyper_params, save_model_with_best_accuracy=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujQVsA_high0",
        "outputId": "71af18f9-51d3-492e-d8e1-f7166b01c6d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropout is enabled with 0.2\n",
            "epoch 0\n",
            "epoch: 0 train_loss: 1.3015 validation loss:0.6617 test accuracy: 74.01%\n",
            "epoch 1\n",
            "epoch: 1 train_loss: 0.6052 validation loss:0.4713 test accuracy: 81.72%\n",
            "epoch 2\n",
            "epoch: 2 train_loss: 0.5070 validation loss:0.4245 test accuracy: 83.77%\n",
            "epoch 3\n",
            "epoch: 3 train_loss: 0.4668 validation loss:0.3999 test accuracy: 84.77%\n",
            "epoch 4\n",
            "epoch: 4 train_loss: 0.4339 validation loss:0.3775 test accuracy: 85.58%\n",
            "epoch 5\n",
            "epoch: 5 train_loss: 0.4131 validation loss:0.3729 test accuracy: 85.36%\n",
            "epoch 6\n",
            "epoch: 6 train_loss: 0.3955 validation loss:0.3591 test accuracy: 85.67%\n",
            "epoch 7\n",
            "epoch: 7 train_loss: 0.3842 validation loss:0.3586 test accuracy: 86.04%\n",
            "epoch 8\n",
            "epoch: 8 train_loss: 0.3704 validation loss:0.3460 test accuracy: 86.09%\n",
            "epoch 9\n",
            "epoch: 9 train_loss: 0.3600 validation loss:0.3363 test accuracy: 86.79%\n",
            "epoch 10\n",
            "epoch: 10 train_loss: 0.3528 validation loss:0.3374 test accuracy: 86.47%\n",
            "epoch 11\n",
            "epoch: 11 train_loss: 0.3453 validation loss:0.3316 test accuracy: 86.82%\n",
            "epoch 12\n",
            "epoch: 12 train_loss: 0.3382 validation loss:0.3269 test accuracy: 87.16%\n",
            "epoch 13\n",
            "epoch: 13 train_loss: 0.3319 validation loss:0.3189 test accuracy: 87.28%\n",
            "epoch 14\n",
            "epoch: 14 train_loss: 0.3235 validation loss:0.3203 test accuracy: 87.18%\n",
            "epoch 15\n",
            "epoch: 15 train_loss: 0.3173 validation loss:0.3283 test accuracy: 86.75%\n",
            "epoch 16\n",
            "epoch: 16 train_loss: 0.3137 validation loss:0.3232 test accuracy: 87.42%\n",
            "epoch 17\n",
            "epoch: 17 train_loss: 0.3067 validation loss:0.3151 test accuracy: 87.54%\n",
            "epoch 18\n",
            "epoch: 18 train_loss: 0.3031 validation loss:0.3156 test accuracy: 87.51%\n",
            "epoch 19\n",
            "epoch: 19 train_loss: 0.2952 validation loss:0.3058 test accuracy: 87.44%\n",
            "epoch 20\n",
            "epoch: 20 train_loss: 0.2898 validation loss:0.3048 test accuracy: 87.97%\n",
            "epoch 21\n",
            "epoch: 21 train_loss: 0.2888 validation loss:0.3035 test accuracy: 87.64%\n",
            "epoch 22\n",
            "epoch: 22 train_loss: 0.2836 validation loss:0.3003 test accuracy: 88.02%\n",
            "epoch 23\n",
            "epoch: 23 train_loss: 0.2784 validation loss:0.2972 test accuracy: 88.42%\n",
            "epoch 24\n",
            "epoch: 24 train_loss: 0.2767 validation loss:0.2931 test accuracy: 88.53%\n",
            "epoch 25\n",
            "epoch: 25 train_loss: 0.2705 validation loss:0.2953 test accuracy: 87.99%\n",
            "epoch 26\n",
            "epoch: 26 train_loss: 0.2674 validation loss:0.2961 test accuracy: 88.09%\n",
            "epoch 27\n",
            "epoch: 27 train_loss: 0.2621 validation loss:0.2911 test accuracy: 88.52%\n",
            "Epoch    28: reducing learning rate of group 0 to 5.0000e-03.\n",
            "epoch 28\n",
            "epoch: 28 train_loss: 0.2393 validation loss:0.2856 test accuracy: 88.76%\n",
            "epoch 29\n",
            "epoch: 29 train_loss: 0.2332 validation loss:0.2850 test accuracy: 88.79%\n",
            "epoch 30\n",
            "epoch: 30 train_loss: 0.2293 validation loss:0.2803 test accuracy: 88.98%\n",
            "epoch 31\n",
            "epoch: 31 train_loss: 0.2271 validation loss:0.2808 test accuracy: 88.84%\n",
            "epoch 32\n",
            "epoch: 32 train_loss: 0.2233 validation loss:0.2842 test accuracy: 89.02%\n",
            "epoch 33\n",
            "epoch: 33 train_loss: 0.2225 validation loss:0.2784 test accuracy: 88.94%\n",
            "epoch 34\n",
            "epoch: 34 train_loss: 0.2189 validation loss:0.2818 test accuracy: 88.75%\n",
            "epoch 35\n",
            "epoch: 35 train_loss: 0.2166 validation loss:0.2794 test accuracy: 88.89%\n",
            "Epoch    36: reducing learning rate of group 0 to 2.5000e-03.\n",
            "epoch 36\n",
            "epoch: 36 train_loss: 0.2005 validation loss:0.2762 test accuracy: 89.06%\n",
            "epoch 37\n",
            "epoch: 37 train_loss: 0.2001 validation loss:0.2748 test accuracy: 89.22%\n",
            "epoch 38\n",
            "epoch: 38 train_loss: 0.1993 validation loss:0.2801 test accuracy: 89.13%\n",
            "epoch 39\n",
            "epoch: 39 train_loss: 0.1954 validation loss:0.2789 test accuracy: 89.30%\n",
            "epoch 40\n",
            "epoch: 40 train_loss: 0.1921 validation loss:0.2754 test accuracy: 89.44%\n",
            "epoch 41\n",
            "epoch: 41 train_loss: 0.1928 validation loss:0.2754 test accuracy: 89.33%\n",
            "epoch 42\n",
            "epoch: 42 train_loss: 0.1897 validation loss:0.2794 test accuracy: 89.32%\n",
            "epoch 43\n",
            "epoch: 43 train_loss: 0.1906 validation loss:0.2776 test accuracy: 89.55%\n",
            "epoch 44\n",
            "epoch: 44 train_loss: 0.1861 validation loss:0.2767 test accuracy: 89.08%\n",
            "epoch 45\n",
            "epoch: 45 train_loss: 0.1876 validation loss:0.2788 test accuracy: 89.35%\n",
            "epoch 46\n",
            "epoch: 46 train_loss: 0.1846 validation loss:0.2815 test accuracy: 89.16%\n",
            "Epoch    47: reducing learning rate of group 0 to 1.2500e-03.\n",
            "epoch 47\n",
            "epoch: 47 train_loss: 0.1768 validation loss:0.2803 test accuracy: 89.32%\n",
            "epoch 48\n",
            "epoch: 48 train_loss: 0.1754 validation loss:0.2782 test accuracy: 89.71%\n",
            "epoch 49\n",
            "epoch: 49 train_loss: 0.1741 validation loss:0.2789 test accuracy: 89.60%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ploting the train and validation losses as a function of the epochs, we can actually see the epochs on which the learning rate was changed by the scheduler on plateau."
      ],
      "metadata": {
        "id": "1G6eTGXEj0Mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses, label='train losses')\n",
        "plt.plot(valid_losses, label='valid losses')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "QYMEGwFCrf1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "58f34fea-b4a9-49ec-d6e8-d1c9db271ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f4dda22af10>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing the accuracy:"
      ],
      "metadata": {
        "id": "FyvuC9TWj_7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"the best accuracy is: {best_accuracy}\")\n",
        "plt.plot(test_accuracies, label='test accuracy vs epochs')\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "vzoNv-qSjomX",
        "outputId": "150888a9-c054-4262-8eb1-4d6876353d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the best accuracy is: 89.71\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f4ddaf21b50>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Samples prediciton vs labels:"
      ],
      "metadata": {
        "id": "lzk2OsZtkxSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = f'accuracy_{best_accuracy}_checkpoint.pth'\n",
        "checkpoint = torch.load(filename)\n",
        "print(f\"train_hyper_params {checkpoint['train_hyper_params']}\")\n",
        "print(f\"model_hyper_params {checkpoint['model_hyper_params']}\")\n",
        "model = MyMLP(model_hyper_params).to(device)\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "normalized_sampled_images = normalize(sampled_images)\n",
        "sampled_images_flat = normalized_sampled_images.view(-1, input_size).to(device)\n",
        "sampled_labels = sampled_labels.to(device)\n",
        "\n",
        "model.eval()\n",
        "predictions = model(sampled_images_flat).detach().cpu().numpy()\n",
        "\n",
        "plot_samples(sampled_images.cpu().numpy(), sampled_labels.cpu().numpy(), predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "fEfWnrzzkv77",
        "outputId": "4642f367-840b-4375-d8ed-e519cb000a11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_hyper_params {'VALID_SIZE': 0.2, 'BATCH_SIZE': 128, 'LEARNING_RATE': 0.01, 'SCHED_FACTOR': 0.5, 'SCHED_PAT': 2, 'NUM_OF_EPOCHS': 50, 'OPTIMIZER': 'sgd', 'OPTIMIZER_WEIGHT_DECAY': 1e-05}\n",
            "model_hyper_params {'input_size': 784, 'layers_sizes': [512, 256, 128, 64, 10], 'activation': 'lrelu', 'weights_init': 'default', 'enable_dropout': True, 'dropout': 0.2, 'enable_batchnorm': True}\n",
            "Dropout is enabled with 0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x1080 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the model graph:"
      ],
      "metadata": {
        "id": "YMvf5w4pkeds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "\n",
        "# for torchviz on google collab\n",
        "!pip install torchviz\n",
        "import torchviz\n",
        "\n",
        "x = torch.randn(1, input_size).to(device)\n",
        "torchviz.make_dot(model(x), params=dict(model.named_parameters()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nd2yx8f5d5_V",
        "outputId": "3453986d-e175-4170-f6be-f736a174d058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyMLP(\n",
            "  (fc): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (4): Linear(in_features=64, out_features=10, bias=True)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n",
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.10.0+cu111)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (3.10.0.2)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4151 sha256=05f7c2af03df0dc3ce6603baf5e318230c87495f1bd40e0cab53ab26766f6cf9\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/38/f5/dc4f85c3909051823df49901e72015d2d750bd26b086480ec2\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7f4dda34c4d0>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"560pt\" height=\"809pt\"\n viewBox=\"0.00 0.00 560.00 809.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 805)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-805 556,-805 556,4 -4,4\"/>\n<!-- 139972350140784 -->\n<g id=\"node1\" class=\"node\">\n<title>139972350140784</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"361,-31 296,-31 296,0 361,0 361,-31\"/>\n<text text-anchor=\"middle\" x=\"328.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1, 10)</text>\n</g>\n<!-- 139972350100560 -->\n<g id=\"node2\" class=\"node\">\n<title>139972350100560</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"379,-86 278,-86 278,-67 379,-67 379,-86\"/>\n<text text-anchor=\"middle\" x=\"328.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward0</text>\n</g>\n<!-- 139972350100560&#45;&gt;139972350140784 -->\n<g id=\"edge34\" class=\"edge\">\n<title>139972350100560&#45;&gt;139972350140784</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M328.5,-66.9688C328.5,-60.1289 328.5,-50.5621 328.5,-41.5298\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"332.0001,-41.3678 328.5,-31.3678 325.0001,-41.3678 332.0001,-41.3678\"/>\n</g>\n<!-- 139972350100816 -->\n<g id=\"node3\" class=\"node\">\n<title>139972350100816</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"248,-141 147,-141 147,-122 248,-122 248,-141\"/>\n<text text-anchor=\"middle\" x=\"197.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 139972350100816&#45;&gt;139972350100560 -->\n<g id=\"edge1\" class=\"edge\">\n<title>139972350100816&#45;&gt;139972350100560</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M220.3185,-121.9197C241.3222,-113.1014 272.6971,-99.9287 296.2577,-90.0369\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"297.6381,-93.2533 305.5035,-86.155 294.9282,-86.7991 297.6381,-93.2533\"/>\n</g>\n<!-- 139972354106096 -->\n<g id=\"node4\" class=\"node\">\n<title>139972354106096</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"233,-207 162,-207 162,-177 233,-177 233,-207\"/>\n<text text-anchor=\"middle\" x=\"197.5\" y=\"-195\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">fc.4.bias</text>\n<text text-anchor=\"middle\" x=\"197.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (10)</text>\n</g>\n<!-- 139972354106096&#45;&gt;139972350100816 -->\n<g id=\"edge2\" class=\"edge\">\n<title>139972354106096&#45;&gt;139972350100816</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M197.5,-176.7333C197.5,-169.0322 197.5,-159.5977 197.5,-151.3414\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"201.0001,-151.0864 197.5,-141.0864 194.0001,-151.0864 201.0001,-151.0864\"/>\n</g>\n<!-- 139972350100432 -->\n<g id=\"node5\" class=\"node\">\n<title>139972350100432</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"391,-141 266,-141 266,-122 391,-122 391,-141\"/>\n<text text-anchor=\"middle\" x=\"328.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">LeakyReluBackward0</text>\n</g>\n<!-- 139972350100432&#45;&gt;139972350100560 -->\n<g id=\"edge3\" class=\"edge\">\n<title>139972350100432&#45;&gt;139972350100560</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M328.5,-121.9197C328.5,-114.9083 328.5,-105.1442 328.5,-96.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"332.0001,-96.3408 328.5,-86.3408 325.0001,-96.3409 332.0001,-96.3408\"/>\n</g>\n<!-- 139972354443984 -->\n<g id=\"node6\" class=\"node\">\n<title>139972354443984</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"365,-201.5 264,-201.5 264,-182.5 365,-182.5 365,-201.5\"/>\n<text text-anchor=\"middle\" x=\"314.5\" y=\"-189.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward0</text>\n</g>\n<!-- 139972354443984&#45;&gt;139972350100432 -->\n<g id=\"edge4\" class=\"edge\">\n<title>139972354443984&#45;&gt;139972350100432</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M316.7493,-182.2796C318.6769,-173.9499 321.5143,-161.688 323.9181,-151.3004\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"327.3651,-151.9289 326.2097,-141.3972 320.5453,-150.3507 327.3651,-151.9289\"/>\n</g>\n<!-- 139972362325584 -->\n<g id=\"node7\" class=\"node\">\n<title>139972362325584</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"213,-267.5 112,-267.5 112,-248.5 213,-248.5 213,-267.5\"/>\n<text text-anchor=\"middle\" x=\"162.5\" y=\"-255.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 139972362325584&#45;&gt;139972354443984 -->\n<g id=\"edge5\" class=\"edge\">\n<title>139972362325584&#45;&gt;139972354443984</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M184.6021,-248.403C210.4934,-237.1608 253.5791,-218.4525 282.9725,-205.6896\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"284.5926,-208.8019 292.3712,-201.6085 281.8046,-202.381 284.5926,-208.8019\"/>\n</g>\n<!-- 139972354105904 -->\n<g id=\"node8\" class=\"node\">\n<title>139972354105904</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"198,-339 127,-339 127,-309 198,-309 198,-339\"/>\n<text text-anchor=\"middle\" x=\"162.5\" y=\"-327\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">fc.3.bias</text>\n<text text-anchor=\"middle\" x=\"162.5\" y=\"-316\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (64)</text>\n</g>\n<!-- 139972354105904&#45;&gt;139972362325584 -->\n<g id=\"edge6\" class=\"edge\">\n<title>139972354105904&#45;&gt;139972362325584</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M162.5,-308.6924C162.5,-299.5067 162.5,-287.7245 162.5,-277.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"166.0001,-277.703 162.5,-267.7031 159.0001,-277.7031 166.0001,-277.703\"/>\n</g>\n<!-- 139972363219664 -->\n<g id=\"node9\" class=\"node\">\n<title>139972363219664</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"356,-267.5 231,-267.5 231,-248.5 356,-248.5 356,-267.5\"/>\n<text text-anchor=\"middle\" x=\"293.5\" y=\"-255.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">LeakyReluBackward0</text>\n</g>\n<!-- 139972363219664&#45;&gt;139972354443984 -->\n<g id=\"edge7\" class=\"edge\">\n<title>139972363219664&#45;&gt;139972354443984</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M296.5986,-248.2615C299.6695,-238.6102 304.4588,-223.558 308.3112,-211.4506\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"311.6918,-212.369 311.3887,-201.7785 305.0213,-210.2465 311.6918,-212.369\"/>\n</g>\n<!-- 139972363220880 -->\n<g id=\"node10\" class=\"node\">\n<title>139972363220880</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"330,-333.5 229,-333.5 229,-314.5 330,-314.5 330,-333.5\"/>\n<text text-anchor=\"middle\" x=\"279.5\" y=\"-321.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward0</text>\n</g>\n<!-- 139972363220880&#45;&gt;139972363219664 -->\n<g id=\"edge8\" class=\"edge\">\n<title>139972363220880&#45;&gt;139972363219664</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M281.5657,-314.2615C283.5923,-304.7077 286.7415,-289.8615 289.2961,-277.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"292.7745,-278.2871 291.4258,-267.7785 285.9268,-276.8345 292.7745,-278.2871\"/>\n</g>\n<!-- 139972354397520 -->\n<g id=\"node11\" class=\"node\">\n<title>139972354397520</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"157,-399.5 56,-399.5 56,-380.5 157,-380.5 157,-399.5\"/>\n<text text-anchor=\"middle\" x=\"106.5\" y=\"-387.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 139972354397520&#45;&gt;139972363220880 -->\n<g id=\"edge9\" class=\"edge\">\n<title>139972354397520&#45;&gt;139972363220880</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M131.6557,-380.403C161.5937,-368.9816 211.7305,-349.8543 245.2043,-337.0839\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"246.499,-340.3361 254.5946,-333.5015 244.0039,-333.7959 246.499,-340.3361\"/>\n</g>\n<!-- 139972354105712 -->\n<g id=\"node12\" class=\"node\">\n<title>139972354105712</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"142,-471 71,-471 71,-441 142,-441 142,-471\"/>\n<text text-anchor=\"middle\" x=\"106.5\" y=\"-459\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">fc.2.bias</text>\n<text text-anchor=\"middle\" x=\"106.5\" y=\"-448\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (128)</text>\n</g>\n<!-- 139972354105712&#45;&gt;139972354397520 -->\n<g id=\"edge10\" class=\"edge\">\n<title>139972354105712&#45;&gt;139972354397520</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M106.5,-440.6924C106.5,-431.5067 106.5,-419.7245 106.5,-409.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"110.0001,-409.703 106.5,-399.7031 103.0001,-409.7031 110.0001,-409.703\"/>\n</g>\n<!-- 139972363221712 -->\n<g id=\"node13\" class=\"node\">\n<title>139972363221712</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"300,-399.5 175,-399.5 175,-380.5 300,-380.5 300,-399.5\"/>\n<text text-anchor=\"middle\" x=\"237.5\" y=\"-387.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">LeakyReluBackward0</text>\n</g>\n<!-- 139972363221712&#45;&gt;139972363220880 -->\n<g id=\"edge11\" class=\"edge\">\n<title>139972363221712&#45;&gt;139972363220880</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M243.6972,-380.2615C250.0251,-370.3178 260.0013,-354.6408 267.8175,-342.3582\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"270.8613,-344.0942 273.2773,-333.7785 264.9557,-340.336 270.8613,-344.0942\"/>\n</g>\n<!-- 139972363221264 -->\n<g id=\"node14\" class=\"node\">\n<title>139972363221264</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"274,-465.5 173,-465.5 173,-446.5 274,-446.5 274,-465.5\"/>\n<text text-anchor=\"middle\" x=\"223.5\" y=\"-453.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward0</text>\n</g>\n<!-- 139972363221264&#45;&gt;139972363221712 -->\n<g id=\"edge12\" class=\"edge\">\n<title>139972363221264&#45;&gt;139972363221712</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M225.5657,-446.2615C227.5923,-436.7077 230.7415,-421.8615 233.2961,-409.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"236.7745,-410.2871 235.4258,-399.7785 229.9268,-408.8345 236.7745,-410.2871\"/>\n</g>\n<!-- 139972363089104 -->\n<g id=\"node15\" class=\"node\">\n<title>139972363089104</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101,-531.5 0,-531.5 0,-512.5 101,-512.5 101,-531.5\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-519.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 139972363089104&#45;&gt;139972363221264 -->\n<g id=\"edge13\" class=\"edge\">\n<title>139972363089104&#45;&gt;139972363221264</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M75.6557,-512.403C105.5937,-500.9816 155.7305,-481.8543 189.2043,-469.0839\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"190.499,-472.3361 198.5946,-465.5015 188.0039,-465.7959 190.499,-472.3361\"/>\n</g>\n<!-- 139972360826608 -->\n<g id=\"node16\" class=\"node\">\n<title>139972360826608</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"86,-603 15,-603 15,-573 86,-573 86,-603\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-591\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">fc.1.bias</text>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-580\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (256)</text>\n</g>\n<!-- 139972360826608&#45;&gt;139972363089104 -->\n<g id=\"edge14\" class=\"edge\">\n<title>139972360826608&#45;&gt;139972363089104</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-572.6924C50.5,-563.5067 50.5,-551.7245 50.5,-541.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-541.703 50.5,-531.7031 47.0001,-541.7031 54.0001,-541.703\"/>\n</g>\n<!-- 139972354591696 -->\n<g id=\"node17\" class=\"node\">\n<title>139972354591696</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"244,-531.5 119,-531.5 119,-512.5 244,-512.5 244,-531.5\"/>\n<text text-anchor=\"middle\" x=\"181.5\" y=\"-519.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">LeakyReluBackward0</text>\n</g>\n<!-- 139972354591696&#45;&gt;139972363221264 -->\n<g id=\"edge15\" class=\"edge\">\n<title>139972354591696&#45;&gt;139972363221264</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M187.6972,-512.2615C194.0251,-502.3178 204.0013,-486.6408 211.8175,-474.3582\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"214.8613,-476.0942 217.2773,-465.7785 208.9557,-472.336 214.8613,-476.0942\"/>\n</g>\n<!-- 139972354592080 -->\n<g id=\"node18\" class=\"node\">\n<title>139972354592080</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"232,-597.5 131,-597.5 131,-578.5 232,-578.5 232,-597.5\"/>\n<text text-anchor=\"middle\" x=\"181.5\" y=\"-585.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward0</text>\n</g>\n<!-- 139972354592080&#45;&gt;139972354591696 -->\n<g id=\"edge16\" class=\"edge\">\n<title>139972354592080&#45;&gt;139972354591696</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M181.5,-578.2615C181.5,-568.7077 181.5,-553.8615 181.5,-541.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"185.0001,-541.7784 181.5,-531.7785 178.0001,-541.7785 185.0001,-541.7784\"/>\n</g>\n<!-- 139972350100368 -->\n<g id=\"node19\" class=\"node\">\n<title>139972350100368</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"135,-663.5 34,-663.5 34,-644.5 135,-644.5 135,-663.5\"/>\n<text text-anchor=\"middle\" x=\"84.5\" y=\"-651.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 139972350100368&#45;&gt;139972354592080 -->\n<g id=\"edge17\" class=\"edge\">\n<title>139972350100368&#45;&gt;139972354592080</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M98.8127,-644.2615C114.6637,-633.4762 140.4274,-615.9463 158.9868,-603.3183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"161.1452,-606.0831 167.444,-597.5639 157.2074,-600.2957 161.1452,-606.0831\"/>\n</g>\n<!-- 139972354105520 -->\n<g id=\"node20\" class=\"node\">\n<title>139972354105520</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"120,-735 49,-735 49,-705 120,-705 120,-735\"/>\n<text text-anchor=\"middle\" x=\"84.5\" y=\"-723\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">fc.0.bias</text>\n<text text-anchor=\"middle\" x=\"84.5\" y=\"-712\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (512)</text>\n</g>\n<!-- 139972354105520&#45;&gt;139972350100368 -->\n<g id=\"edge18\" class=\"edge\">\n<title>139972354105520&#45;&gt;139972350100368</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M84.5,-704.6924C84.5,-695.5067 84.5,-683.7245 84.5,-673.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"88.0001,-673.703 84.5,-663.7031 81.0001,-673.7031 88.0001,-673.703\"/>\n</g>\n<!-- 139972350100240 -->\n<g id=\"node21\" class=\"node\">\n<title>139972350100240</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"230,-663.5 153,-663.5 153,-644.5 230,-644.5 230,-663.5\"/>\n<text text-anchor=\"middle\" x=\"191.5\" y=\"-651.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward0</text>\n</g>\n<!-- 139972350100240&#45;&gt;139972354592080 -->\n<g id=\"edge19\" class=\"edge\">\n<title>139972350100240&#45;&gt;139972354592080</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M190.0245,-644.2615C188.5769,-634.7077 186.3275,-619.8615 184.5028,-607.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"187.9402,-607.1413 182.9816,-597.7785 181.0192,-608.19 187.9402,-607.1413\"/>\n</g>\n<!-- 139972350100752 -->\n<g id=\"node22\" class=\"node\">\n<title>139972350100752</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"242,-729.5 141,-729.5 141,-710.5 242,-710.5 242,-729.5\"/>\n<text text-anchor=\"middle\" x=\"191.5\" y=\"-717.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 139972350100752&#45;&gt;139972350100240 -->\n<g id=\"edge20\" class=\"edge\">\n<title>139972350100752&#45;&gt;139972350100240</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M191.5,-710.2615C191.5,-700.7077 191.5,-685.8615 191.5,-673.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"195.0001,-673.7784 191.5,-663.7785 188.0001,-673.7785 195.0001,-673.7784\"/>\n</g>\n<!-- 139972354105424 -->\n<g id=\"node23\" class=\"node\">\n<title>139972354105424</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"233,-801 150,-801 150,-771 233,-771 233,-801\"/>\n<text text-anchor=\"middle\" x=\"191.5\" y=\"-789\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">fc.0.weight</text>\n<text text-anchor=\"middle\" x=\"191.5\" y=\"-778\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (512, 784)</text>\n</g>\n<!-- 139972354105424&#45;&gt;139972350100752 -->\n<g id=\"edge21\" class=\"edge\">\n<title>139972354105424&#45;&gt;139972350100752</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M191.5,-770.6924C191.5,-761.5067 191.5,-749.7245 191.5,-739.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"195.0001,-739.703 191.5,-729.7031 188.0001,-739.7031 195.0001,-739.703\"/>\n</g>\n<!-- 139972354590864 -->\n<g id=\"node24\" class=\"node\">\n<title>139972354590864</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"339,-531.5 262,-531.5 262,-512.5 339,-512.5 339,-531.5\"/>\n<text text-anchor=\"middle\" x=\"300.5\" y=\"-519.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward0</text>\n</g>\n<!-- 139972354590864&#45;&gt;139972363221264 -->\n<g id=\"edge22\" class=\"edge\">\n<title>139972354590864&#45;&gt;139972363221264</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M289.1384,-512.2615C276.8988,-501.7704 257.2137,-484.8974 242.5892,-472.3622\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"244.5283,-469.4144 234.6579,-465.5639 239.9727,-474.7293 244.5283,-469.4144\"/>\n</g>\n<!-- 139972359426320 -->\n<g id=\"node25\" class=\"node\">\n<title>139972359426320</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"351,-597.5 250,-597.5 250,-578.5 351,-578.5 351,-597.5\"/>\n<text text-anchor=\"middle\" x=\"300.5\" y=\"-585.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 139972359426320&#45;&gt;139972354590864 -->\n<g id=\"edge23\" class=\"edge\">\n<title>139972359426320&#45;&gt;139972354590864</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M300.5,-578.2615C300.5,-568.7077 300.5,-553.8615 300.5,-541.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"304.0001,-541.7784 300.5,-531.7785 297.0001,-541.7785 304.0001,-541.7784\"/>\n</g>\n<!-- 139972360826704 -->\n<g id=\"node26\" class=\"node\">\n<title>139972360826704</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"342,-669 259,-669 259,-639 342,-639 342,-669\"/>\n<text text-anchor=\"middle\" x=\"300.5\" y=\"-657\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">fc.1.weight</text>\n<text text-anchor=\"middle\" x=\"300.5\" y=\"-646\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (256, 512)</text>\n</g>\n<!-- 139972360826704&#45;&gt;139972359426320 -->\n<g id=\"edge24\" class=\"edge\">\n<title>139972360826704&#45;&gt;139972359426320</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M300.5,-638.6924C300.5,-629.5067 300.5,-617.7245 300.5,-607.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"304.0001,-607.703 300.5,-597.7031 297.0001,-607.7031 304.0001,-607.703\"/>\n</g>\n<!-- 139972363222096 -->\n<g id=\"node27\" class=\"node\">\n<title>139972363222096</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"395,-399.5 318,-399.5 318,-380.5 395,-380.5 395,-399.5\"/>\n<text text-anchor=\"middle\" x=\"356.5\" y=\"-387.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward0</text>\n</g>\n<!-- 139972363222096&#45;&gt;139972363220880 -->\n<g id=\"edge25\" class=\"edge\">\n<title>139972363222096&#45;&gt;139972363220880</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M345.1384,-380.2615C332.8988,-369.7704 313.2137,-352.8974 298.5892,-340.3622\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"300.5283,-337.4144 290.6579,-333.5639 295.9727,-342.7293 300.5283,-337.4144\"/>\n</g>\n<!-- 139976776166288 -->\n<g id=\"node28\" class=\"node\">\n<title>139976776166288</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"428,-465.5 327,-465.5 327,-446.5 428,-446.5 428,-465.5\"/>\n<text text-anchor=\"middle\" x=\"377.5\" y=\"-453.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 139976776166288&#45;&gt;139972363222096 -->\n<g id=\"edge26\" class=\"edge\">\n<title>139976776166288&#45;&gt;139972363222096</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M374.4014,-446.2615C371.3305,-436.6102 366.5412,-421.558 362.6888,-409.4506\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"365.9787,-408.2465 359.6113,-399.7785 359.3082,-410.369 365.9787,-408.2465\"/>\n</g>\n<!-- 139972354105616 -->\n<g id=\"node29\" class=\"node\">\n<title>139972354105616</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"440,-537 357,-537 357,-507 440,-507 440,-537\"/>\n<text text-anchor=\"middle\" x=\"398.5\" y=\"-525\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">fc.2.weight</text>\n<text text-anchor=\"middle\" x=\"398.5\" y=\"-514\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (128, 256)</text>\n</g>\n<!-- 139972354105616&#45;&gt;139976776166288 -->\n<g id=\"edge27\" class=\"edge\">\n<title>139972354105616&#45;&gt;139976776166288</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M393.6294,-506.6924C390.6753,-497.408 386.8771,-485.4708 383.7086,-475.5127\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"386.9547,-474.1711 380.5873,-465.7031 380.2842,-476.2936 386.9547,-474.1711\"/>\n</g>\n<!-- 139972363221136 -->\n<g id=\"node30\" class=\"node\">\n<title>139972363221136</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"451,-267.5 374,-267.5 374,-248.5 451,-248.5 451,-267.5\"/>\n<text text-anchor=\"middle\" x=\"412.5\" y=\"-255.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward0</text>\n</g>\n<!-- 139972363221136&#45;&gt;139972354443984 -->\n<g id=\"edge28\" class=\"edge\">\n<title>139972363221136&#45;&gt;139972354443984</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M398.0398,-248.2615C382.0253,-237.4762 355.996,-219.9463 337.2453,-207.3183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"338.9504,-204.2469 328.7009,-201.5639 335.0402,-210.053 338.9504,-204.2469\"/>\n</g>\n<!-- 139972354588880 -->\n<g id=\"node31\" class=\"node\">\n<title>139972354588880</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"484,-333.5 383,-333.5 383,-314.5 484,-314.5 484,-333.5\"/>\n<text text-anchor=\"middle\" x=\"433.5\" y=\"-321.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 139972354588880&#45;&gt;139972363221136 -->\n<g id=\"edge29\" class=\"edge\">\n<title>139972354588880&#45;&gt;139972363221136</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M430.4014,-314.2615C427.3305,-304.6102 422.5412,-289.558 418.6888,-277.4506\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"421.9787,-276.2465 415.6113,-267.7785 415.3082,-278.369 421.9787,-276.2465\"/>\n</g>\n<!-- 139972354105808 -->\n<g id=\"node32\" class=\"node\">\n<title>139972354105808</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"496,-405 413,-405 413,-375 496,-375 496,-405\"/>\n<text text-anchor=\"middle\" x=\"454.5\" y=\"-393\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">fc.3.weight</text>\n<text text-anchor=\"middle\" x=\"454.5\" y=\"-382\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (64, 128)</text>\n</g>\n<!-- 139972354105808&#45;&gt;139972354588880 -->\n<g id=\"edge30\" class=\"edge\">\n<title>139972354105808&#45;&gt;139972354588880</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M449.6294,-374.6924C446.6753,-365.408 442.8771,-353.4708 439.7086,-343.5127\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"442.9547,-342.1711 436.5873,-333.7031 436.2842,-344.2936 442.9547,-342.1711\"/>\n</g>\n<!-- 139972350100496 -->\n<g id=\"node33\" class=\"node\">\n<title>139972350100496</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"528,-141 451,-141 451,-122 528,-122 528,-141\"/>\n<text text-anchor=\"middle\" x=\"489.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward0</text>\n</g>\n<!-- 139972350100496&#45;&gt;139972350100560 -->\n<g id=\"edge31\" class=\"edge\">\n<title>139972350100496&#45;&gt;139972350100560</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M461.4559,-121.9197C435.1027,-112.9171 395.4648,-99.3762 366.3233,-89.421\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"367.0869,-85.9833 356.4923,-86.0626 364.8239,-92.6075 367.0869,-85.9833\"/>\n</g>\n<!-- 139972362975696 -->\n<g id=\"node34\" class=\"node\">\n<title>139972362975696</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"550,-201.5 449,-201.5 449,-182.5 550,-182.5 550,-201.5\"/>\n<text text-anchor=\"middle\" x=\"499.5\" y=\"-189.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 139972362975696&#45;&gt;139972350100496 -->\n<g id=\"edge32\" class=\"edge\">\n<title>139972362975696&#45;&gt;139972350100496</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M497.8933,-182.2796C496.5165,-173.9499 494.4898,-161.688 492.7728,-151.3004\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"496.2199,-150.6926 491.1359,-141.3972 489.3136,-151.8342 496.2199,-150.6926\"/>\n</g>\n<!-- 139972354106000 -->\n<g id=\"node35\" class=\"node\">\n<title>139972354106000</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"552,-273 469,-273 469,-243 552,-243 552,-273\"/>\n<text text-anchor=\"middle\" x=\"510.5\" y=\"-261\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">fc.4.weight</text>\n<text text-anchor=\"middle\" x=\"510.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (10, 64)</text>\n</g>\n<!-- 139972354106000&#45;&gt;139972362975696 -->\n<g id=\"edge33\" class=\"edge\">\n<title>139972354106000&#45;&gt;139972362975696</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M507.9487,-242.6924C506.4178,-233.5067 504.4541,-221.7245 502.8052,-211.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"506.2136,-210.9916 501.1172,-201.7031 499.3089,-212.1424 506.2136,-210.9916\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Changing the initialization method to Kaming normal with fan_in and leaky relu, we get 89.52 accuracy.  \n",
        "It seems that the default pytorch initialization method is sligtly better than this one. Also tried kaiming normal fan out relu, but it was also slightly worse (88.53) than the default one."
      ],
      "metadata": {
        "id": "1jbNvnBoBQlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 211\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "model_hyper_params['weights_init'] = 'kaiming_normal_fan_in_lrelu'\n",
        "\n",
        "best_accuracy, train_losses, valid_losses, test_accuracies = train(\n",
        "    model_hyper_params, train_hyper_params, save_model_with_best_accuracy=True)\n",
        "\n",
        "print(f\"the best accuracy is: {best_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riTP7AbyBbas",
        "outputId": "a99d6498-1fa6-48d9-b8c8-1f0a9fb07cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropout is enabled with 0.2\n",
            "custom init\n",
            "kaiming_normal_fan_in_lrelu\n",
            "kaiming_normal_fan_in_lrelu\n",
            "kaiming_normal_fan_in_lrelu\n",
            "kaiming_normal_fan_in_lrelu\n",
            "kaiming_normal_fan_in_lrelu\n",
            "epoch 0\n",
            "epoch: 0 train_loss: 1.2838 validation loss:0.6096 test accuracy: 76.85%\n",
            "epoch 1\n",
            "epoch: 1 train_loss: 0.5978 validation loss:0.4904 test accuracy: 81.27%\n",
            "epoch 2\n",
            "epoch: 2 train_loss: 0.5082 validation loss:0.4366 test accuracy: 82.76%\n",
            "epoch 3\n",
            "epoch: 3 train_loss: 0.4661 validation loss:0.4083 test accuracy: 84.31%\n",
            "epoch 4\n",
            "epoch: 4 train_loss: 0.4388 validation loss:0.3846 test accuracy: 85.11%\n",
            "epoch 5\n",
            "epoch: 5 train_loss: 0.4175 validation loss:0.3642 test accuracy: 85.53%\n",
            "epoch 6\n",
            "epoch: 6 train_loss: 0.3996 validation loss:0.3660 test accuracy: 85.94%\n",
            "epoch 7\n",
            "epoch: 7 train_loss: 0.3872 validation loss:0.3497 test accuracy: 86.42%\n",
            "epoch 8\n",
            "epoch: 8 train_loss: 0.3739 validation loss:0.3545 test accuracy: 86.36%\n",
            "epoch 9\n",
            "epoch: 9 train_loss: 0.3679 validation loss:0.3399 test accuracy: 86.46%\n",
            "epoch 10\n",
            "epoch: 10 train_loss: 0.3548 validation loss:0.3419 test accuracy: 86.40%\n",
            "epoch 11\n",
            "epoch: 11 train_loss: 0.3432 validation loss:0.3408 test accuracy: 86.44%\n",
            "epoch 12\n",
            "epoch: 12 train_loss: 0.3393 validation loss:0.3343 test accuracy: 86.75%\n",
            "epoch 13\n",
            "epoch: 13 train_loss: 0.3342 validation loss:0.3173 test accuracy: 87.09%\n",
            "epoch 14\n",
            "epoch: 14 train_loss: 0.3249 validation loss:0.3382 test accuracy: 86.67%\n",
            "epoch 15\n",
            "epoch: 15 train_loss: 0.3198 validation loss:0.3304 test accuracy: 86.96%\n",
            "epoch 16\n",
            "epoch: 16 train_loss: 0.3161 validation loss:0.3173 test accuracy: 87.16%\n",
            "epoch 17\n",
            "epoch: 17 train_loss: 0.3081 validation loss:0.3119 test accuracy: 87.43%\n",
            "epoch 18\n",
            "epoch: 18 train_loss: 0.3061 validation loss:0.3031 test accuracy: 88.03%\n",
            "epoch 19\n",
            "epoch: 19 train_loss: 0.2979 validation loss:0.3031 test accuracy: 87.79%\n",
            "epoch 20\n",
            "epoch: 20 train_loss: 0.2931 validation loss:0.3042 test accuracy: 88.14%\n",
            "epoch 21\n",
            "epoch: 21 train_loss: 0.2907 validation loss:0.3050 test accuracy: 88.02%\n",
            "epoch 22\n",
            "epoch: 22 train_loss: 0.2820 validation loss:0.2980 test accuracy: 88.02%\n",
            "epoch 23\n",
            "epoch: 23 train_loss: 0.2812 validation loss:0.2956 test accuracy: 88.19%\n",
            "epoch 24\n",
            "epoch: 24 train_loss: 0.2770 validation loss:0.2958 test accuracy: 88.27%\n",
            "epoch 25\n",
            "epoch: 25 train_loss: 0.2727 validation loss:0.2884 test accuracy: 88.27%\n",
            "epoch 26\n",
            "epoch: 26 train_loss: 0.2673 validation loss:0.2948 test accuracy: 88.12%\n",
            "epoch 27\n",
            "epoch: 27 train_loss: 0.2662 validation loss:0.2889 test accuracy: 88.43%\n",
            "epoch 28\n",
            "epoch: 28 train_loss: 0.2573 validation loss:0.2950 test accuracy: 88.25%\n",
            "epoch 29\n",
            "epoch: 29 train_loss: 0.2575 validation loss:0.3033 test accuracy: 88.38%\n",
            "epoch 30\n",
            "epoch: 30 train_loss: 0.2534 validation loss:0.2956 test accuracy: 88.22%\n",
            "Epoch    31: reducing learning rate of group 0 to 5.0000e-03.\n",
            "epoch 31\n",
            "epoch: 31 train_loss: 0.2312 validation loss:0.2824 test accuracy: 88.94%\n",
            "epoch 32\n",
            "epoch: 32 train_loss: 0.2271 validation loss:0.2799 test accuracy: 88.84%\n",
            "epoch 33\n",
            "epoch: 33 train_loss: 0.2212 validation loss:0.2783 test accuracy: 88.77%\n",
            "epoch 34\n",
            "epoch: 34 train_loss: 0.2164 validation loss:0.2791 test accuracy: 89.11%\n",
            "epoch 35\n",
            "epoch: 35 train_loss: 0.2169 validation loss:0.2788 test accuracy: 89.19%\n",
            "epoch 36\n",
            "epoch: 36 train_loss: 0.2139 validation loss:0.2814 test accuracy: 88.82%\n",
            "epoch 37\n",
            "epoch: 37 train_loss: 0.2147 validation loss:0.2855 test accuracy: 89.22%\n",
            "epoch 38\n",
            "epoch: 38 train_loss: 0.2100 validation loss:0.2818 test accuracy: 89.07%\n",
            "epoch 39\n",
            "epoch: 39 train_loss: 0.2079 validation loss:0.2805 test accuracy: 88.87%\n",
            "epoch 40\n",
            "epoch: 40 train_loss: 0.2034 validation loss:0.2781 test accuracy: 89.28%\n",
            "epoch 41\n",
            "epoch: 41 train_loss: 0.2051 validation loss:0.2743 test accuracy: 89.25%\n",
            "epoch 42\n",
            "epoch: 42 train_loss: 0.2024 validation loss:0.2869 test accuracy: 89.35%\n",
            "epoch 43\n",
            "epoch: 43 train_loss: 0.1988 validation loss:0.2812 test accuracy: 89.08%\n",
            "epoch 44\n",
            "epoch: 44 train_loss: 0.2012 validation loss:0.2805 test accuracy: 89.31%\n",
            "epoch 45\n",
            "epoch: 45 train_loss: 0.1978 validation loss:0.2805 test accuracy: 89.44%\n",
            "epoch 46\n",
            "epoch: 46 train_loss: 0.1942 validation loss:0.2800 test accuracy: 89.25%\n",
            "epoch 47\n",
            "epoch: 47 train_loss: 0.1921 validation loss:0.2881 test accuracy: 89.10%\n",
            "epoch 48\n",
            "epoch: 48 train_loss: 0.1907 validation loss:0.2807 test accuracy: 89.41%\n",
            "Epoch    49: reducing learning rate of group 0 to 2.5000e-03.\n",
            "epoch 49\n",
            "epoch: 49 train_loss: 0.1724 validation loss:0.2841 test accuracy: 89.52%\n",
            "the best accuracy is: 89.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAINGxl0OF1o"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 3 - Design a CNN\n",
        "---\n",
        "In this task you are going to design a deep convolutional neural network to classify house number digits from the **The Street View House Numbers (SVHN)** Dataset. \n",
        "\n",
        "SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It can be seen as similar in flavor to MNIST (e.g., the images are of small cropped digits), but incorporates an order of magnitude more labeled data (over 600,000 digit images) and comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images.\n",
        "\n",
        "* 10 classes, 1 for each digit. Digit '0' has label 0, '1' has label 1,...\n",
        "* 73257 digits for training, 26032 digits for testing, and 531131 additional, somewhat less difficult samples, to use as extra training data.\n",
        "\n",
        "<img src=\"http://ufldl.stanford.edu/housenumbers/32x32eg.png\" style=\"height:250px\">\n",
        "\n",
        "1. Load the SVHN dataset with PyTorch using `torchvision.datasets.SVHN(root, split='train', transform=None, target_transform=None, download=True)`, you can read more here: https://pytorch.org/docs/stable/torchvision/datasets.html#svhn. Display 5 images from the train set.\n",
        "2. Design a Convolutional Neural Network (CNN) to classify digits from the images.\n",
        "    * Describe the chosen architecture, how many layers? What activations did you choose? What are the filter sizes? Did you use fully-connected layers (if you did, explain their sizes)?\n",
        "    * What is the input dimension? What is the output dimension?\n",
        "    * Calculate the number of parameters (weights) in the network. **Print** this number.\n",
        "3. Train the classifier (preferably on a GPU - use Colab for this part if you don't have a GPU).\n",
        "    * Describe the the hyper-parameters of the model (batch size, epochs, learning rate....). How did you tune your model? Did you use a validation set to tune the model?\n",
        "    * What is the final accuracy on the test set? **Print** it.\n",
        "        * You need to reach at least 86% accuracy in this section, and 90% for a full grade.\n",
        "    * **Plot** the loss curves (and any other statistic you want) as a function of epochs/iterations.\n",
        "4. For the trained classifier, what is the accuracy on the test set when each test image is added a small noise $a=(0.05, 0.01, 0.005)$: $$ \\text{image} + a \\times \\mathcal{N}(0, 1) $$. **Print** the result for each value of $a$.\n",
        "5. Retrain the classifier, but this time use data augementation of your choosing. Briefly explain what augmentation you chose and how it works. Did the test accuracy improve? **Print** the result.\n",
        "    * You can use transformations available in `torchvision.transforms` as shown in the tutorial.\n",
        "    * You are welcome to use <a href=\"https://kornia.github.io/\">`kornia`</a> for the augmentations.\n",
        "    * **Plot** the loss curves (and any other statistic you want) as a function of epochs/iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 3 Code and Answers - Design a CNN"
      ],
      "metadata": {
        "id": "QYTowW1Sk2x7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import copy\n",
        "\n",
        "# %matplotlib notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# check if there is gpu avilable, if there is, use it\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.current_device()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"running calculations on: \", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioRPXCDknthg",
        "outputId": "54e710ab-b07b-4eb2-a19c-96f80a83a765"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running calculations on:  cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Showing some samples:"
      ],
      "metadata": {
        "id": "y7uTu9skkKCn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "yAFN-QAXOF1o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "outputId": "611d0256-f0e3-49ac-c178-254fd2ba8b8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: ./datasets/svhn/train_32x32.mat\n",
            "Using downloaded and verified file: ./datasets/svhn/train_32x32.mat\n",
            "Using downloaded and verified file: ./datasets/svhn/test_32x32.mat\n",
            "Using downloaded and verified file: ./datasets/svhn/train_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x1440 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#3.1\n",
        "\n",
        "\n",
        "\n",
        "seed = 210\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "mean = [0.4380, 0.4440, 0.4730]\n",
        "std = [0.1751, 0.1771, 0.1744]\n",
        "\n",
        "normalize = transforms.Normalize(mean=mean, std=std)\n",
        "\n",
        "# define transforms\n",
        "valid_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "train_transform = transforms.Compose([\n",
        "    #transforms.RandomCrop(32, padding=4),\n",
        "    # up to 4 pixels translation (no rotation)\n",
        "    transforms.RandomAffine(0, translate=(0.125,0.125)),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "# load the dataset\n",
        "data_dir = './datasets/svhn/'\n",
        "train_dataset = datasets.SVHN(\n",
        "    root=data_dir, split='train', transform=train_transform, download=True)\n",
        "valid_dataset = datasets.SVHN(\n",
        "    root=data_dir, split='train', transform=valid_transform, download=True)\n",
        "test_dataset = datasets.SVHN(\n",
        "    root=data_dir, split='test', transform=valid_transform, download=True)\n",
        "\n",
        "# visualize some images\n",
        "n_samples = 5\n",
        "train_dataset_no_transform = datasets.SVHN(\n",
        "    root=data_dir, split='train', transform=transforms.ToTensor(), download=True)\n",
        "sample_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset_no_transform, batch_size=n_samples, shuffle=True)\n",
        "data_iter = iter(sample_loader)\n",
        "sampled_images, sampled_labels = data_iter.next()\n",
        "\n",
        "def convert_to_imshow_format(image):\n",
        "  image = image.numpy()\n",
        "  # convert from CHW to HWC\n",
        "  # from 3x32x32 to 32x32x3\n",
        "  return image.transpose(1,2,0)\n",
        "\n",
        "def plot_samples(samples, labels, predictions=None):\n",
        "  fig = plt.figure(figsize=(20, 20))\n",
        "  _, axes = plt.subplots(1, len(samples))\n",
        "\n",
        "  for i, ax in enumerate(axes.flat):\n",
        "    \n",
        "    ax.imshow(convert_to_imshow_format(samples[i]))\n",
        "    xlabel = str(labels[i].numpy())\n",
        "    title = f\"GT: {xlabel}\"\n",
        "    if np.any(predictions) != None:\n",
        "      xpred = str(np.argmax(predictions[i]))\n",
        "      title += f\", P: {xpred}\"\n",
        "    ax.set_title(title)    \n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_axis_off()\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "plot_samples(sampled_images, sampled_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. We've designed the following CNN model architecture.\n",
        "    * how many layers?  \n",
        "      We've split the network to two main parts. The convolution layers in one sequantial NN module and the fully connected in another.  \n",
        "      The convolutional part has 19 layers (includes Conv2d, ReLU, BatchNorm2d and Dropout2d)  \n",
        "      The FC part has 7 layers (includes Linear, ReLU and Dropout)\n",
        "      Total of 26 layers.\n",
        "    * What activations did you choose?  \n",
        "      ReLU\n",
        "    * What are the filter sizes?  \n",
        "      We used 3x3 filter sizes along the whole conv part and increased receptive feild by having stacked conv layers with the same filter size. We also didn't shrink the image channels size by adequate padding.\n",
        "    * Did you use fully-connected layers (if you did, explain their sizes)?  \n",
        "      Yes.\n",
        "      The first FC hidden layer is with the size 512 and its linear $W_l$ matrix maps the flattened output of the last convolultion layer with the size 256x4x4 = 4096 (actually after decimation by max pooling) to 512.  \n",
        "      Meaning, the 256 channels of size 4x4 are flattened and mapped to FC layer.  \n",
        "      The final output size of the FC part is of size 10 as the number of classes we classify.\n",
        "    * What is the input dimension?  \n",
        "      The input deimension is 3x32x32. Meaning, images of 32x32 pixels with 3 RGB channels.\n",
        "    * What is the output dimension?  \n",
        "      The output diemnsion is of size 10 as the number of classes.\n",
        "    * The number of parameters is 5852170. See the print below."
      ],
      "metadata": {
        "id": "h8nHODJuJiti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3.2\n",
        "\n",
        "\n",
        "class MyCNN(nn.Module):\n",
        "  def __init__(self, params):\n",
        "    super(MyCNN, self).__init__()\n",
        "    self.params = copy.deepcopy(params)\n",
        "\n",
        "    self.conv = torch.nn.Sequential()\n",
        "    for idx, layer in enumerate(self.params['conv_layers']):\n",
        "      if layer['type'] == 'Conv2d':\n",
        "        module = nn.Conv2d(\n",
        "          in_channels=layer['in_channels'], out_channels=layer['out_channels'],\n",
        "          kernel_size=layer['kernel_size'], padding=layer['padding'])\n",
        "      if layer['type'] == 'ReLU':\n",
        "        module = nn.ReLU(inplace=True)\n",
        "      if layer['type'] == 'MaxPool2d':\n",
        "        module = nn.MaxPool2d(kernel_size=layer['kernel_size'], stride=layer['stride'])\n",
        "      if layer['type'] == 'Dropout2d':\n",
        "        module = nn.Dropout2d(p=layer['p'])\n",
        "      if layer['type'] == 'BatchNorm2d':\n",
        "        module = nn.BatchNorm2d(num_features=layer['num_features'])\n",
        "      key = f\"{layer['type']}_{idx}\" \n",
        "      self.conv.add_module(key, module)\n",
        "\n",
        "    self.fc = torch.nn.Sequential()\n",
        "    for idx, layer in enumerate(self.params['fc_layers']):\n",
        "      if layer['type'] == 'Dropout':\n",
        "        module = nn.Dropout(p=layer['p'])\n",
        "      if layer['type'] == 'Linear':\n",
        "        module = nn.Linear(\n",
        "            in_features=layer['in_features'], out_features=layer['out_features'])\n",
        "      if layer['type'] == 'ReLU':\n",
        "        module = nn.ReLU(inplace=True)\n",
        "      key = f\"{layer['type']}_{idx}\" \n",
        "      self.fc.add_module(key, module)\n",
        "\n",
        "  def forward(self, x):\n",
        "      # conv layers\n",
        "      x = self.conv(x)\n",
        "\n",
        "      # flatten\n",
        "      x = x.view(x.size(0), -1)\n",
        "\n",
        "      # fc layer\n",
        "      x = self.fc(x)\n",
        "\n",
        "      return x\n"
      ],
      "metadata": {
        "id": "AdRO_95XOyb8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# building the model with model hyper parameters\n",
        "model_hyper_parameters = {\n",
        "    'conv_layers' :\n",
        "    [\n",
        "      # Conv Layer block 1\n",
        "     {'type':'Conv2d', 'in_channels':3, 'out_channels':32, 'kernel_size':3, 'padding':1},\n",
        "     {'type':'BatchNorm2d', 'num_features':32},\n",
        "     {'type':'ReLU'},\n",
        "     {'type':'Conv2d', 'in_channels':32, 'out_channels':64, 'kernel_size':3, 'padding':1},\n",
        "     {'type':'ReLU'},\n",
        "     {'type':'MaxPool2d', 'kernel_size':2, 'stride':2},\n",
        "\n",
        "      # Conv Layer block 2\n",
        "     {'type':'Conv2d', 'in_channels':64, 'out_channels':128, 'kernel_size':3, 'padding':1},\n",
        "     {'type':'BatchNorm2d', 'num_features':128},\n",
        "     {'type':'ReLU'},\n",
        "     {'type':'Conv2d', 'in_channels':128, 'out_channels':128, 'kernel_size':3, 'padding':1},\n",
        "     {'type':'ReLU'},\n",
        "     {'type':'MaxPool2d', 'kernel_size':2, 'stride':2},\n",
        "     {'type':'Dropout2d', 'p':0.05},\n",
        "\n",
        "      # Conv Layer block 3\n",
        "     {'type':'Conv2d', 'in_channels':128, 'out_channels':256, 'kernel_size':3, 'padding':1},\n",
        "     {'type':'BatchNorm2d', 'num_features':256},\n",
        "     {'type':'ReLU'},\n",
        "     {'type':'Conv2d', 'in_channels':256, 'out_channels':256, 'kernel_size':3, 'padding':1},\n",
        "     {'type':'ReLU'},\n",
        "     {'type':'MaxPool2d', 'kernel_size':2, 'stride':2}\n",
        "    ],\n",
        "    'fc_layers' :\n",
        "    [\n",
        "     {'type':'Dropout', 'p':0.1},\n",
        "     {'type':'Linear', 'in_features':4096, 'out_features':1024},\n",
        "     {'type':'ReLU'},\n",
        "     {'type':'Linear', 'in_features':1024, 'out_features':512},\n",
        "     {'type':'ReLU'},\n",
        "     {'type':'Dropout', 'p':0.1},\n",
        "     {'type':'Linear', 'in_features':512, 'out_features':10}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# we calcualte the output of the convolution layers\n",
        "dummy_input = torch.zeros([1, 3, 32, 32])\n",
        "dummy_model = MyCNN(model_hyper_parameters)\n",
        "dummy_output = dummy_model.conv(dummy_input)\n",
        "#print(dummy_output.shape)\n",
        "dummy_output = dummy_output.view(dummy_output.size(0), -1)\n",
        "#print(dummy_output.shape)\n",
        "# how many weights (trainable parameters) we have in our model?\n",
        "list_of_trainable_parameters_per_layer = [p.numel() for p in dummy_model.parameters() if p.requires_grad]\n",
        "#print(dummy_model)\n",
        "#print(list_of_trainable_parameters_per_layer)\n",
        "\n",
        "num_trainable_params = sum(list_of_trainable_parameters_per_layer)\n",
        "print(\"num trainable weights: \", num_trainable_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6REv8o1WmrZ5",
        "outputId": "01dcab6e-1467-442b-9994-f701dba81743"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num trainable weights:  5852170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Training the model\n",
        "    * We've selected the follwing hyper parameters:  \n",
        "      * batch size = 128  \n",
        "      not too small as we run batch normalization in some layers, and too small is might have more noise. Not too big in order have it more like SGD with less compute per update iteration.\n",
        "      * learning rate = 1e-4  \n",
        "      We started with what we saw in the tutuorial and used ReduceLROnPlateau LR scheduler. schduler factor 0.5 and patience 2.\n",
        "      * epochs = 20  \n",
        "      The number of epochs was selected based on empirical loss curves. once we coudln't get much improvment on test accuracy and the validation loss started rising and we were starting to overfit the training set.\n",
        "      *  optimizer selection - Adam with its default configration.\n",
        "      *  train/validation set split factor = 0.2  \n",
        "    * We used a validation set to understand when we start to overfit. We used the test accuracy as the main stopping criteria as zero-one loss is better inidcation than validation loss (which uses probabilities) as we saw in the lectures.\n",
        " "
      ],
      "metadata": {
        "id": "ZLlY4GZo4pvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# time to train our model\n",
        "\n",
        "def train_cnn(model, train_hyper_params):\n",
        "\n",
        "  # loss criterion\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # optimizer - SGD, Adam, RMSProp...\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=train_hyper_params['LEARNING_RATE'])\n",
        "\n",
        "  scheduler = ReduceLROnPlateau(\n",
        "      optimizer, mode='max', factor=train_hyper_params['SCHED_FACTOR'],\n",
        "      patience=train_hyper_params['SCHED_PAT'], verbose=True)\n",
        "\n",
        "  # training loop\n",
        "\n",
        "  epochs_train_losses = []\n",
        "  epochs_valid_losses = []\n",
        "  train_accuracies = []\n",
        "  test_accuracies = []\n",
        "\n",
        "  max_test_accuracy = 0\n",
        "\n",
        "  for epoch in range(1, train_hyper_params['NUM_OF_EPOCHS'] + 1):\n",
        "    model.train() # put in training mode\n",
        "\n",
        "    epoch_time = time.time()\n",
        "    epoch_train_losses = []\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "      # get the inputs\n",
        "      inputs, labels = data\n",
        "      # send them to device\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # forward + backward + optimize\n",
        "      outputs = model(inputs) # forward pass\n",
        "      loss = criterion(outputs, labels) # calculate the loss\n",
        "\n",
        "      # always the same 3 steps\n",
        "      optimizer.zero_grad() # zero the parameter gradients\n",
        "      loss.backward() # backpropagation\n",
        "      optimizer.step() # update parameters\n",
        "\n",
        "      # collect statistics\n",
        "      epoch_train_losses.append(loss.item())\n",
        "\n",
        "    epochs_train_losses.append(np.mean(epoch_train_losses))\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      epoch_valid_losses = []\n",
        "      for i, data in enumerate(valid_loader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        # send them to device\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs) # forward pass\n",
        "        loss = criterion(outputs, labels) # calculate the loss\n",
        "\n",
        "        # collect statistics\n",
        "        epoch_valid_losses.append(loss.item())\n",
        "\n",
        "    epochs_valid_losses.append(np.mean(epoch_valid_losses))\n",
        "\n",
        "    # Calculate training/test set accuracy of the existing model\n",
        "    train_accuracy, _ = calculate_accuracy(model, train_loader, device)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    test_accuracy, _ = calculate_accuracy(model, test_loader, device)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "    scheduler.step(test_accuracy) # accuracy is used to track down a plateau\n",
        "    \n",
        "    log = f\"Epoch: {epoch} |\"\\\n",
        "      f\" Train Loss: {epochs_train_losses[-1]:.4f} |\"\\\n",
        "      f\" Validation Loss: {epochs_valid_losses[-1]:.4f} |\"\\\n",
        "      f\" Training accuracy: {train_accuracy:.3f}% |\"\\\n",
        "      f\" Test accuracy: {test_accuracy:.3f}% | \"\n",
        "    epoch_time = time.time() - epoch_time\n",
        "    log += f\"Epoch Time: {epoch_time:.2f} secs\"\n",
        "    print(log)\n",
        "\n",
        "    # save model\n",
        "    if test_accuracy > max_test_accuracy:\n",
        "      print('==> Saving model ...')\n",
        "      state = {\n",
        "      'net': model.state_dict(),\n",
        "      'epoch': epoch,\n",
        "      }\n",
        "      torch.save(state, './checkpoints/svhn_cnn_ckpt.pth')\n",
        "      max_test_accuracy = test_accuracy\n",
        "  print('==> Finished Training ...')\n",
        "  return max_test_accuracy, epochs_train_losses, epochs_valid_losses, train_accuracies, test_accuracies\n",
        "\n",
        "# function to calcualte accuracy of the model\n",
        "def calculate_accuracy(model, dataloader, device, alpha_added_noise=0):\n",
        "  model.eval() # put in evaluation mode\n",
        "  total_correct = 0\n",
        "  total_images = 0\n",
        "  confusion_matrix = np.zeros([10,10], int)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data in dataloader:\n",
        "      images, labels = data\n",
        "      images = images.to(device)\n",
        "      if alpha_added_noise != 0:\n",
        "        noise = alpha_added_noise * torch.randn_like(images)\n",
        "        images += noise\n",
        "      labels = labels.to(device)\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total_images += labels.size(0)\n",
        "      total_correct += (predicted == labels).sum().item()\n",
        "      for i, l in enumerate(labels):\n",
        "        confusion_matrix[l.item(), predicted[i].item()] += 1\n",
        "\n",
        "  model_accuracy = total_correct / total_images * 100\n",
        "\n",
        "  return model_accuracy, confusion_matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "D-N7Faja5Ejb"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.isdir('checkpoints'):\n",
        "  os.mkdir('checkpoints')\n",
        "\n",
        "#training hyper parameters\n",
        "train_hyper_params = {}\n",
        "train_hyper_params['VALID_SIZE'] = 0.2\n",
        "train_hyper_params['BATCH_SIZE'] = 128\n",
        "train_hyper_params['LEARNING_RATE'] = 1e-4\n",
        "train_hyper_params['SCHED_FACTOR'] = 0.5\n",
        "train_hyper_params['SCHED_PAT'] = 2\n",
        "train_hyper_params['NUM_OF_EPOCHS'] = 20\n",
        "\n",
        "seed = 211\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "num_train = len(train_dataset)\n",
        "indices = list(range(num_train))\n",
        "split = int(np.floor(train_hyper_params['VALID_SIZE'] * num_train))\n",
        "\n",
        "np.random.shuffle(indices)\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=train_hyper_params['BATCH_SIZE'],\n",
        "    sampler=train_sampler, num_workers=2)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    valid_dataset, batch_size=train_hyper_params['BATCH_SIZE'],\n",
        "    sampler=valid_sampler, num_workers=2)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  test_dataset, batch_size=train_hyper_params['BATCH_SIZE'],\n",
        "  shuffle=False, num_workers=2)\n",
        "\n",
        "# build our model and send it to the device\n",
        "model = MyCNN(model_hyper_parameters).to(device)\n",
        "\n",
        "best_accuracy, train_losses, valid_losses, train_accuracies, test_accuracies = train_cnn(\n",
        "    model, train_hyper_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFQz239lSZ1W",
        "outputId": "9028800a-97b3-4b8b-cb3c-f69f4d178359"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | Train Loss: 1.1924 | Validation Loss: 0.4620 | Training accuracy: 84.688% | Test accuracy: 85.253% | Epoch Time: 89.47 secs\n",
            "==> Saving model ...\n",
            "Epoch: 2 | Train Loss: 0.4298 | Validation Loss: 0.3327 | Training accuracy: 89.853% | Test accuracy: 89.912% | Epoch Time: 89.38 secs\n",
            "==> Saving model ...\n",
            "Epoch: 3 | Train Loss: 0.3263 | Validation Loss: 0.2787 | Training accuracy: 92.177% | Test accuracy: 92.067% | Epoch Time: 89.38 secs\n",
            "==> Saving model ...\n",
            "Epoch: 4 | Train Loss: 0.2741 | Validation Loss: 0.2555 | Training accuracy: 92.895% | Test accuracy: 92.686% | Epoch Time: 89.02 secs\n",
            "==> Saving model ...\n",
            "Epoch: 5 | Train Loss: 0.2449 | Validation Loss: 0.2421 | Training accuracy: 93.951% | Test accuracy: 93.158% | Epoch Time: 88.89 secs\n",
            "==> Saving model ...\n",
            "Epoch: 6 | Train Loss: 0.2249 | Validation Loss: 0.2358 | Training accuracy: 94.482% | Test accuracy: 93.608% | Epoch Time: 88.86 secs\n",
            "==> Saving model ...\n",
            "Epoch: 7 | Train Loss: 0.2032 | Validation Loss: 0.2257 | Training accuracy: 94.728% | Test accuracy: 94.211% | Epoch Time: 88.81 secs\n",
            "==> Saving model ...\n",
            "Epoch: 8 | Train Loss: 0.1919 | Validation Loss: 0.2150 | Training accuracy: 95.371% | Test accuracy: 94.380% | Epoch Time: 88.94 secs\n",
            "==> Saving model ...\n",
            "Epoch: 9 | Train Loss: 0.1764 | Validation Loss: 0.2126 | Training accuracy: 95.688% | Test accuracy: 94.372% | Epoch Time: 88.74 secs\n",
            "Epoch: 10 | Train Loss: 0.1671 | Validation Loss: 0.2074 | Training accuracy: 95.994% | Test accuracy: 94.776% | Epoch Time: 88.74 secs\n",
            "==> Saving model ...\n",
            "Epoch: 11 | Train Loss: 0.1576 | Validation Loss: 0.2086 | Training accuracy: 96.425% | Test accuracy: 94.584% | Epoch Time: 88.83 secs\n",
            "Epoch: 12 | Train Loss: 0.1495 | Validation Loss: 0.2115 | Training accuracy: 96.465% | Test accuracy: 94.710% | Epoch Time: 88.54 secs\n",
            "Epoch    13: reducing learning rate of group 0 to 5.0000e-05.\n",
            "Epoch: 13 | Train Loss: 0.1372 | Validation Loss: 0.2078 | Training accuracy: 96.835% | Test accuracy: 94.634% | Epoch Time: 88.41 secs\n",
            "Epoch: 14 | Train Loss: 0.1111 | Validation Loss: 0.2002 | Training accuracy: 97.715% | Test accuracy: 95.094% | Epoch Time: 88.19 secs\n",
            "==> Saving model ...\n",
            "Epoch: 15 | Train Loss: 0.1038 | Validation Loss: 0.2024 | Training accuracy: 97.952% | Test accuracy: 95.214% | Epoch Time: 88.48 secs\n",
            "==> Saving model ...\n",
            "Epoch: 16 | Train Loss: 0.0975 | Validation Loss: 0.1961 | Training accuracy: 98.096% | Test accuracy: 95.271% | Epoch Time: 88.30 secs\n",
            "==> Saving model ...\n",
            "Epoch: 17 | Train Loss: 0.0903 | Validation Loss: 0.2061 | Training accuracy: 98.188% | Test accuracy: 95.264% | Epoch Time: 88.22 secs\n",
            "Epoch: 18 | Train Loss: 0.0859 | Validation Loss: 0.2116 | Training accuracy: 98.330% | Test accuracy: 95.356% | Epoch Time: 88.21 secs\n",
            "==> Saving model ...\n",
            "Epoch: 19 | Train Loss: 0.0837 | Validation Loss: 0.2080 | Training accuracy: 98.425% | Test accuracy: 95.248% | Epoch Time: 88.22 secs\n",
            "Epoch: 20 | Train Loss: 0.0772 | Validation Loss: 0.2062 | Training accuracy: 98.521% | Test accuracy: 95.271% | Epoch Time: 88.40 secs\n",
            "==> Finished Training ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   * The final best  test set accuracy point we recorded between epochs was 95.394. see below the output."
      ],
      "metadata": {
        "id": "WUsvdU3OcQ_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"the best accuracy recorded along epochs {best_accuracy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqOOswRocaBU",
        "outputId": "4fcb7fc8-bc5c-40d6-a464-cf4158eb7f9c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the best accuracy recorded along epochs 95.356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The train and validation losses as a function of the epochs:\n",
        "(at start the train set loss is much higher as it is done before the weights are updated, while the validation loss is calculated based on the already updated ones. even so, the validation loss at some point becomes higher than the training set, as we start to overfit.)"
      ],
      "metadata": {
        "id": "H6fWBDOBc027"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses, label='train losses')\n",
        "plt.plot(valid_losses, label='valid losses')\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "JR7cq8s0c8_g",
        "outputId": "5d71210b-6dcf-41ef-c1b8-9fa92feb48e7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f7bed0b8250>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The train and test accuracies vs epochs:"
      ],
      "metadata": {
        "id": "nLZqm0A8dzHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_accuracies, label='train accuracies')\n",
        "plt.plot(test_accuracies, label='test accuracies')\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "NvEmGyk5duhr",
        "outputId": "9cf2cc8e-c029-4a59-9bde-2ab7190a5d74"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f7bec64b690>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. adding noise to the test data set and the resulted test accuracy:"
      ],
      "metadata": {
        "id": "aARU_LdSAhBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "alphas = [0.05, 0.01, 0.005]\n",
        "for alpha in alphas:\n",
        "  test_accuracy, _ = calculate_accuracy(model, test_loader, device, alpha_added_noise=alpha)\n",
        "  display(Markdown(rf\"\"\"For $\\alpha={alpha}$, Test set accuracy ={test_accuracy:.3f}\"\"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "pdAuFYXJAriF",
        "outputId": "47352591-0183-454f-fc7a-c3d85a8c78ba"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "For $\\alpha=0.05$, Test set accuracy =95.152",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "For $\\alpha=0.01$, Test set accuracy =95.298",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "For $\\alpha=0.005$, Test set accuracy =95.264",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. We actually added some augmentations already before and probably it contributed to the high accuracy. We now add some noise augmentation and check the influence."
      ],
      "metadata": {
        "id": "gDl2624LYqS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform_more = transforms.Compose([\n",
        "    #transforms.RandomCrop(32, padding=4),\n",
        "    # up to 4 pixels translation (no rotation)\n",
        "    transforms.ColorJitter(0.1,0.1,0.1,0.1),\n",
        "    transforms.RandomAffine(0, translate=(0.125,0.125)),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "# load the dataset\n",
        "data_dir = './datasets/svhn/'\n",
        "train_dataset = datasets.SVHN(\n",
        "    root=data_dir, split='train', transform=train_transform_more, download=True)\n",
        "\n",
        "# build our model and send it to the device\n",
        "model2 = MyCNN(model_hyper_parameters).to(device)\n",
        "\n",
        "best_accuracy, train_losses, valid_losses, train_accuracies, test_accuracies = train_cnn(\n",
        "    model2, train_hyper_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9goOPYcZBEB",
        "outputId": "49bb24ad-a170-4a18-b19f-45dcc27ee2d5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: ./datasets/svhn/train_32x32.mat\n",
            "Epoch: 1 | Train Loss: 1.1169 | Validation Loss: 0.4673 | Training accuracy: 83.966% | Test accuracy: 85.437% | Epoch Time: 91.18 secs\n",
            "==> Saving model ...\n",
            "Epoch: 2 | Train Loss: 0.4292 | Validation Loss: 0.3570 | Training accuracy: 89.051% | Test accuracy: 89.252% | Epoch Time: 89.42 secs\n",
            "==> Saving model ...\n",
            "Epoch: 3 | Train Loss: 0.3279 | Validation Loss: 0.2913 | Training accuracy: 91.793% | Test accuracy: 91.580% | Epoch Time: 89.18 secs\n",
            "==> Saving model ...\n",
            "Epoch: 4 | Train Loss: 0.2783 | Validation Loss: 0.2517 | Training accuracy: 93.115% | Test accuracy: 93.009% | Epoch Time: 89.47 secs\n",
            "==> Saving model ...\n",
            "Epoch: 5 | Train Loss: 0.2490 | Validation Loss: 0.2434 | Training accuracy: 93.729% | Test accuracy: 93.285% | Epoch Time: 89.11 secs\n",
            "==> Saving model ...\n",
            "Epoch: 6 | Train Loss: 0.2258 | Validation Loss: 0.2337 | Training accuracy: 94.547% | Test accuracy: 93.815% | Epoch Time: 89.15 secs\n",
            "==> Saving model ...\n",
            "Epoch: 7 | Train Loss: 0.2092 | Validation Loss: 0.2200 | Training accuracy: 95.041% | Test accuracy: 94.119% | Epoch Time: 89.21 secs\n",
            "==> Saving model ...\n",
            "Epoch: 8 | Train Loss: 0.1922 | Validation Loss: 0.2260 | Training accuracy: 94.956% | Test accuracy: 94.134% | Epoch Time: 88.93 secs\n",
            "==> Saving model ...\n",
            "Epoch: 9 | Train Loss: 0.1799 | Validation Loss: 0.2145 | Training accuracy: 95.468% | Test accuracy: 94.334% | Epoch Time: 89.25 secs\n",
            "==> Saving model ...\n",
            "Epoch: 10 | Train Loss: 0.1679 | Validation Loss: 0.2178 | Training accuracy: 96.021% | Test accuracy: 94.687% | Epoch Time: 89.04 secs\n",
            "==> Saving model ...\n",
            "Epoch: 11 | Train Loss: 0.1604 | Validation Loss: 0.2124 | Training accuracy: 96.014% | Test accuracy: 94.361% | Epoch Time: 89.47 secs\n",
            "Epoch: 12 | Train Loss: 0.1493 | Validation Loss: 0.2042 | Training accuracy: 96.872% | Test accuracy: 94.910% | Epoch Time: 88.79 secs\n",
            "==> Saving model ...\n",
            "Epoch: 13 | Train Loss: 0.1397 | Validation Loss: 0.2047 | Training accuracy: 96.668% | Test accuracy: 94.776% | Epoch Time: 88.95 secs\n",
            "Epoch: 14 | Train Loss: 0.1297 | Validation Loss: 0.2053 | Training accuracy: 97.055% | Test accuracy: 94.902% | Epoch Time: 88.62 secs\n",
            "Epoch    15: reducing learning rate of group 0 to 5.0000e-05.\n",
            "Epoch: 15 | Train Loss: 0.1227 | Validation Loss: 0.2042 | Training accuracy: 97.360% | Test accuracy: 94.676% | Epoch Time: 88.57 secs\n",
            "Epoch: 16 | Train Loss: 0.0974 | Validation Loss: 0.2007 | Training accuracy: 97.968% | Test accuracy: 95.482% | Epoch Time: 88.76 secs\n",
            "==> Saving model ...\n",
            "Epoch: 17 | Train Loss: 0.0895 | Validation Loss: 0.1943 | Training accuracy: 98.307% | Test accuracy: 95.379% | Epoch Time: 88.44 secs\n",
            "Epoch: 18 | Train Loss: 0.0835 | Validation Loss: 0.2028 | Training accuracy: 98.309% | Test accuracy: 95.260% | Epoch Time: 88.56 secs\n",
            "Epoch    19: reducing learning rate of group 0 to 2.5000e-05.\n",
            "Epoch: 19 | Train Loss: 0.0785 | Validation Loss: 0.2103 | Training accuracy: 98.481% | Test accuracy: 95.306% | Epoch Time: 88.58 secs\n",
            "Epoch: 20 | Train Loss: 0.0664 | Validation Loss: 0.2036 | Training accuracy: 98.823% | Test accuracy: 95.571% | Epoch Time: 88.33 secs\n",
            "==> Saving model ...\n",
            "==> Finished Training ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyAGHBuqOF1o"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
        "---\n",
        "* Icons made by <a href=\"https://www.flaticon.com/authors/becris\" title=\"Becris\">Becris</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\">www.flaticon.com</a>\n",
        "* Icons from <a href=\"https://icons8.com/\">Icons8.com</a> - https://icons8.com\n",
        "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "name": "ee046211_hw2_034462796_204034953.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}