{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzV9wsJ5pGhf"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/bubbles/50/000000/mind-map.png\" style=\"height:50px;display:inline\"> EE 046211 - Technion - Deep Learning\n",
        "---\n",
        "\n",
        "## HW2 - Multilayer NNs and Convolutional NNs\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq2c8X93pGhh"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/clouds/96/000000/keyboard.png\" style=\"height:50px;display:inline\"> Keyboard Shortcuts\n",
        "---\n",
        "* Run current cell: **Ctrl + Enter**\n",
        "* Run current cell and move to the next: **Shift + Enter**\n",
        "* Show lines in a code cell: **Esc + L**\n",
        "* View function documentation: **Shift + Tab** inside the parenthesis or `help(name_of_module)`\n",
        "* New cell below: **Esc + B**\n",
        "* Delete cell: **Esc + D, D** (two D's)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZZybn3NpGhh"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Students Information\n",
        "---\n",
        "* Fill in\n",
        "\n",
        "|Name         |Campus Email                     | ID       |\n",
        "|-------------|---------------------------------|----------|\n",
        "|Lior Friedman| liorf@campus.technion.ac.il     | 204034953|\n",
        "|Yair Nahum   | nahum.yair@campus.technion.ac.il| 034462796|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDK5zqhdpGhi"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
        "---\n",
        "* Maximal garde: 100.\n",
        "* Submission only in **pairs**. \n",
        "    * Please make sure you have registered your group in Moodle (there is a group creation component on the Moodle where you need to create your group and assign members).\n",
        "* **No handwritten submissions.** You can choose whether to answer in a Markdown cell in this notebook or attach a PDF with your answers.\n",
        "* <a style='color:red'> SAVE THE NOTEBOOKS WITH THE OUTPUT, CODE CELLS THAT WERE NOT RUN WILL NOT GET ANY POINTS! </a>\n",
        "* What you have to submit:\n",
        "    * If you have answered the questions in the notebook, you should submit this file only, with the name: `ee046211_hw2_id1_id2.ipynb`.\n",
        "    * If you answered the questions in a different file you should submit a `.zip` file with the name `ee046211_hw2_id1_id2.zip` with content:\n",
        "        * `ee046211_hw2_id1_id2.ipynb` - the code tasks\n",
        "        * `ee046211_hw2_id1_id2.pdf` - answers to questions.\n",
        "    * No other file-types (`.py`, `.docx`...) will be accepted.\n",
        "* Submission on the course website (Moodle).\n",
        "* **Latex in Colab** - in some cases, Latex equations may no be rendered. To avoid this, make sure to not use *bullets* in your answers (\"* some text here with Latex equations\" -> \"some text here with Latex equations\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmSj_UufpGhi"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/dusk/64/000000/online.png\" style=\"height:50px;display:inline\"> Working Online and Locally\n",
        "---\n",
        "* You can choose your working environment:\n",
        "    1. `Jupyter Notebook`, **locally** with <a href=\"https://www.anaconda.com/distribution/\">Anaconda</a> or **online** on <a href=\"https://colab.research.google.com/\">Google Colab</a>\n",
        "        * Colab also supports running code on GPU, so if you don't have one, Colab is the way to go. To enable GPU on Colab, in the menu: `Runtime`$\\rightarrow$ `Change Runtime Type` $\\rightarrow$`GPU`.\n",
        "    2. Python IDE such as <a href=\"https://www.jetbrains.com/pycharm/\">PyCharm</a> or <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a>.\n",
        "        * Both allow editing and running Jupyter Notebooks.\n",
        "\n",
        "* Please refer to `Setting Up the Working Environment.pdf` on the Moodle or our GitHub (https://github.com/taldatech/ee046211-deep-learning) to help you get everything installed.\n",
        "* If you need any technical assistance, please go to our Piazza forum (`hw2` folder) and describe your problem (preferably with images)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlp1Fp4ppGhj"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
        "---\n",
        "\n",
        "* [Part 1 - Theory](#-Part-1---Theory)\n",
        "    * [Q1 - Generalization in A Teacher-Student Setup](#-Question-1--Generalization-in-A-Teacher-Student-Setup)\n",
        "    * [Q2 - Backpropagation By Hand](#-Question-2---Backpropagation-By-Hand)\n",
        "    * [Q3 - Deep Double Descent](#-Question-3---Deep-Double-Descent)\n",
        "    * [Q4 - Initialization](#-Question-4---Initialization)\n",
        "    * [Q5 - MLP and Invaraince](#-Question-5---MLP-and-Invaraince)\n",
        "    * [Q6 - VGG Architecture](#-Question-6--VGG-Architecture)\n",
        "* [Part 2 - Code Assignments](#-Part-2---Code-Assignments)\n",
        "    * [Task 1 - The Importance of Activation and Initialization](#-Task-1---The-Importance-of-Activation-and-Initialization)\n",
        "    * [Task 2 - FashionMNIST Deep Classifer](#-Task-2---FashionMNIST-Deep-Classifer)\n",
        "    * [Task 3 - Design a CNN](#-Task-3---Design-a-CNN)\n",
        "* [Credits](#-Credits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKtSiQX_pGhj"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/cute-clipart/64/000000/ball-point-pen.png\" style=\"height:50px;display:inline\"> Part 1 - Theory\n",
        "---\n",
        "* You can choose whether to answser these straight in the notebook (Markdown + Latex) or use another editor (Word, LyX, Latex, Overleaf...) and submit an additional PDF file, **but no handwritten submissions**.\n",
        "* You can attach additional figures (drawings, graphs,...) in a separate PDF file, just make sure to refer to them in your answers.\n",
        "\n",
        "* $\\large\\LaTeX$ <a href=\"https://kapeli.com/cheat_sheets/LaTeX_Math_Symbols.docset/Contents/Resources/Documents/index\">Cheat-Sheet</a> (to write equations)\n",
        "    * <a href=\"http://tug.ctan.org/info/latex-refsheet/LaTeX_RefSheet.pdf\">Another Cheat-Sheet</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsqSFZG1pGhj"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 1 -Generalization in A Teacher-Student Setup\n",
        "---\n",
        "\n",
        "Recall from lecture 4 the Risk $\\mathcal{R}(w)$: $$ \\mathcal{R}(w) \\triangleq \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) } \\left[ ||w^Tx^{(0)} - w_t^Tx^{(0)}||^2 \\right] $$\n",
        "\n",
        "Prove:\n",
        "\n",
        "$$ \\mathcal{R}(w) = ||w-w_t||^2 $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%%md\n"
        },
        "id": "CjAgwlEAOF1I"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/fluency/48/000000/exclamation-mark.png\" style=\"height:50px;display:inline\">  Answer 1 - Generalization in A Teacher-Student Setup \n",
        "---\n",
        "Proof:  \n",
        "$$ \\mathcal{R}(w) \\triangleq \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) } \\left[ ||w^Tx^{(0)} - w_t^Tx^{(0)}||^2 \\right]$$\n",
        "$$= \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) } \\left[ ||{x^{(0)}}^T(w - w_t)||^2 \\right]$$\n",
        "$$\\underbrace{=}_{(1)} \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) } \\left[ (w - w_t)^T x^{(0)} {x^{(0)}}^T(w - w_t) \\right]$$\n",
        "$$\\underbrace{=}_{(2)} (w - w_t)^T \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) } \\left[  x^{(0)} {x^{(0)}}^T \\right] (w - w_t)$$\n",
        "$$\\underbrace{=}_{(3)} (w - w_t)^T I (w - w_t) \\underbrace{=}_{(4)} ||w - w_t||^2$$\n",
        "Where (1) and (4) are due to the L2 norm definition, (2) is due to $(w-w_t)$ is constant relative to the expectation and (3) is due to the fact that the covariance matrix is $I$.  \n",
        "$\\blacksquare$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StkdA5hUOF1K"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 2 - Backpropagation By Hand\n",
        "---\n",
        "Consider the following network:\n",
        "<img src=\"https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/backprop_by_hand_ex1.png\" style=\"height:300px\">\n",
        "\n",
        "We will work with one sample for this example, but it can be extended to mini-batches.\n",
        "\n",
        "* Input: $x = \\begin{bmatrix} 1 \\\\ 4 \\\\ 5 \\end{bmatrix} \\in \\mathbb{R}^3$\n",
        "* Output (target): $ t = \\begin{bmatrix} 0.1 \\\\ 0.05 \\end{bmatrix} \\in \\mathbb{R}^2 $\n",
        "* Number of Hidden Layers: 1\n",
        "* Activation: Sigmoid for both hidden and output layers\n",
        "* Loss Functions: MSE\n",
        "\n",
        "We initialize the weights and biases to random values as follows:\n",
        "<img src=\"https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/backprop_by_hand_ex2.png\" style=\"height:300px\">\n",
        "\n",
        "1. Perform one forward pass and calculate the MSE.\n",
        "2. Perform backpropagation (one backward pass, i.e., calculate the gradients).\n",
        "3. With a learning rate of $\\alpha = 0.01$, what are the new values of the weights after performing the forward pass and backward pass (assume we use SGD)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJZk_9OxOF1M"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/fluency/48/000000/exclamation-mark.png\" style=\"height:50px;display:inline\"> Answer 2 -Backprop by hand\n",
        "\n",
        "1. All calculations rounded to 2 significant digits\n",
        "\n",
        "$$ h_1(x)=S(0.1\\cdot 1+0.3\\cdot 4+0.5\\cdot 5+0.5\\cdot 1)=S(4.3)=0.99$$\n",
        "$$ h_2(x)=S(0.2\\cdot 1+0.4\\cdot 4+0.6\\cdot 5+0.5\\cdot 1)=S(5.3)=1$$\n",
        "$$ o_1(h(x))=S(0.7\\cdot h_1(x)+0.9\\cdot h_2(x)+0.5\\cdot 1)=S(2.093)=0.89$$\n",
        "$$ o_2(h(x))=S(0.8\\cdot h_1(x)+0.1\\cdot h_2(x)+0.5\\cdot 1)=S(1.392)=0.8$$\n",
        "$$ MSE = (0.1-0.89)^2+(0.05-0.8)^2=1.19$$\n",
        "\n",
        "2. \n",
        "\n",
        "$$\\frac{d\\mathcal{L}}{do_1}=2(o_1-t_1)=1.58, \\frac{d\\mathcal{L}}{do_2}=2(o_2-t_2)=1.5$$\n",
        "\n",
        "$$\\frac{d\\mathcal{L}}{dw_7}=\\frac{d\\mathcal{L}}{do_1}\\frac{do_1}{dw_7}=1.58\\cdot S'(o_1(h(x)))h_1(x)=1.58\\cdot 0.206\\cdot 0.99=0.32$$\n",
        "$$\\frac{d\\mathcal{L}}{dw_8}=\\frac{d\\mathcal{L}}{do_2}\\frac{do_2}{dw_8}=1.5\\cdot S'(o_2(h(x)))h_1(x)=1.5\\cdot 0.214\\cdot 0.99=0.32$$\n",
        "$$\\frac{d\\mathcal{L}}{dw_9}=\\frac{d\\mathcal{L}}{do_1}\\frac{do_1}{dw_9}=1.58\\cdot S'(o_1(h(x)))h_2(x)=1.58\\cdot 0.206\\cdot 1=0.33$$\n",
        "$$\\frac{d\\mathcal{L}}{dw_{10}}=\\frac{d\\mathcal{L}}{do_2}\\frac{do_2}{dw_{10}}=1.5\\cdot S'(o_2(h(x)))h_2(x)=1.5\\cdot 0.214\\cdot 1=0.32$$\n",
        "\n",
        "$$\\frac{d\\mathcal{L}}{dh_1}=\\frac{d\\mathcal{L}}{do_1}\\frac{do_1}{dh_1}+\\frac{d\\mathcal{L}}{do_2}\\frac{do_2}{dh_1}=1.58\\cdot S'(o_1(h(x)))w_7+1.5\\cdot S'(o_2(h(x)))w_8\\\\=1.58\\cdot 0.206\\cdot 0.7+1.5\\cdot 0.214\\cdot 0.8 =0.228+0.257=0.48$$\n",
        "\n",
        "$$\\frac{d\\mathcal{L}}{dh_2}=\\frac{d\\mathcal{L}}{do_1}\\frac{do_1}{dh_2}+\\frac{d\\mathcal{L}}{do_2}\\frac{do_2}{dh_2}=1.58\\cdot S'(o_1(h(x)))w_9+1.5\\cdot S'(o_2(h(x)))w_{10}\\\\=1.58\\cdot 0.206\\cdot 0.9+1.5\\cdot 0.214\\cdot 0.1 =0.293+0.032=0.33$$\n",
        "\n",
        "$$\\frac{d\\mathcal{L}}{dw_1}=\\frac{d\\mathcal{L}}{dh_1}\\frac{dh_1}{dw_1}=0.48\\cdot S'(h_1(x))x_1=0.48\\cdot 0.197\\cdot 1=0.09$$\n",
        "$$\\frac{d\\mathcal{L}}{dw_2}=\\frac{d\\mathcal{L}}{dh_2}\\frac{dh_2}{dw_1}=0.33\\cdot S'(h_2(x))x_1=0.33\\cdot 0.197\\cdot 1=0.07$$\n",
        "$$\\frac{d\\mathcal{L}}{dw_3}=\\frac{d\\mathcal{L}}{dh_1}\\frac{dh_1}{dw_3}=0.48\\cdot S'(h_1(x))x_2=0.48\\cdot 0.197\\cdot 4=0.38$$\n",
        "$$\\frac{d\\mathcal{L}}{dw_4}=\\frac{d\\mathcal{L}}{dh_2}\\frac{dh_2}{dw_4}=0.33\\cdot S'(h_2(x))x_2=0.33\\cdot 0.197\\cdot 4=0.26$$\n",
        "$$\\frac{d\\mathcal{L}}{dw_5}=\\frac{d\\mathcal{L}}{dh_1}\\frac{dh_1}{dw_5}=0.48\\cdot S'(h_1(x))x_3=0.48\\cdot 0.197\\cdot 5=0.47$$\n",
        "$$\\frac{d\\mathcal{L}}{dw_6}=\\frac{d\\mathcal{L}}{dh_2}\\frac{dh_2}{dw_6}=0.33\\cdot S'(h_2(x))x_3=0.33\\cdot 0.197\\cdot 5=0.33$$\n",
        "\n",
        "\n",
        "$$\\nabla_w \\mathcal{L}=\\begin{bmatrix} 0.09 \\\\ 0.07 \\\\ 0.38 \\\\ 0.26 \\\\0.47 \\\\0.33 \\\\0.32 \\\\0.32\\\\0.33 \\\\ 0.32\\end{bmatrix}$$\n",
        "\n",
        "3. \n",
        "$$w_{t+1}=w_t-\\alpha \\nabla_w \\mathcal{L}(w_t)=\\begin{bmatrix} 0.1 \\\\ 0.2 \\\\ 0.3 \\\\ 0.4 \\\\0.5 \\\\0.6 \\\\0.7 \\\\0.8 \\\\0.9 \\\\ 0.1\\end{bmatrix}-0.01\\begin{bmatrix} 0.09 \\\\ 0.07 \\\\ 0.38 \\\\ 0.26 \\\\0.47 \\\\0.33 \\\\0.32 \\\\0.32\\\\0.33 \\\\ 0.32\\end{bmatrix}=\\begin{bmatrix} 0.099 \\\\ 0.199 \\\\ 0.296 \\\\ 0.397 \\\\0.495 \\\\0.597 \\\\0.697 \\\\0.797 \\\\0.897 \\\\ 0.097\\end{bmatrix}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjO6xMRAOF1N"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 3 - Deep Double Descent\n",
        "---\n",
        "\n",
        "For the following plots:\n",
        "1. Where is the critical point (the point of transition between the \"Classical Regime\" and \"Modern Regime\") of the deep double descent?\n",
        "2. What type of double descent is shown? Explain.\n",
        "    \n",
        "\n",
        "a. <img src='https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/double_descent_transformer.PNG' style=\"height:300px\">\n",
        "\n",
        "b. <img src='https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/double_descent_resnet.PNG' style=\"height:300px\">\n",
        "\n",
        "c. <img src='https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/double_descent_intermediate.PNG' style=\"height:300px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJvVLhojOF1O"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/fluency/48/000000/exclamation-mark.png\" style=\"height:50px;display:inline\"> Answer 3 -Deep Double Decent\n",
        "\n",
        "a. \n",
        "The critical point is around dimension=200 for the blue (German-English) model and around dimension=250 for the green (English-French) model. This is an example of model-wise double decent as we see that larger models have worse test error until a critical region is reached and the trend reverses. \n",
        "\n",
        "b. The critical point is around width=10 for the least noisy case and around width=13 for the highest noise.  \n",
        "This is an example of model-wise double decent, and we also see that adding label noise causes the critical region to move further away.  \n",
        "As the noise increases, the SNR and thus the optimal step size decreases. This effectively increases the dimensionallity of the data and moves the interpolation threshold location ($\\gamma=\\frac{d}{n}=1$ requires bigger dimensioanlity of the model parameters).\n",
        "\n",
        "c. Here we see that the large model at around 100 epochs. This is an example of epoch-wise deep double decent, as increasing the training time reverses the overfitting in the test error. The small and intermediate models does not exhibit deep double decent. This effect is noticable in large models.  \n",
        "In intermediate sized models, we see the classical U shape of ML that requires early stopping.  \n",
        "In small sized models, we stay in the biased location, where increasing the training time improves the test error.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU-XkJf5OF1P"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 4 - Initialization\n",
        "---\n",
        "\n",
        "Recall that in lecture 5 we were discussing how to calculate the initialization variance, and reached the conclusion that $$ \\sigma_l =\\frac{1}{\\sqrt{d_{l-1}\\mathbb{E}_{z\\sim \\mathcal{N}(0, 1)} \\left[\\varphi^2(z)\\right]}} $$\n",
        "Show that for ReLU activation ($\\varphi(z) = max(0,z)$), the optimal variance satisfies: $$ \\sigma_l = \\sqrt{\\frac{2}{d_{l-1}}}$$\n",
        "\n",
        "All the notations are the same as in the lecture slides."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoOZF5z4OF1Q"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/fluency/48/000000/exclamation-mark.png\" style=\"height:50px;display:inline\"> Answer 4 -Initialization\n",
        "\n",
        "We assume that $\\forall i, \\mathbb{E}\\left [u_{l-1}[i]\\right ]=0, \\text{Var}(u_{l-1}[i])=1$\n",
        "From the lecture we know that under our assumptions for $W_{l-1}$,\n",
        "\n",
        "$$\\text{Var}(u_l[i])=\\sigma_l^2\\sum_{j=1}^{d_{l-1}}\\mathbb{E}\\left [\\varphi^2 (u_{l-1}[i])\\right ]$$\n",
        "\n",
        "For ReLU, $\\varphi^2(z)=\\begin{cases} z^2 & z>0 \\\\ 0 & \\text{otherwise} \\end{cases}$  \n",
        "Moreover, for Relu we don't need to assume/use the central limit theorem for any symetric PDF $f(u_{l-1}[i]) = f(-u_{l-1}[i])$ for example $u_{l-1} \\sim \\mathcal{N}(0,\\sigma^{2}I)$ or $u_{l-1}[i] \\sim \\mathcal{Uniform}(-1,1)$.\n",
        "\n",
        "$$\\mathbb{E}\\left [\\varphi^2 (u_{l-1}[i])\\right ] = \\int \\varphi^2 (u_{l-1}[i])f(u_{l-1}[i])du_{l-1}[i] = \\int_{0}^{\\infty} u_{l-1}[i]^2 f(u_{l-1}[i]) du_{l-1}[i] \\underbrace{=}_{(1)} \\frac{1}{2}\\int_{-\\infty}^{\\infty} u_{l-1}[i]^2 f(u_{l-1}[i]) du_{l-1}[i]=\\frac{1}{2}\\mathbb{E}\\left [u_{l-1}[i]^2\\right ]\\underbrace{=}_{(2)}\\frac{1}{2}\\text{Var}(u_{l-1}[i])\\underbrace{=}_{(3)}\\frac{1}{2}  $$\n",
        "\n",
        "The transitions are explained as follows:  \n",
        "$(1)$ is due to the fact that the integrand is an even/symetric function.  \n",
        "$(2)$ is due to the fact that $\\mathbb{E}\\left [u_{l-1}[i]\\right ]=0 \\Rightarrow  \\text{Var}(u_{l-1}[i]) = \\mathbb{E}\\left [u_{l-1}[i]^2\\right ] - \\mathbb{E}\\left [u_{l-1}[i]\\right ]^2 = \\mathbb{E}\\left [u_{l-1}[i]^2\\right ]$   \n",
        "And $(3)$ since $\\text{Var}(u_{l-1}[i])=1 (by the induction step over layers and inputs)$   \n",
        "\n",
        "From this we get $$\\text{Var}(u_l[i])=\\sigma_l^2\\sum_{j=1}^{d_{l-1}}\\frac{1}{2}=1$$\n",
        "\n",
        "$$\\frac{1}{2}\\sigma_l^2 d_{l-1}=1$$\n",
        "\n",
        "$$\\Rightarrow \\sigma_l = \\sqrt{\\frac{2}{d_{l-1}}}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eurEw1szOF1Q"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 5 - MLP and Invaraince\n",
        "---\n",
        "\n",
        "You have to design an MLP with the following input: DNA sequences of length $d$. The DNA is a sequence of bases, where each base can be one of 4 options: $(C, T, G, A)$. Thus, the input can be described as the following matrix: $$ X \\in \\mathcal{R}^{4 \\times d}, $$ where $X[j,i]$ denotes the measured value of base concentration of the $j^{th}$ base at location $i$. \n",
        "\n",
        "The network should output a **binary** classification $y \\in \\{-1, 1\\}$ for a specific property we wish to find. The network will be trained on samples $\\{X^{(n)}, y^{(n)} \\}_{n=1}^{N}$, with a **logistic loss function**.\n",
        "\n",
        "First, we will examine a network with 1 hidden layerof size $4 \\times d$ and a **LeakyReLU** activation $\\phi$: $$ f_w(X) = \\sum_{r=1}^{4}\\sum_{k=1}^d W_2[r,k]\\phi\\left(\\sum_{j=1}^{4}\\sum_{i=1}^d W_1[r, k,j, i]X[j, i] \\right),$$ where $w=\\{W_1, W_2\\}$ are the layers of the weight **tensors**. After training is done, the classification will be done with $\\text{sign}(f(X))$.\n",
        "\n",
        "1. Which invariances exist in the network's parameters?\n",
        "2. Now, we notice the fact that: the *direction* in which the DNA is scanned is arbitrary. Thus, if for two inputs $X, \\tilde{X}$: $$ \\forall i,j: \\: X[j,i] = \\tilde{X}[j, d-i], $$ then the two inputs are **equivalent** in their meaning. What constraints should we put on the network's parameters to improve the network's classification performance?\n",
        "3. After that, we now recall that the DNA bases come in pairs, and thus if for two inputs $X, \\tilde{X}$: $$ \\forall i,j : \\: X[j,i] = \\tilde{X}[(j+2)\\text{mod}4,i], $$ then the two inputs are **equivalent** in their meaning. What constraints should we put on the network's parameters to improve the network's classification performance?\n",
        "4. We now notice that the measurement process in noisy, each sample $X^{(n)}$ is in arbitrary scale, and thus if for two $X, \\tilde{X}$: $$ \\forall i,j: \\: X[j,i] = c\\tilde{X}[j,i], $$ for some constant $c>0$, then the two inputs are **equivalent** in their meaning.\n",
        "    * (a) For the given network, that **is already trained**, what is the effect of the scale $c$ on the classification result?\n",
        "    * (b) Can the arbitrary scale hurt the training process? Hint: think what happens to the gradient of each sample.\n",
        "    * (c) How can use this information to improve the classifier performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJb5wlHSOF1R"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/fluency/48/000000/exclamation-mark.png\" style=\"height:50px;display:inline\"> Answer 5 - MLP and Invaraince\n",
        "\n",
        "1. The LeakyReLU activation is rescale symmetric, so there is rescale invariance between $z_1=W_1X$ and $\\phi(z_1)$. Meaning, $\\forall c >0$ if we multiply $W_1$ by $c$ and multiply  $W_2$ by $\\frac{1}{c}$ we get the same function.  \n",
        "The MNN is also invariant to permutations on the nuerons with some permutation matrix $P$ and its inverse $P^{-1}=P^T$.  \n",
        "The final layer is also sign symmetric so we can flip the weights $W_2$ and the sign activation and also get the same result (but this is not a proper invariant).\n",
        "\n",
        "2. To achieve this equivalence, we can require that  $\\forall r,k,j$ the $W_1[r,k,j,:]$ is a symetric tensor relative to the last dimension such that $W_1[r,k,j,i]=W_1[r,k,j,d-i], \\forall i\\in \\left [1,\\frac{d}{2}\\right ]$\n",
        "\n",
        "3. Similarly to section 2, we can require $W_1[r,k,0,i]=W_1[r,k,2,i],W_1[r,k,1,i]=W_1[r,k,3,i]$ for all $r,k,i$.\n",
        "\n",
        "4.  \n",
        "    (a) There is no effect on the classification result itself. The output will be scaled based on the input, so $f_w(X)=c\\cdot f_w(\\tilde{X}))$, but the sign is the same.  \n",
        "    (b) Yes, the training loss is logistic loss, which is not scale invariant, and the gradient of each example is proportional to $$\\frac{\\partial L}{\\partial \\hat{Y}} = \\frac{\\partial \\log(1+e^{-\\hat{Y} Y})}{\\partial \\hat{Y}} = -\\frac{Ye^{-\\hat{Y}Y}}{1+e^{-\\hat{Y}Y}} = -\\frac{Y}{1+e^{\\hat{Y}Y}} = -\\frac{Y}{1+e^{f_w(X)Y}}$$\n",
        "    so having an arbitrary scale may cause the gradient to vanish due to numerical approximation issues.\n",
        "    As an example, if the label is $Y=1$ we can scale $f(X)$ by a big contant $c>0$ and have a vanishing gradient when $f(X)>0$.  \n",
        "    (c) If this property is desired, then we can purposly scale the gradient for all incorrectly labeled examples to increase the gradient, resulting in a situation similar to max-margin classification (we artifically increase the cost of mistakes, thereby increasing the gradient only for incorrectly labeled examples).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68ebTszMOF1R"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 6 -VGG Architecture\n",
        "---\n",
        "\n",
        "1. The VGG-11 CNN architecture consists of 11 convolution (CONV)/fully-connected (FC) layers (every CONV layer has the same padding and stride, every MAXPOOL layer is 2×2 and has padding of 0 and stride 2). Fill in the table. You need to **consider the bias**.\n",
        "\n",
        "\n",
        "* CONV$M$-$N$: a convolutional layer with $N$ neurons, each of size $M \\times M \\times D$, where $D$ is the number of filters. $stride=1, padding=1$ \n",
        "* POOL2: $2 \\times 2$ Max Pooling with $stride=2$\n",
        "    * In case the input of the layer is odd, you should round down. For example, if the output of the layer should be $3.5 \\times 3.5 \\times 3$, you should round to $3 \\times 3 \\times 3$ (i.e., ignore the last column of the input image when performing MaxPooling).\n",
        "* FC-N: a fully connected layer with $N$ neurons.\n",
        "\n",
        "\n",
        "| Layer  | Output Dimension  | Number of Parameters (Weights) |\n",
        "|---|---|---|\n",
        "| INPUT  |  224x224x3 | 0  |\n",
        "|  CONV3-64 | -  | -  | \n",
        "| ReLU |  - | -  |\n",
        "| POOL2|  - | -  |\n",
        "|CONV3-128 | - | -|\n",
        "|ReLU | - | -|\n",
        "| POOL2|  - | -  |\n",
        "|CONV3-256 | - | -|\n",
        "|ReLU | - | -|\n",
        "|CONV3-256 | - | -|\n",
        "|ReLU | - | -|\n",
        "| POOL2|  - | -  |\n",
        "|CONV3-512 | - | -|\n",
        "|ReLU | - | -|\n",
        "|CONV3-512 | - | -|\n",
        "|ReLU | - | -|\n",
        "| POOL2|  - | -  |\n",
        "|CONV3-512 | - | -|\n",
        "|ReLU | - | -|\n",
        "|CONV3-512 | - | -|\n",
        "|ReLU | - | -|\n",
        "| POOL2|  - | -  |\n",
        "| FC-4096|  - | -  |\n",
        "| FC-4096|  - | -  |\n",
        "| FC-1000|  - | -  |\n",
        "| SOFTMAX|  - | -  |\n",
        "\n",
        "2. What is the total number of parameters? (use a calculator for this one)\n",
        "3. What percentage of the weights are found in the fully-connected layers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpLC9zRMOF1T"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/fluency/48/000000/exclamation-mark.png\" style=\"height:50px;display:inline\"> Answer 6 -VGG architecture\n",
        "\n",
        "1. Filled in the table  \n",
        "\n",
        "| Layer  | Output Dimension  | Number of Parameters (Weights) |\n",
        "|---|---|---|\n",
        "| INPUT  |  224x224x3 | 0  |\n",
        "|  CONV3-64 | 224x224x64  | 64x(3x3x3+1)=1792  | \n",
        "| ReLU |  224x224x64 | 0  |\n",
        "| POOL2|  112x112x64 | 0  |\n",
        "|CONV3-128 | 112x112x128 | 128x(3x3x64+1)=73,856  |\n",
        "|ReLU | 112x112x128 | 0|\n",
        "| POOL2|  56x56x128 | 0|\n",
        "|CONV3-256 | 56x56x256 | 256x(3x3x128+1)=295,168|\n",
        "|ReLU | 56x56x256 | 0|\n",
        "|CONV3-256 | 56x56x256 | 256x(3x3x256+1)=590,080|\n",
        "|ReLU | 56x56x256 | 0|\n",
        "| POOL2|  28x28x256 | 0  |\n",
        "|CONV3-512 | 28x28x512 | 512x(3x3x256+1)=1,180,160|\n",
        "|ReLU | 28x28x512 | 0|\n",
        "|CONV3-512 | 28x28x512 | 512x(3x3x512+1)=2,359,808|\n",
        "|ReLU | 28x28x512 | 0|\n",
        "| POOL2|  14x14x512 | 0  |\n",
        "|CONV3-512 | 14x14x512 | 512x(3x3x512+1)=2,359,808|\n",
        "|ReLU | 14x14x512 | 0|\n",
        "|CONV3-512 | 14x14x512 | 512x(3x3x512+1)=2,359,808|\n",
        "|ReLU | 14x14x512 | 0|\n",
        "| POOL2|  7x7x512 | 0  |\n",
        "| FC-4096|  4096 | (7x7x512+1)x4096=102,764,544  |\n",
        "| FC-4096|  4096 | (4096+1)x4096=16,781,312  |\n",
        "| FC-1000|  1000 | (4096+1)x1000=4,097,000  |\n",
        "| SOFTMAX|  1000 | 0  |\n",
        "\n",
        "2. convolution layers weights : 9,220,480  \n",
        "   FC layers weights : 123,642,856  \n",
        "   Total: 132,863,336  \n",
        "3. 100 x 123,642,856 / 132,863,336 = 93.06% of the total weights are in the FC layers' weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D-14iM7pGhm"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/officel/80/000000/code.png\" style=\"height:50px;display:inline\"> Part 2 - Code Assignments\n",
        "---\n",
        "* You must write your code in this notebook and save it with the output of all of the code cells.\n",
        "* Additional text can be added in Markdown cells.\n",
        "* You can use any other IDE you like (PyCharm, VSCode...) to write/debug your code, but for the submission you must copy it to this notebook, run the code and save the notebook with the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMvyKarhOF1Y"
      },
      "source": [
        "#### Tips\n",
        "---\n",
        "1. Uniformly distributed tensors - `torch.Tensor(dim1, dim2, ...,dimN).uniform_(-1, 1)`\n",
        "2. Separation to **validation set** in PyTorch - <a href=\"https://gist.github.com/MattKleinsmith/5226a94bad5dd12ed0b871aed98cb123\">See example here</a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Jj_VZlEOF1Y",
        "outputId": "d1d770cd-d581-413f-ebc2-aa02cc892719"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fe0f5718a90>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# imports for the practice (you can add more if you need)\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "# %matplotlib notebook\n",
        "%matplotlib inline\n",
        "\n",
        "seed = 211\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g6fMD9VOF1c"
      },
      "source": [
        "\n",
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 1 - The Importance of Activation and Initialization\n",
        "---\n",
        "In this task, we are going to use $x \\in \\mathcal{R}^{512}$ and simple neural network that outputs $f(x) \\in \\mathcal{R}^{512}$. The network will have 100 layers with 512 units in each layer.\n",
        "\n",
        "1. We initialize the weights from a unit normal distribution. Run the following code cell and explain what happens. Add a short piece of code that locates when it happens (hint: use `torch.isnan()`). **Print** the layer number.\n",
        "2. We can demonstrate that at a given layer, the matrix product of inputs $x$ and weight matrix $a$ that is initialized from a standard normal distribution will, on average, have a standard deviation very close to the square root of the number of input connections. For our example, with 512 dimensions, show that for 10,000 multiplications of $a$ and $x$, the empirical standard deviation is similar to the square root of the number of input connections. Use the unbiased version: $$ \\hat{std} = \\sqrt{\\frac{\\sum_{i=1}^{10000}\\frac{1}{N}\\sum_{j=1}^N y^2}{10000}}, $$ where $y=ax$ and $N$ is the number of input connections. **Print** the mean, std and the square root of the number of input connections.\n",
        "3. For the code from 1, normalize the weight initialization by the square root of the input connections. How does that change the outcome? **Print** the mean and std after the modification.\n",
        "4. Add a `tanh()` activation after each layer for the code from 1. **Print** the mean and std after the modification. Explain the result.\n",
        "5. Xavier initialization sets a layer’s weights to values chosen from a random uniform distribution that’s bounded between $$\\pm \\sqrt{\\frac{6}{n_i + n_{i+1}}}$$ where $n_i$ is the number of incoming network connections, or “fan-in,” to the layer, and $ n_{i+1}$ is the number of outgoing network connections from that layer, also known as the “fan-out”. Glorot and Bengio believed that Xavier weight initialization would maintain the variance of activations and back-propagated gradients all the way up or down the layers of a network and demonstrated that networks initialized with Xavier achieved substantially quicker convergence and higher accuracy. Implement **Xavier Uniform** as `xavier_init(fan_in, fan_out)`, a function that returns a tensor initialized according to **Xavier Uniform**. Use it on the simple network from 1 with `tanh` activation. **Print** the mean and std after the modification.\n",
        "6. If you try to replace the `tanh` activation with `relu` activation in section 5, you will see very different results. Xavier strives to acheive activation outputs of each layer to have a mean of 0 and a standard deviation around 1, on average. When using a ReLU activation, a single layer will, on average have standard deviation that’s very close to the square root of the number of input connections, **divided by the square root of two** ($\\sqrt{\\frac{512}{2}}$ in our example). **Kaiming He et. al.** proposed an initialization scheme that’s tailored for deep neural nets that use these kinds of asymmetric, non-linear activations. Implement **Kaiming Normal** as `kaiming_init(fan_in, fan_out)`, a function that returns a tensor initialized according to **Kaiming Normal** (use `fan_in` mode). Use it on the simple network from 1 with `relu` activation. **Print** the mean and std after the modification. What happens when you use Xavier with RelU activation?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZL2A1iIQOF1e",
        "outputId": "bd4bb708-1be1-46df-8019-572436f1cb8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(nan) tensor(nan)\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(512)\n",
        "for i in range(100):\n",
        "    a = torch.randn(512, 512)\n",
        "    x = a @ x\n",
        "print(x.mean(), x.std())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G29XbXyoOF1e"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 1 Code and Answers - The Importance of Activation and InitializationYour answers here\n",
        "\n",
        "1. At some point we hit numerical problems as the vector x components explode beyond the CPU max floating point percision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3lQL4qjOF1f",
        "outputId": "e209faec-2621-4f9a-bbf4-008ae499297e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x tensot is all NaN at layer 29\n",
            "tensor(nan) tensor(nan)\n"
          ]
        }
      ],
      "source": [
        "#1.1\n",
        "N=512\n",
        "x = torch.randn(N)\n",
        "for i in range(100):\n",
        "    a = torch.randn(N, N)\n",
        "    x = a @ x\n",
        "    #print(x.numpy())\n",
        "    if torch.isnan(x).all():\n",
        "        print(f\"x tensot is all NaN at layer {i}\")\n",
        "        #print(x.numpy())\n",
        "        break\n",
        "print(x.mean(), x.std())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IqmmQZrOF1f"
      },
      "source": [
        "2. We get that with 10000 calculations of empirical std, the mean (estimator) is about the same as the square root on the input connections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzh8yVu8OF1f",
        "outputId": "8349182e-4bcb-4a15-b8a1-4982af5f34c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean of stds:22.610655394172667\n",
            "std of stds:1.0025907195653077\n",
            "number of input connections square root:22.627416997969522\n"
          ]
        }
      ],
      "source": [
        "#1.2\n",
        "N=512\n",
        "num_of_runs=10000\n",
        "arr_of_stds=np.zeros((num_of_runs,))\n",
        "for i in range(num_of_runs):\n",
        "    x = torch.randn(N)\n",
        "    a = torch.randn(N, N)\n",
        "    y = a @ x\n",
        "    arr_of_stds[i] = torch.sqrt(torch.sum(y**2)/N).numpy()\n",
        "mean_of_stds = np.mean(arr_of_stds)\n",
        "std_of_stds = np.std(arr_of_stds)\n",
        "squar_root_of_N = np.sqrt(N)\n",
        "print(f\"mean of stds:{mean_of_stds}\")\n",
        "print(f\"std of stds:{std_of_stds}\")\n",
        "print(f\"number of input connections square root:{squar_root_of_N}\")\n",
        "                      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6H43XYiOF1g"
      },
      "source": [
        "3. As we saw in the lecture, once we divide each layer by the square root of the layer's input width, we can maintain the input mean and variance $u_{l-1} \\sim \\mathcal{N}(0,I)$ on the output of that layer $u_l$. Thus, the outputs of the NN don't explode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1ezxJzWOF1g",
        "outputId": "09e8f4d8-28db-4acf-f694-c77bd205d3eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x mean:-0.012621086090803146\n",
            "x std:0.7621493339538574\n"
          ]
        }
      ],
      "source": [
        "#1.3\n",
        "N=512\n",
        "x = torch.randn(N)\n",
        "for i in range(100):\n",
        "    a = torch.randn(N, N) / np.sqrt(N)\n",
        "    x = a @ x\n",
        "print(f\"x mean:{x.mean()}\\nx std:{x.std()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVutw1CaOF1h"
      },
      "source": [
        "4. Since tanh activation output is bounded by (-1,1), the output values can't explode and even w/o the normalization we maintain zero mean and variance one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyXIzRT8OF1k",
        "outputId": "82056828-a68a-4940-81e8-cff3403d0956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x mean:-0.03518223389983177\n",
            "x std:0.9775998592376709\n"
          ]
        }
      ],
      "source": [
        "#1.4\n",
        "N=512\n",
        "x = torch.randn(N)\n",
        "for i in range(100):\n",
        "    a = torch.randn(N, N) \n",
        "    x = torch.tanh(a @ x)\n",
        "print(f\"x mean:{x.mean()}\\nx std:{x.std()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihmZB4R1OF1k"
      },
      "source": [
        "5. With Xavier init We can see the mean is closer to 0 compared to previous section w/o it. The variance has changed and got smaller though, which can cause vanishing values at the last layers of the NN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjTXcstZOF1m",
        "outputId": "099c8c76-4a97-4b29-83ab-15e67b28f351"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x mean:-0.00306722242385149\n",
            "x std:0.06797510385513306\n"
          ]
        }
      ],
      "source": [
        "#1.5\n",
        "def xavier_init(fan_in, fan_out):\n",
        "    xavier_coef = np.sqrt(6/(fan_in + fan_out))\n",
        "    a = torch.rand(fan_out, fan_in)\n",
        "    a = xavier_coef * (2 * a - 1) \n",
        "    return a\n",
        "\n",
        "N=512\n",
        "x = torch.randn(N)\n",
        "for i in range(100):\n",
        "    a = xavier_init(N, N) \n",
        "    x = torch.tanh(a @ x)\n",
        "print(f\"x mean:{x.mean()}\\nx std:{x.std()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCgdVwvUOF1m"
      },
      "source": [
        "6. Using Relu activation instead of tanh with Xavier initialization, the mean and std are zeroed almost completely, thus the values at output will be effectively 0.  \n",
        "  With Kaiming He init, as we saw in the dry part Q4, for the Relu activation we can use any symmetric distribution and when we enforce the weights std to be  $\\sqrt{\\frac{2}{d_{l-1}}}$ we preserve the input mean and variance on output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8HI86pVOF1n",
        "outputId": "135d7d16-0abc-4022-d923-181a143edc42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relu with xavier init mean:3.6174410778934776e-16\n",
            "relu with xavier init std:5.593861699600155e-16\n",
            "relu with kaiming init mean:0.5281035900115967\n",
            "relu with kaiming init std:0.8080969452857971\n"
          ]
        }
      ],
      "source": [
        "#1.6\n",
        "N=512\n",
        "x = torch.randn(N)\n",
        "for i in range(100):\n",
        "    a = xavier_init(N, N) \n",
        "    x = torch.relu(a @ x)\n",
        "print(f\"relu with xavier init mean:{x.mean()}\\nrelu with xavier init std:{x.std()}\")\n",
        "\n",
        "def kaiming_init(fan_in, fan_out):\n",
        "    return torch.randn(fan_out, fan_in) * np.sqrt(2/fan_in)\n",
        "\n",
        "N=512\n",
        "x = torch.randn(N)\n",
        "num_of_layers = 100\n",
        "#arr_of_x=np.zeros((N,num_of_runs))\n",
        "for i in range(num_of_layers):\n",
        "    a = kaiming_init(N,N) \n",
        "    x = torch.relu(a @ x)\n",
        "    #arr_of_x[:,i] = x.numpy()\n",
        "\n",
        "print(f\"relu with kaiming init mean:{x.mean()}\\nrelu with kaiming init std:{x.std()}\")\n",
        "#print(f\"relu with kaiming init mean:{arr_of_x.mean()}\\nrelu with kaiming init std:{arr_of_x.std()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlSATRm2OF1n"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 2 - FashionMNIST Deep Classifer\n",
        "---\n",
        "In this task you are going to design and train your first neural network for classification.\n",
        "1. Load the FashionMNIST dataset `torchvision.datasets.FashionMNIST` and display 6 images with their labels from the dataset.\n",
        "2. Design a **MLP** to classify images from the FashionMNIST dataset. **You need to reach at least 85% accuracy on the test set, and 89% for a full grade**.\n",
        "    * You have a free choice of architecture, optimizer, learning scheduler, initialization, regularization and activations.\n",
        "    * In a Markdown block, write down the chosen architectures and all the hyper-parameters.\n",
        "    * **Plot** the loss curves (and any oter statistic you want) as a function of epochs/iterations.\n",
        "    * **Print** the test accuracy.\n",
        "3. Change the initialization of the linear layers and re-train the model. You can pick an initialization of your choosing from : https://pytorch.org/docs/stable/nn.init.html . See example below how to use. **Print** the change in accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vBsEvB25OF1n"
      },
      "outputs": [],
      "source": [
        "# example of weight initialization\n",
        "import torch.nn as nn\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, parmaeters):\n",
        "        super(MyModel, self).__init__()\n",
        "        # model definitions/blocks\n",
        "        # ...\n",
        "        # custom initialization\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                # pick initialzation: https://pytorch.org/docs/stable/nn.init.html\n",
        "                # examples\n",
        "                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                # nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu', a=math.sqrt(5))\n",
        "                # nn.init.normal_(m.weight, 0, 0.005)\n",
        "                # don't forget the bias term (m.bias)\n",
        "                pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ops on x\n",
        "        # ...\n",
        "        # output = f(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 2 Code and Answers - FashionMNIST Deep Classifer"
      ],
      "metadata": {
        "id": "h4mTzjp_CTmS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "MQwBVxNAOF1o",
        "outputId": "dc9416d8-2c18-4e7e-d188-7b33284e46d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running calculations on:  cuda:0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#2.1\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# check if there is gpu avilable, if there is, use it\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.current_device()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"running calculations on: \", device)\n",
        "\n",
        "seed = 211\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "normalize = transforms.Normalize((0.5,), (0.5,))\n",
        "\n",
        "# define transforms\n",
        "valid_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(28, padding=4),\n",
        "    #transforms.RandomAffine(0, translate=(0.7,0.7)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "# load the dataset\n",
        "data_dir = './datasets/'\n",
        "train_dataset = datasets.FashionMNIST(\n",
        "    root=data_dir, train=True, transform=train_transform, download=True)\n",
        "valid_dataset = datasets.FashionMNIST(\n",
        "    root=data_dir, train=True, transform=valid_transform, download=True)\n",
        "test_dataset = datasets.FashionMNIST(\n",
        "    root=data_dir, train=False, transform=valid_transform, download=True)\n",
        "\n",
        "# visualize some images\n",
        "n_samples = 6\n",
        "sample_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=n_samples, shuffle=True)\n",
        "data_iter = iter(sample_loader)\n",
        "images, labels = data_iter.next()\n",
        "X = images.numpy()\n",
        "\n",
        "fig = plt.figure(figsize=(10 ,10))\n",
        "_, axes = plt.subplots(2, 3)\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(X[i, 0, :, :], interpolation='none', cmap='gray')\n",
        "        \n",
        "    xlabel = str(labels[i].numpy())\n",
        "\n",
        "    ax.set_title(\"Ground Truth: {}\".format(xlabel))    \n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_axis_off()\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.2\n",
        "\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "\n",
        "class MyMLP(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(MyMLP, self).__init__()\n",
        "        self.params = copy.deepcopy(params)\n",
        "        self.hidden_layer_sizes = self.params['layers_sizes'][:]\n",
        "        self.num_of_hidden_layers = len(self.hidden_layer_sizes)\n",
        "        assert self.num_of_hidden_layers > 0, \"number of hidden layers should be at least 1\"\n",
        "        self.input_size = self.params['input_size']\n",
        "\n",
        "        self.fc = nn.ModuleList()\n",
        "        prev_layer_size = self.input_size\n",
        "        for layer_index, current_hidden_layer_size in enumerate(self.hidden_layer_sizes):\n",
        "          #print(f\"Adding FC Linear layer {prev_layer_size}x{current_hidden_layer_size}\")\n",
        "          self.fc.append(nn.Linear(prev_layer_size, current_hidden_layer_size))\n",
        "          prev_layer_size = current_hidden_layer_size\n",
        "\n",
        "        if self.params['enable_dropout']:\n",
        "          print(f\"Dropout is enabled with {self.params['dropout']}\")\n",
        "          self.dropout = nn.Dropout(self.params['dropout'])\n",
        "        if self.params['weights_init'] != 'default':\n",
        "          self.init_weights(self.params['weights_init'])\n",
        "\n",
        "    def init_weights(self, init_type):\n",
        "        print('custom init')\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                # pick initialzation: https://pytorch.org/docs/stable/nn.init.html\n",
        "                # examples\n",
        "                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                # nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu', a=math.sqrt(5))\n",
        "                # nn.init.normal_(m.weight, 0, 0.005)\n",
        "                # don't forget the bias term (m.bias)\n",
        "                pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        #prev_layer_size = self.input_size\n",
        "        for layer_index, current_hidden_layer_size in enumerate(self.hidden_layer_sizes):\n",
        "          #print(f\"Apply FC layer {layer_index}: {prev_layer_size}x{current_hidden_layer_size}\")\n",
        "          #prev_layer_size = current_hidden_layer_size\n",
        "          x = self.fc[layer_index](x)\n",
        "          if self.params['enable_batchnorm']:\n",
        "            nn.BatchNorm1d(current_hidden_layer_size)\n",
        "          if (layer_index < (len(self.hidden_layer_sizes)-1)):\n",
        "            if self.params['activation'] == 'relu':\n",
        "              #print(f\"Apply relu activation\")\n",
        "              x = nn.functional.relu(x)\n",
        "            elif self.params['activation'] == 'tanh':\n",
        "              x = torch.tanh(x)\n",
        "            elif self.params['activation'] == 'lrelu':\n",
        "              x = nn.functional.leaky_relu(x)\n",
        "            elif self.params['activation'] == 'elu':\n",
        "              x = nn.functional.elu(x)\n",
        "            elif self.params['activation'] == 'gelu':\n",
        "              x = nn.functional.gelu(x)\n",
        "            else:\n",
        "              pass\n",
        "            if self.params['enable_dropout']:\n",
        "              #print(f\"Apply dropout activation\")\n",
        "              x = self.dropout(x)\n",
        "        #output = nn.functional.softmax(x, dim=1)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "qeDx-BEK4wg5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training\n",
        "\n",
        "seed = 211\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# model arch hyper params\n",
        "model_hyper_params = {}\n",
        "hidden_layer_sizes = [512, 256, 128, 64, 10]\n",
        "input_size = 28*28\n",
        "model_hyper_params['input_size'] = input_size \n",
        "model_hyper_params['layers_sizes'] = hidden_layer_sizes\n",
        "model_hyper_params['activation'] = 'lrelu'\n",
        "model_hyper_params['weights_init'] = 'default'\n",
        "model_hyper_params['enable_dropout'] = True \n",
        "model_hyper_params['dropout'] = 0.2\n",
        "model_hyper_params['enable_batchnorm'] = True\n",
        "\n",
        "\n",
        "#training hyper parameters\n",
        "train_hyper_params = {}\n",
        "train_hyper_params['VALID_SIZE'] = 0.2\n",
        "train_hyper_params['BATCH_SIZE'] = 64\n",
        "train_hyper_params['LEARNING_RATE'] = 0.01\n",
        "train_hyper_params['SCHED_FACTOR'] = 0.5\n",
        "train_hyper_params['SCHED_PAT'] = 1\n",
        "train_hyper_params['NUM_OF_EPOCHS'] = 50\n",
        "train_hyper_params['OPTIMIZER'] = 'sgd'\n",
        "\n",
        "\n",
        "num_train = len(train_dataset)\n",
        "indices = list(range(num_train))\n",
        "split = int(np.floor(train_hyper_params['VALID_SIZE'] * num_train))\n",
        "\n",
        "np.random.shuffle(indices)\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=train_hyper_params['BATCH_SIZE'], sampler=train_sampler)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    valid_dataset, batch_size=train_hyper_params['BATCH_SIZE'], sampler=valid_sampler)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=train_hyper_params['BATCH_SIZE'])\n",
        "\n",
        "# loss criterion\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# model\n",
        "model = MyMLP(model_hyper_params).to(device)\n",
        "#model(images[0].to(device))\n",
        "# optimizer\n",
        "if train_hyper_params['OPTIMIZER'] == 'adam':\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=train_hyper_params['LEARNING_RATE'])\n",
        "elif train_hyper_params['OPTIMIZER'] == 'sgd':\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=train_hyper_params['LEARNING_RATE'], momentum=0.9, nesterov=True, weight_decay=1e-5)\n",
        "else:\n",
        "  assert False, \"optimizer was not defined\"\n",
        "\n",
        "#!rm accuracy_*\n",
        "\n",
        "model.train()\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=train_hyper_params['SCHED_FACTOR'], patience=train_hyper_params['SCHED_PAT'], verbose=True)\n",
        "\n",
        "epochs_train_losses = []\n",
        "epochs_valid_losses = []\n",
        "epochs_test_accuracy = []\n",
        "max_accuracy = 0\n",
        "\n",
        "for epoch in range(train_hyper_params['NUM_OF_EPOCHS']):\n",
        "  print(f'epoch {epoch}')\n",
        "\n",
        "  model.train()\n",
        "  epoch_train_losses = []\n",
        "\n",
        "  for images, labels in train_loader:\n",
        "    images = images.view(-1, input_size).to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # forward pass\n",
        "    predictions = model(images)\n",
        "\n",
        "    # loss\n",
        "    loss = criterion(predictions, labels)\n",
        "\n",
        "    # backward pass\n",
        "    optimizer.zero_grad() # clean the gradients from previous iteration\n",
        "    loss.backward() # autograd backward to calculate gradients\n",
        "    optimizer.step() # apply update to the weights\n",
        "    \n",
        "    epoch_train_losses.append(loss.item())\n",
        "\n",
        "  model.eval()\n",
        "  epoch_valid_losses = []\n",
        "\n",
        "  for images, labels in valid_loader:\n",
        "    images = images.view(-1, input_size).to(device)\n",
        "    labels = labels.to(device)\n",
        "    predictions = model(images)\n",
        "    loss = criterion(predictions, labels)\n",
        "    epoch_valid_losses.append(loss.item())\n",
        "\n",
        "  # Calculate Accuracy\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # Iterate through test dataset\n",
        "  for images, labels in test_loader:\n",
        "    # Send images and labels to device\n",
        "    images = images.view(-1, input_size).to(device)\n",
        "    labels = labels.to(device)\n",
        "    # Forward pass only to get logits/output\n",
        "    outputs = model(images)\n",
        "    # Get predictions from the maximum value\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    predicted = predicted.to(device)\n",
        "    # Total number of labels\n",
        "    total += labels.size(0)\n",
        "    # Total correct predictions\n",
        "    # Without .item(), it is a uint8 tensor which will not work when you pass this number to the scheduler\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "  epochs_train_losses.append(np.mean(epoch_train_losses))\n",
        "  epochs_valid_losses.append(np.mean(epoch_valid_losses))\n",
        "  epochs_test_accuracy.append(accuracy)\n",
        "\n",
        "  # Decay Learning Rate, pass validation accuracy for tracking at every epoch\n",
        "  print(f'epoch: {epoch} train_loss: {epochs_train_losses[-1]:.5f} '\n",
        "    f'validation loss:{epochs_valid_losses[-1]:.5f} test accuracy: {accuracy:.1f}%')\n",
        "\n",
        "  scheduler.step(accuracy) # accuracy is used to track down a plateau\n",
        "\n",
        "  if accuracy > max_accuracy:\n",
        "    checkpoint = {'model_hyper_params': model_hyper_params,\n",
        "                  'train_hyper_params': train_hyper_params,\n",
        "                  'state_dict': model.state_dict()}\n",
        "    filename = f'accuracy_{accuracy}_checkpoint.pth'\n",
        "    torch.save(checkpoint, filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ujQVsA_high0",
        "outputId": "b90984e0-b2b2-4413-d8d7-cabd6c82a4dd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropout is enabled with 0.2\n",
            "epoch 0\n",
            "epoch: 0 train_loss: 1.40618 validation loss:0.79232 test accuracy: 69.3%\n",
            "epoch 1\n",
            "epoch: 1 train_loss: 0.86707 validation loss:0.66531 test accuracy: 74.0%\n",
            "epoch 2\n",
            "epoch: 2 train_loss: 0.79069 validation loss:0.62926 test accuracy: 74.5%\n",
            "epoch 3\n",
            "epoch: 3 train_loss: 0.74294 validation loss:0.65438 test accuracy: 73.0%\n",
            "epoch 4\n",
            "epoch: 4 train_loss: 0.71066 validation loss:0.59383 test accuracy: 77.1%\n",
            "epoch 5\n",
            "epoch: 5 train_loss: 0.69423 validation loss:0.57697 test accuracy: 77.6%\n",
            "epoch 6\n",
            "epoch: 6 train_loss: 0.67377 validation loss:0.55541 test accuracy: 78.1%\n",
            "epoch 7\n",
            "epoch: 7 train_loss: 0.65536 validation loss:0.54300 test accuracy: 78.4%\n",
            "epoch 8\n",
            "epoch: 8 train_loss: 0.63910 validation loss:0.53045 test accuracy: 79.5%\n",
            "epoch 9\n",
            "epoch: 9 train_loss: 0.62870 validation loss:0.51947 test accuracy: 79.2%\n",
            "epoch 10\n",
            "epoch: 10 train_loss: 0.62050 validation loss:0.52180 test accuracy: 79.2%\n",
            "Epoch    11: reducing learning rate of group 0 to 5.0000e-03.\n",
            "epoch 11\n",
            "epoch: 11 train_loss: 0.58322 validation loss:0.49085 test accuracy: 80.8%\n",
            "epoch 12\n",
            "epoch: 12 train_loss: 0.57280 validation loss:0.47736 test accuracy: 81.1%\n",
            "epoch 13\n",
            "epoch: 13 train_loss: 0.56741 validation loss:0.47145 test accuracy: 81.3%\n",
            "epoch 14\n",
            "epoch: 14 train_loss: 0.56408 validation loss:0.46077 test accuracy: 81.9%\n",
            "epoch 15\n",
            "epoch: 15 train_loss: 0.55392 validation loss:0.45881 test accuracy: 81.7%\n",
            "epoch 16\n",
            "epoch: 16 train_loss: 0.55055 validation loss:0.46849 test accuracy: 82.0%\n",
            "epoch 17\n",
            "epoch: 17 train_loss: 0.54984 validation loss:0.48237 test accuracy: 81.0%\n",
            "epoch 18\n",
            "epoch: 18 train_loss: 0.54573 validation loss:0.44963 test accuracy: 82.6%\n",
            "epoch 19\n",
            "epoch: 19 train_loss: 0.53812 validation loss:0.46675 test accuracy: 81.9%\n",
            "epoch 20\n",
            "epoch: 20 train_loss: 0.53790 validation loss:0.45412 test accuracy: 82.3%\n",
            "Epoch    21: reducing learning rate of group 0 to 2.5000e-03.\n",
            "epoch 21\n",
            "epoch: 21 train_loss: 0.52220 validation loss:0.43618 test accuracy: 82.9%\n",
            "epoch 22\n",
            "epoch: 22 train_loss: 0.51554 validation loss:0.42921 test accuracy: 83.0%\n",
            "epoch 23\n",
            "epoch: 23 train_loss: 0.51622 validation loss:0.44211 test accuracy: 82.6%\n",
            "epoch 24\n",
            "epoch: 24 train_loss: 0.50825 validation loss:0.42930 test accuracy: 82.9%\n",
            "Epoch    25: reducing learning rate of group 0 to 1.2500e-03.\n",
            "epoch 25\n",
            "epoch: 25 train_loss: 0.49758 validation loss:0.41908 test accuracy: 83.3%\n",
            "epoch 26\n",
            "epoch: 26 train_loss: 0.49815 validation loss:0.42332 test accuracy: 83.2%\n",
            "epoch 27\n",
            "epoch: 27 train_loss: 0.49398 validation loss:0.42076 test accuracy: 83.2%\n",
            "Epoch    28: reducing learning rate of group 0 to 6.2500e-04.\n",
            "epoch 28\n",
            "epoch: 28 train_loss: 0.49432 validation loss:0.41179 test accuracy: 83.6%\n",
            "epoch 29\n",
            "epoch: 29 train_loss: 0.49308 validation loss:0.41334 test accuracy: 83.5%\n",
            "epoch 30\n",
            "epoch: 30 train_loss: 0.49086 validation loss:0.41220 test accuracy: 83.5%\n",
            "Epoch    31: reducing learning rate of group 0 to 3.1250e-04.\n",
            "epoch 31\n",
            "epoch: 31 train_loss: 0.49018 validation loss:0.41135 test accuracy: 83.6%\n",
            "epoch 32\n",
            "epoch: 32 train_loss: 0.48868 validation loss:0.41052 test accuracy: 83.5%\n",
            "epoch 33\n",
            "epoch: 33 train_loss: 0.48880 validation loss:0.41068 test accuracy: 83.5%\n",
            "Epoch    34: reducing learning rate of group 0 to 1.5625e-04.\n",
            "epoch 34\n",
            "epoch: 34 train_loss: 0.48593 validation loss:0.41099 test accuracy: 83.5%\n",
            "epoch 35\n",
            "epoch: 35 train_loss: 0.48525 validation loss:0.40832 test accuracy: 83.7%\n",
            "epoch 36\n",
            "epoch: 36 train_loss: 0.48630 validation loss:0.40874 test accuracy: 83.6%\n",
            "epoch 37\n",
            "epoch: 37 train_loss: 0.48607 validation loss:0.40867 test accuracy: 83.5%\n",
            "Epoch    38: reducing learning rate of group 0 to 7.8125e-05.\n",
            "epoch 38\n",
            "epoch: 38 train_loss: 0.48546 validation loss:0.40827 test accuracy: 83.6%\n",
            "epoch 39\n",
            "epoch: 39 train_loss: 0.48391 validation loss:0.40837 test accuracy: 83.6%\n",
            "Epoch    40: reducing learning rate of group 0 to 3.9063e-05.\n",
            "epoch 40\n",
            "epoch: 40 train_loss: 0.48363 validation loss:0.40839 test accuracy: 83.6%\n",
            "epoch 41\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-cec81e8c1098>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m   \u001b[0mepoch_train_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \"\"\"\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'std evaluated to zero after conversion to {}, leading to division by zero.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm accuracy_*\n",
        "plt.plot(epochs_train_losses, label='train losses')\n",
        "plt.plot(epochs_valid_losses, label='valid losses')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "\n",
        "# for torchviz on google collab\n",
        "#!pip install torchviz"
      ],
      "metadata": {
        "id": "QYMEGwFCrf1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "729aefdf-36ee-47bc-8d62-488430ca68bd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe0f1db9dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy\n",
        "filename = f'accuracy_{accuracy}_checkpoint.pth'\n",
        "checkpoint = torch.load(filename)\n",
        "print(f\"train_hyper_params {checkpoint['train_hyper_params']}\")\n",
        "print(f\"model_hyper_params {checkpoint['model_hyper_params']}\")\n",
        "model = load_state_dict(checkpoint['state_dict'])\n"
      ],
      "metadata": {
        "id": "nd2yx8f5d5_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAINGxl0OF1o"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 3 - Design a CNN\n",
        "---\n",
        "In this task you are going to design a deep convolutional neural network to classify house number digits from the **The Street View House Numbers (SVHN)** Dataset. \n",
        "\n",
        "SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It can be seen as similar in flavor to MNIST (e.g., the images are of small cropped digits), but incorporates an order of magnitude more labeled data (over 600,000 digit images) and comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images.\n",
        "\n",
        "* 10 classes, 1 for each digit. Digit '0' has label 0, '1' has label 1,...\n",
        "* 73257 digits for training, 26032 digits for testing, and 531131 additional, somewhat less difficult samples, to use as extra training data.\n",
        "\n",
        "<img src=\"http://ufldl.stanford.edu/housenumbers/32x32eg.png\" style=\"height:250px\">\n",
        "\n",
        "1. Load the SVHN dataset with PyTorch using `torchvision.datasets.SVHN(root, split='train', transform=None, target_transform=None, download=True)`, you can read more here: https://pytorch.org/docs/stable/torchvision/datasets.html#svhn. Display 5 images from the train set.\n",
        "2. Design a Convolutional Neural Network (CNN) to classify digits from the images.\n",
        "    * Describe the chosen architecture, how many layers? What activations did you choose? What are the filter sizes? Did you use fully-connected layers (if you did, explain their sizes)?\n",
        "    * What is the input dimension? What is the output dimension?\n",
        "    * Calculate the number of parameters (weights) in the network. **Print** this number.\n",
        "3. Train the classifier (preferably on a GPU - use Colab for this part if you don't have a GPU).\n",
        "    * Describe the the hyper-parameters of the model (batch size, epochs, learning rate....). How did you tune your model? Did you use a validation set to tune the model?\n",
        "    * What is the final accuracy on the test set? **Print** it.\n",
        "        * You need to reach at least 86% accuracy in this section, and 90% for a full grade.\n",
        "    * **Plot** the loss curves (and any other statistic you want) as a function of epochs/iterations.\n",
        "4. For the trained classifier, what is the accuracy on the test set when each test image is added a small noise $a=(0.05, 0.01, 0.005)$: $$ \\text{image} + a \\times \\mathcal{N}(0, 1) $$. **Print** the result for each value of $a$.\n",
        "5. Retrain the classifier, but this time use data augementation of your choosing. Briefly explain what augmentation you chose and how it works. Did the test accuracy improve? **Print** the result.\n",
        "    * You can use transformations available in `torchvision.transforms` as shown in the tutorial.\n",
        "    * You are welcome to use <a href=\"https://kornia.github.io/\">`kornia`</a> for the augmentations.\n",
        "    * **Plot** the loss curves (and any other statistic you want) as a function of epochs/iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAFN-QAXOF1o"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Your Code Here\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyAGHBuqOF1o"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
        "---\n",
        "* Icons made by <a href=\"https://www.flaticon.com/authors/becris\" title=\"Becris\">Becris</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\">www.flaticon.com</a>\n",
        "* Icons from <a href=\"https://icons8.com/\">Icons8.com</a> - https://icons8.com\n",
        "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "name": "ee046211_hw2_034462796_204034953.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}