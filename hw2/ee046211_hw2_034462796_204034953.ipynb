{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzV9wsJ5pGhf"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/bubbles/50/000000/mind-map.png\" style=\"height:50px;display:inline\"> EE 046211 - Technion - Deep Learning\n",
        "---\n",
        "\n",
        "## HW2 - Multilayer NNs and Convolutional NNs\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq2c8X93pGhh"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/clouds/96/000000/keyboard.png\" style=\"height:50px;display:inline\"> Keyboard Shortcuts\n",
        "---\n",
        "* Run current cell: **Ctrl + Enter**\n",
        "* Run current cell and move to the next: **Shift + Enter**\n",
        "* Show lines in a code cell: **Esc + L**\n",
        "* View function documentation: **Shift + Tab** inside the parenthesis or `help(name_of_module)`\n",
        "* New cell below: **Esc + B**\n",
        "* Delete cell: **Esc + D, D** (two D's)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZZybn3NpGhh"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Students Information\n",
        "---\n",
        "* Fill in\n",
        "\n",
        "|Name         |Campus Email                     | ID       |\n",
        "|-------------|---------------------------------|----------|\n",
        "|Lior Friedman| liorf@campus.technion.ac.il     | 204034953|\n",
        "|Yair Nahum   | nahum.yair@campus.technion.ac.il| 034462796|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDK5zqhdpGhi"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
        "---\n",
        "* Maximal garde: 100.\n",
        "* Submission only in **pairs**. \n",
        "    * Please make sure you have registered your group in Moodle (there is a group creation component on the Moodle where you need to create your group and assign members).\n",
        "* **No handwritten submissions.** You can choose whether to answer in a Markdown cell in this notebook or attach a PDF with your answers.\n",
        "* <a style='color:red'> SAVE THE NOTEBOOKS WITH THE OUTPUT, CODE CELLS THAT WERE NOT RUN WILL NOT GET ANY POINTS! </a>\n",
        "* What you have to submit:\n",
        "    * If you have answered the questions in the notebook, you should submit this file only, with the name: `ee046211_hw2_id1_id2.ipynb`.\n",
        "    * If you answered the questions in a different file you should submit a `.zip` file with the name `ee046211_hw2_id1_id2.zip` with content:\n",
        "        * `ee046211_hw2_id1_id2.ipynb` - the code tasks\n",
        "        * `ee046211_hw2_id1_id2.pdf` - answers to questions.\n",
        "    * No other file-types (`.py`, `.docx`...) will be accepted.\n",
        "* Submission on the course website (Moodle).\n",
        "* **Latex in Colab** - in some cases, Latex equations may no be rendered. To avoid this, make sure to not use *bullets* in your answers (\"* some text here with Latex equations\" -> \"some text here with Latex equations\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmSj_UufpGhi"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/dusk/64/000000/online.png\" style=\"height:50px;display:inline\"> Working Online and Locally\n",
        "---\n",
        "* You can choose your working environment:\n",
        "    1. `Jupyter Notebook`, **locally** with <a href=\"https://www.anaconda.com/distribution/\">Anaconda</a> or **online** on <a href=\"https://colab.research.google.com/\">Google Colab</a>\n",
        "        * Colab also supports running code on GPU, so if you don't have one, Colab is the way to go. To enable GPU on Colab, in the menu: `Runtime`$\\rightarrow$ `Change Runtime Type` $\\rightarrow$`GPU`.\n",
        "    2. Python IDE such as <a href=\"https://www.jetbrains.com/pycharm/\">PyCharm</a> or <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a>.\n",
        "        * Both allow editing and running Jupyter Notebooks.\n",
        "\n",
        "* Please refer to `Setting Up the Working Environment.pdf` on the Moodle or our GitHub (https://github.com/taldatech/ee046211-deep-learning) to help you get everything installed.\n",
        "* If you need any technical assistance, please go to our Piazza forum (`hw2` folder) and describe your problem (preferably with images)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlp1Fp4ppGhj"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
        "---\n",
        "\n",
        "* [Part 1 - Theory](#-Part-1---Theory)\n",
        "    * [Q1 - Generalization in A Teacher-Student Setup](#-Question-1--Generalization-in-A-Teacher-Student-Setup)\n",
        "    * [Q2 - Backpropagation By Hand](#-Question-2---Backpropagation-By-Hand)\n",
        "    * [Q3 - Deep Double Descent](#-Question-3---Deep-Double-Descent)\n",
        "    * [Q4 - Initialization](#-Question-4---Initialization)\n",
        "    * [Q5 - MLP and Invaraince](#-Question-5---MLP-and-Invaraince)\n",
        "    * [Q6 - VGG Architecture](#-Question-6--VGG-Architecture)\n",
        "* [Part 2 - Code Assignments](#-Part-2---Code-Assignments)\n",
        "    * [Task 1 - The Importance of Activation and Initialization](#-Task-1---The-Importance-of-Activation-and-Initialization)\n",
        "    * [Task 2 - FashionMNIST Deep Classifer](#-Task-2---FashionMNIST-Deep-Classifer)\n",
        "    * [Task 3 - Design a CNN](#-Task-3---Design-a-CNN)\n",
        "* [Credits](#-Credits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKtSiQX_pGhj"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/cute-clipart/64/000000/ball-point-pen.png\" style=\"height:50px;display:inline\"> Part 1 - Theory\n",
        "---\n",
        "* You can choose whether to answser these straight in the notebook (Markdown + Latex) or use another editor (Word, LyX, Latex, Overleaf...) and submit an additional PDF file, **but no handwritten submissions**.\n",
        "* You can attach additional figures (drawings, graphs,...) in a separate PDF file, just make sure to refer to them in your answers.\n",
        "\n",
        "* $\\large\\LaTeX$ <a href=\"https://kapeli.com/cheat_sheets/LaTeX_Math_Symbols.docset/Contents/Resources/Documents/index\">Cheat-Sheet</a> (to write equations)\n",
        "    * <a href=\"http://tug.ctan.org/info/latex-refsheet/LaTeX_RefSheet.pdf\">Another Cheat-Sheet</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsqSFZG1pGhj"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 1 -Generalization in A Teacher-Student Setup\n",
        "---\n",
        "\n",
        "Recall from lecture 4 the Risk $\\mathcal{R}(w)$: $$ \\mathcal{R}(w) \\triangleq \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) } \\left[ ||w^Tx^{(0)} - w_t^Tx^{(0)}||^2 \\right] $$\n",
        "\n",
        "Prove:\n",
        "\n",
        "$$ \\mathcal{R}(w) = ||w-w_t||^2 $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%%md\n"
        },
        "id": "CjAgwlEAOF1I"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/fluency/48/000000/exclamation-mark.png\" style=\"height:50px;display:inline\">  Answer 1 - Generalization in A Teacher-Student Setup \n",
        "---\n",
        "Proof:  \n",
        "$$ \\mathcal{R}(w) \\triangleq \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) } \\left[ ||w^Tx^{(0)} - w_t^Tx^{(0)}||^2 \\right]$$\n",
        "$$= \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) } \\left[ ||{x^{(0)}}^T(w - w_t)||^2 \\right]$$\n",
        "$$\\underbrace{=}_{(1)} \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) } \\left[ (w - w_t)^T x^{(0)} {x^{(0)}}^T(w - w_t) \\right]$$\n",
        "$$\\underbrace{=}_{(2)} (w - w_t)^T \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) } \\left[  x^{(0)} {x^{(0)}}^T \\right] (w - w_t)$$\n",
        "$$\\underbrace{=}_{(3)} (w - w_t)^T I (w - w_t) \\underbrace{=}_{(4)} ||w - w_t||^2$$\n",
        "Where (1) and (4) are due to the L2 norm definition, (2) is due to $(w-w_t)$ is constant relative to the expectation and (3) is due to the fact that the covariance matrix is $I$.  \n",
        "$\\blacksquare$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StkdA5hUOF1K"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 2 - Backpropagation By Hand\n",
        "---\n",
        "Consider the following network:\n",
        "<img src=\"https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/backprop_by_hand_ex1.png\" style=\"height:300px\">\n",
        "\n",
        "We will work with one sample for this example, but it can be extended to mini-batches.\n",
        "\n",
        "* Input: $x = \\begin{bmatrix} 1 \\\\ 4 \\\\ 5 \\end{bmatrix} \\in \\mathbb{R}^3$\n",
        "* Output (target): $ t = \\begin{bmatrix} 0.1 \\\\ 0.05 \\end{bmatrix} \\in \\mathbb{R}^2 $\n",
        "* Number of Hidden Layers: 1\n",
        "* Activation: Sigmoid for both hidden and output layers\n",
        "* Loss Functions: MSE\n",
        "\n",
        "We initialize the weights and biases to random values as follows:\n",
        "<img src=\"https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/backprop_by_hand_ex2.png\" style=\"height:300px\">\n",
        "\n",
        "1. Perform one forward pass and calculate the MSE.\n",
        "2. Perform backpropagation (one backward pass, i.e., calculate the gradients).\n",
        "3. With a learning rate of $\\alpha = 0.01$, what are the new values of the weights after performing the forward pass and backward pass (assume we use SGD)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJZk_9OxOF1M"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/fluency/48/000000/exclamation-mark.png\" style=\"height:50px;display:inline\"> Answer 2 -Backprop by hand\n",
        "\n",
        "1. All calculations rounded to 2 significant digits\n",
        "\n",
        "$$ h_1(x)=S(0.1\\cdot 1+0.3\\cdot 4+0.5\\cdot 5+0.5\\cdot 1)=S(4.3)=0.99$$\n",
        "$$ h_2(x)=S(0.2\\cdot 1+0.4\\cdot 4+0.6\\cdot 5+0.5\\cdot 1)=S(5.3)=1$$\n",
        "$$ o_1(h(x))=S(0.7\\cdot h_1(x)+0.9\\cdot h_2(x)+0.5\\cdot 1)=S(2.093)=0.89$$\n",
        "$$ o_2(h(x))=S(0.8\\cdot h_1(x)+0.1\\cdot h_2(x)+0.5\\cdot 1)=S(1.392)=0.8$$\n",
        "$$ MSE = (0.1-0.89)^2+(0.05-0.8)^2=1.19$$\n",
        "\n",
        "2. \n",
        "\n",
        "$$\\frac{d\\mathcal{L}}{do_1}=2(o_1-t_1)=1.58, \\frac{d\\mathcal{L}}{do_2}=2(o_2-t_2)=1.5$$\n",
        "\n",
        "$$\\frac{d\\mathcal{L}}{dw_7}=\\frac{d\\mathcal{L}}{do_1}\\frac{do_1}{dw_7}=1.58\\cdot S'(o_1(h(x)))h_1(x)=1.58\\cdot 0.206\\cdot 0.99=0.32$$\n",
        "$$\\frac{d\\mathcal{L}}{dw_8}=\\frac{d\\mathcal{L}}{do_2}\\frac{do_2}{dw_8}=1.5\\cdot S'(o_2(h(x)))h_1(x)=1.5\\cdot 0.214\\cdot 0.99=0.32$$\n",
        "$$\\frac{d\\mathcal{L}}{dw_9}=\\frac{d\\mathcal{L}}{do_1}\\frac{do_1}{dw_9}=1.58\\cdot S'(o_1(h(x)))h_2(x)=1.58\\cdot 0.206\\cdot 1=0.33$$\n",
        "$$\\frac{d\\mathcal{L}}{dw_{10}}=\\frac{d\\mathcal{L}}{do_2}\\frac{do_2}{dw_{10}}=1.5\\cdot S'(o_2(h(x)))h_2(x)=1.5\\cdot 0.214\\cdot 1=0.32$$\n",
        "\n",
        "$$\\frac{d\\mathcal{L}}{dh_1}=\\frac{d\\mathcal{L}}{do_1}\\frac{do_1}{dh_1}+\\frac{d\\mathcal{L}}{do_2}\\frac{do_2}{dh_1}=1.58\\cdot S'(o_1(h(x)))w_7+1.5\\cdot S'(o_2(h(x)))w_8\\\\=1.58\\cdot 0.206\\cdot 0.7+1.5\\cdot 0.214\\cdot 0.8 =0.228+0.257=0.48$$\n",
        "\n",
        "$$\\frac{d\\mathcal{L}}{dh_2}=\\frac{d\\mathcal{L}}{do_1}\\frac{do_1}{dh_2}+\\frac{d\\mathcal{L}}{do_2}\\frac{do_2}{dh_2}=1.58\\cdot S'(o_1(h(x)))w_9+1.5\\cdot S'(o_2(h(x)))w_{10}\\\\=1.58\\cdot 0.206\\cdot 0.9+1.5\\cdot 0.214\\cdot 0.1 =0.293+0.032=0.33$$\n",
        "\n",
        "$$\\frac{d\\mathcal{L}}{dw_1}=\\frac{d\\mathcal{L}}{dh_1}\\frac{dh_1}{dw_1}=0.48\\cdot S'(h_1(x))x_1=0.48\\cdot 0.197\\cdot 1=0.09$$\n",
        "$$\\frac{d\\mathcal{L}}{dw_2}=\\frac{d\\mathcal{L}}{dh_2}\\frac{dh_2}{dw_1}=0.33\\cdot S'(h_2(x))x_1=0.33\\cdot 0.197\\cdot 1=0.07$$\n",
        "$$\\frac{d\\mathcal{L}}{dw_3}=\\frac{d\\mathcal{L}}{dh_1}\\frac{dh_1}{dw_3}=0.48\\cdot S'(h_1(x))x_2=0.48\\cdot 0.197\\cdot 4=0.38$$\n",
        "$$\\frac{d\\mathcal{L}}{dw_4}=\\frac{d\\mathcal{L}}{dh_2}\\frac{dh_2}{dw_4}=0.33\\cdot S'(h_2(x))x_2=0.33\\cdot 0.197\\cdot 4=0.26$$\n",
        "$$\\frac{d\\mathcal{L}}{dw_5}=\\frac{d\\mathcal{L}}{dh_1}\\frac{dh_1}{dw_5}=0.48\\cdot S'(h_1(x))x_3=0.48\\cdot 0.197\\cdot 5=0.47$$\n",
        "$$\\frac{d\\mathcal{L}}{dw_6}=\\frac{d\\mathcal{L}}{dh_2}\\frac{dh_2}{dw_6}=0.33\\cdot S'(h_2(x))x_3=0.33\\cdot 0.197\\cdot 5=0.33$$\n",
        "\n",
        "\n",
        "$$\\nabla_w \\mathcal{L}=\\begin{bmatrix} 0.09 \\\\ 0.07 \\\\ 0.38 \\\\ 0.26 \\\\0.47 \\\\0.33 \\\\0.32 \\\\0.32\\\\0.33 \\\\ 0.32\\end{bmatrix}$$\n",
        "\n",
        "3. \n",
        "$$w_{t+1}=w_t-\\alpha \\nabla_w \\mathcal{L}(w_t)=\\begin{bmatrix} 0.1 \\\\ 0.2 \\\\ 0.3 \\\\ 0.4 \\\\0.5 \\\\0.6 \\\\0.7 \\\\0.8 \\\\0.9 \\\\ 0.1\\end{bmatrix}-0.01\\begin{bmatrix} 0.09 \\\\ 0.07 \\\\ 0.38 \\\\ 0.26 \\\\0.47 \\\\0.33 \\\\0.32 \\\\0.32\\\\0.33 \\\\ 0.32\\end{bmatrix}=\\begin{bmatrix} 0.099 \\\\ 0.199 \\\\ 0.296 \\\\ 0.397 \\\\0.495 \\\\0.597 \\\\0.697 \\\\0.797 \\\\0.897 \\\\ 0.097\\end{bmatrix}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjO6xMRAOF1N"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 3 - Deep Double Descent\n",
        "---\n",
        "\n",
        "For the following plots:\n",
        "1. Where is the critical point (the point of transition between the \"Classical Regime\" and \"Modern Regime\") of the deep double descent?\n",
        "2. What type of double descent is shown? Explain.\n",
        "    \n",
        "\n",
        "a. <img src='https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/double_descent_transformer.PNG' style=\"height:300px\">\n",
        "\n",
        "b. <img src='https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/double_descent_resnet.PNG' style=\"height:300px\">\n",
        "\n",
        "c. <img src='https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/double_descent_intermediate.PNG' style=\"height:300px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJvVLhojOF1O"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/fluency/48/000000/exclamation-mark.png\" style=\"height:50px;display:inline\"> Answer 3 -Deep Double Decent\n",
        "\n",
        "a. \n",
        "The critical point is around dimension=200 for the blue (German-English) model and around dimension=250 for the green (English-French) model. This is an example of model-wise double decent as we see that larger models have worse test error until a critical region is reached and the trend reverses. \n",
        "\n",
        "b. The critical point is around width=10 for the least noisy case and around width=13 for the highest noise.  \n",
        "This is an example of model-wise double decent, and we also see that adding label noise causes the critical region to move further away.  \n",
        "As the noise increases, the SNR and thus the optimal step size decreases. This effectively increases the dimensionallity of the data and moves the interpolation threshold location ($\\gamma=\\frac{d}{n}=1$ requires bigger dimensioanlity of the model parameters).\n",
        "\n",
        "c. Here we see that the large model at around 100 epochs. This is an example of epoch-wise deep double decent, as increasing the training time reverses the overfitting in the test error. The small and intermediate models does not exhibit deep double decent. This effect is noticable in large models.  \n",
        "In intermediate sized models, we see the classical U shape of ML that requires early stopping.  \n",
        "In small sized models, we stay in the biased location, where increasing the training time improves the test error.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU-XkJf5OF1P"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 4 - Initialization\n",
        "---\n",
        "\n",
        "Recall that in lecture 5 we were discussing how to calculate the initialization variance, and reached the conclusion that $$ \\sigma_l =\\frac{1}{\\sqrt{d_{l-1}\\mathbb{E}_{z\\sim \\mathcal{N}(0, 1)} \\left[\\varphi^2(z)\\right]}} $$\n",
        "Show that for ReLU activation ($\\varphi(z) = max(0,z)$), the optimal variance satisfies: $$ \\sigma_l = \\sqrt{\\frac{2}{d_{l-1}}}$$\n",
        "\n",
        "All the notations are the same as in the lecture slides."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoOZF5z4OF1Q"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/fluency/48/000000/exclamation-mark.png\" style=\"height:50px;display:inline\"> Answer 4 -Initialization\n",
        "\n",
        "We assume that $\\forall i, \\mathbb{E}\\left [u_{l-1}[i]\\right ]=0, \\text{Var}(u_{l-1}[i])=1$\n",
        "From the lecture we know that under our assumptions for $W_{l-1}$,\n",
        "\n",
        "$$\\text{Var}(u_l[i])=\\sigma_l^2\\sum_{j=1}^{d_{l-1}}\\mathbb{E}\\left [\\varphi^2 (u_{l-1}[i])\\right ]$$\n",
        "\n",
        "For ReLU, $\\varphi^2(z)=\\begin{cases} z^2 & z>0 \\\\ 0 & \\text{otherwise} \\end{cases}$  \n",
        "Moreover, for Relu we don't need to assume/use the central limit theorem for any symetric PDF $f(u_{l-1}[i]) = f(-u_{l-1}[i])$ for example $u_{l-1} \\sim \\mathcal{N}(0,\\sigma^{2}I)$ or $u_{l-1}[i] \\sim \\mathcal{Uniform}(-1,1)$.\n",
        "\n",
        "$$\\mathbb{E}\\left [\\varphi^2 (u_{l-1}[i])\\right ] = \\int \\varphi^2 (u_{l-1}[i])f(u_{l-1}[i])du_{l-1}[i] = \\int_{0}^{\\infty} u_{l-1}[i]^2 f(u_{l-1}[i]) du_{l-1}[i] \\underbrace{=}_{(1)} \\frac{1}{2}\\int_{-\\infty}^{\\infty} u_{l-1}[i]^2 f(u_{l-1}[i]) du_{l-1}[i]=\\frac{1}{2}\\mathbb{E}\\left [u_{l-1}[i]^2\\right ]\\underbrace{=}_{(2)}\\frac{1}{2}\\text{Var}(u_{l-1}[i])\\underbrace{=}_{(3)}\\frac{1}{2}  $$\n",
        "\n",
        "The transitions are explained as follows:  \n",
        "$(1)$ is due to the fact that the integrand is an even/symetric function.  \n",
        "$(2)$ is due to the fact that $\\mathbb{E}\\left [u_{l-1}[i]\\right ]=0 \\Rightarrow  \\text{Var}(u_{l-1}[i]) = \\mathbb{E}\\left [u_{l-1}[i]^2\\right ] - \\mathbb{E}\\left [u_{l-1}[i]\\right ]^2 = \\mathbb{E}\\left [u_{l-1}[i]^2\\right ]$   \n",
        "And $(3)$ since $\\text{Var}(u_{l-1}[i])=1 (by the induction step over layers and inputs)$   \n",
        "\n",
        "From this we get $$\\text{Var}(u_l[i])=\\sigma_l^2\\sum_{j=1}^{d_{l-1}}\\frac{1}{2}=1$$\n",
        "\n",
        "$$\\frac{1}{2}\\sigma_l^2 d_{l-1}=1$$\n",
        "\n",
        "$$\\Rightarrow \\sigma_l = \\sqrt{\\frac{2}{d_{l-1}}}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eurEw1szOF1Q"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 5 - MLP and Invaraince\n",
        "---\n",
        "\n",
        "You have to design an MLP with the following input: DNA sequences of length $d$. The DNA is a sequence of bases, where each base can be one of 4 options: $(C, T, G, A)$. Thus, the input can be described as the following matrix: $$ X \\in \\mathcal{R}^{4 \\times d}, $$ where $X[j,i]$ denotes the measured value of base concentration of the $j^{th}$ base at location $i$. \n",
        "\n",
        "The network should output a **binary** classification $y \\in \\{-1, 1\\}$ for a specific property we wish to find. The network will be trained on samples $\\{X^{(n)}, y^{(n)} \\}_{n=1}^{N}$, with a **logistic loss function**.\n",
        "\n",
        "First, we will examine a network with 1 hidden layerof size $4 \\times d$ and a **LeakyReLU** activation $\\phi$: $$ f_w(X) = \\sum_{r=1}^{4}\\sum_{k=1}^d W_2[r,k]\\phi\\left(\\sum_{j=1}^{4}\\sum_{i=1}^d W_1[r, k,j, i]X[j, i] \\right),$$ where $w=\\{W_1, W_2\\}$ are the layers of the weight **tensors**. After training is done, the classification will be done with $\\text{sign}(f(X))$.\n",
        "\n",
        "1. Which invariances exist in the network's parameters?\n",
        "2. Now, we notice the fact that: the *direction* in which the DNA is scanned is arbitrary. Thus, if for two inputs $X, \\tilde{X}$: $$ \\forall i,j: \\: X[j,i] = \\tilde{X}[j, d-i], $$ then the two inputs are **equivalent** in their meaning. What constraints should we put on the network's parameters to improve the network's classification performance?\n",
        "3. After that, we now recall that the DNA bases come in pairs, and thus if for two inputs $X, \\tilde{X}$: $$ \\forall i,j : \\: X[j,i] = \\tilde{X}[(j+2)\\text{mod}4,i], $$ then the two inputs are **equivalent** in their meaning. What constraints should we put on the network's parameters to improve the network's classification performance?\n",
        "4. We now notice that the measurement process in noisy, each sample $X^{(n)}$ is in arbitrary scale, and thus if for two $X, \\tilde{X}$: $$ \\forall i,j: \\: X[j,i] = c\\tilde{X}[j,i], $$ for some constant $c>0$, then the two inputs are **equivalent** in their meaning.\n",
        "    * (a) For the given network, that **is already trained**, what is the effect of the scale $c$ on the classification result?\n",
        "    * (b) Can the arbitrary scale hurt the training process? Hint: think what happens to the gradient of each sample.\n",
        "    * (c) How can use this information to improve the classifier performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJb5wlHSOF1R"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/fluency/48/000000/exclamation-mark.png\" style=\"height:50px;display:inline\"> Answer 5 - MLP and Invaraince\n",
        "\n",
        "1. The LeakyReLU activation is rescale symmetric, so there is rescale invariance between $z_1=W_1X$ and $\\phi(z_1)$. Meaning, $\\forall c >0$ if we multiply $W_1$ by $c$ and multiply  $W_2$ by $\\frac{1}{c}$ we get the same function.  \n",
        "The MNN is also invariant to permutations on the nuerons with some permutation matrix $P$ and its inverse $P^{-1}=P^T$.  \n",
        "The final layer is also sign symmetric so we can flip the weights $W_2$ and the sign activation and also get the same result (but this is not a proper invariant).\n",
        "\n",
        "2. To achieve this equivalence, we can require that  $\\forall r,k,j$ the $W_1[r,k,j,:]$ is a symetric tensor relative to the last dimension such that $W_1[r,k,j,i]=W_1[r,k,j,d-i], \\forall i\\in \\left [1,\\frac{d}{2}\\right ]$\n",
        "\n",
        "3. Similarly to section 2, we can require $W_1[r,k,0,i]=W_1[r,k,2,i],W_1[r,k,1,i]=W_1[r,k,3,i]$ for all $r,k,i$.\n",
        "\n",
        "4.  \n",
        "    (a) There is no effect on the classification result itself. The output will be scaled based on the input, so $f_w(X)=c\\cdot f_w(\\tilde{X}))$, but the sign is the same.  \n",
        "    (b) Yes, the training loss is logistic loss, which is not scale invariant, and the gradient of each example is proportional to $$\\frac{\\partial L}{\\partial \\hat{Y}} = \\frac{\\partial \\log(1+e^{-\\hat{Y} Y})}{\\partial \\hat{Y}} = -\\frac{Ye^{-\\hat{Y}Y}}{1+e^{-\\hat{Y}Y}} = -\\frac{Y}{1+e^{\\hat{Y}Y}} = -\\frac{Y}{1+e^{f_w(X)Y}}$$\n",
        "    so having an arbitrary scale may cause the gradient to vanish due to numerical approximation issues.\n",
        "    As an example, if the label is $Y=1$ we can scale $f(X)$ by a big contant $c>0$ and have a vanishing gradient when $f(X)>0$.  \n",
        "    (c) If this property is desired, then we can purposly scale the gradient for all incorrectly labeled examples to increase the gradient, resulting in a situation similar to max-margin classification (we artifically increase the cost of mistakes, thereby increasing the gradient only for incorrectly labeled examples).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68ebTszMOF1R"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 6 -VGG Architecture\n",
        "---\n",
        "\n",
        "1. The VGG-11 CNN architecture consists of 11 convolution (CONV)/fully-connected (FC) layers (every CONV layer has the same padding and stride, every MAXPOOL layer is 2×2 and has padding of 0 and stride 2). Fill in the table. You need to **consider the bias**.\n",
        "\n",
        "\n",
        "* CONV$M$-$N$: a convolutional layer with $N$ neurons, each of size $M \\times M \\times D$, where $D$ is the number of filters. $stride=1, padding=1$ \n",
        "* POOL2: $2 \\times 2$ Max Pooling with $stride=2$\n",
        "    * In case the input of the layer is odd, you should round down. For example, if the output of the layer should be $3.5 \\times 3.5 \\times 3$, you should round to $3 \\times 3 \\times 3$ (i.e., ignore the last column of the input image when performing MaxPooling).\n",
        "* FC-N: a fully connected layer with $N$ neurons.\n",
        "\n",
        "\n",
        "| Layer  | Output Dimension  | Number of Parameters (Weights) |\n",
        "|---|---|---|\n",
        "| INPUT  |  224x224x3 | 0  |\n",
        "|  CONV3-64 | -  | -  | \n",
        "| ReLU |  - | -  |\n",
        "| POOL2|  - | -  |\n",
        "|CONV3-128 | - | -|\n",
        "|ReLU | - | -|\n",
        "| POOL2|  - | -  |\n",
        "|CONV3-256 | - | -|\n",
        "|ReLU | - | -|\n",
        "|CONV3-256 | - | -|\n",
        "|ReLU | - | -|\n",
        "| POOL2|  - | -  |\n",
        "|CONV3-512 | - | -|\n",
        "|ReLU | - | -|\n",
        "|CONV3-512 | - | -|\n",
        "|ReLU | - | -|\n",
        "| POOL2|  - | -  |\n",
        "|CONV3-512 | - | -|\n",
        "|ReLU | - | -|\n",
        "|CONV3-512 | - | -|\n",
        "|ReLU | - | -|\n",
        "| POOL2|  - | -  |\n",
        "| FC-4096|  - | -  |\n",
        "| FC-4096|  - | -  |\n",
        "| FC-1000|  - | -  |\n",
        "| SOFTMAX|  - | -  |\n",
        "\n",
        "2. What is the total number of parameters? (use a calculator for this one)\n",
        "3. What percentage of the weights are found in the fully-connected layers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpLC9zRMOF1T"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/fluency/48/000000/exclamation-mark.png\" style=\"height:50px;display:inline\"> Answer 6 -VGG architecture\n",
        "\n",
        "1. Filled in the table  \n",
        "\n",
        "| Layer  | Output Dimension  | Number of Parameters (Weights) |\n",
        "|---|---|---|\n",
        "| INPUT  |  224x224x3 | 0  |\n",
        "|  CONV3-64 | 224x224x64  | 64x(3x3x3+1)=1792  | \n",
        "| ReLU |  224x224x64 | 0  |\n",
        "| POOL2|  112x112x64 | 0  |\n",
        "|CONV3-128 | 112x112x128 | 128x(3x3x64+1)=73,856  |\n",
        "|ReLU | 112x112x128 | 0|\n",
        "| POOL2|  56x56x128 | 0|\n",
        "|CONV3-256 | 56x56x256 | 256x(3x3x128+1)=295,168|\n",
        "|ReLU | 56x56x256 | 0|\n",
        "|CONV3-256 | 56x56x256 | 256x(3x3x256+1)=590,080|\n",
        "|ReLU | 56x56x256 | 0|\n",
        "| POOL2|  28x28x256 | 0  |\n",
        "|CONV3-512 | 28x28x512 | 512x(3x3x256+1)=1,180,160|\n",
        "|ReLU | 28x28x512 | 0|\n",
        "|CONV3-512 | 28x28x512 | 512x(3x3x512+1)=2,359,808|\n",
        "|ReLU | 28x28x512 | 0|\n",
        "| POOL2|  14x14x512 | 0  |\n",
        "|CONV3-512 | 14x14x512 | 512x(3x3x512+1)=2,359,808|\n",
        "|ReLU | 14x14x512 | 0|\n",
        "|CONV3-512 | 14x14x512 | 512x(3x3x512+1)=2,359,808|\n",
        "|ReLU | 14x14x512 | 0|\n",
        "| POOL2|  7x7x512 | 0  |\n",
        "| FC-4096|  4096 | (7x7x512+1)x4096=102,764,544  |\n",
        "| FC-4096|  4096 | (4096+1)x4096=16,781,312  |\n",
        "| FC-1000|  1000 | (4096+1)x1000=4,097,000  |\n",
        "| SOFTMAX|  1000 | 0  |\n",
        "\n",
        "2. convolution layers weights : 9,220,480  \n",
        "   FC layers weights : 123,642,856  \n",
        "   Total: 132,863,336  \n",
        "3. 100 x 123,642,856 / 132,863,336 = 93.06% of the total weights are in the FC layers' weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D-14iM7pGhm"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/officel/80/000000/code.png\" style=\"height:50px;display:inline\"> Part 2 - Code Assignments\n",
        "---\n",
        "* You must write your code in this notebook and save it with the output of all of the code cells.\n",
        "* Additional text can be added in Markdown cells.\n",
        "* You can use any other IDE you like (PyCharm, VSCode...) to write/debug your code, but for the submission you must copy it to this notebook, run the code and save the notebook with the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMvyKarhOF1Y"
      },
      "source": [
        "#### Tips\n",
        "---\n",
        "1. Uniformly distributed tensors - `torch.Tensor(dim1, dim2, ...,dimN).uniform_(-1, 1)`\n",
        "2. Separation to **validation set** in PyTorch - <a href=\"https://gist.github.com/MattKleinsmith/5226a94bad5dd12ed0b871aed98cb123\">See example here</a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Jj_VZlEOF1Y",
        "outputId": "d1d770cd-d581-413f-ebc2-aa02cc892719"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fe0f5718a90>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# imports for the practice (you can add more if you need)\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "# %matplotlib notebook\n",
        "%matplotlib inline\n",
        "\n",
        "seed = 211\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g6fMD9VOF1c"
      },
      "source": [
        "\n",
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 1 - The Importance of Activation and Initialization\n",
        "---\n",
        "In this task, we are going to use $x \\in \\mathcal{R}^{512}$ and simple neural network that outputs $f(x) \\in \\mathcal{R}^{512}$. The network will have 100 layers with 512 units in each layer.\n",
        "\n",
        "1. We initialize the weights from a unit normal distribution. Run the following code cell and explain what happens. Add a short piece of code that locates when it happens (hint: use `torch.isnan()`). **Print** the layer number.\n",
        "2. We can demonstrate that at a given layer, the matrix product of inputs $x$ and weight matrix $a$ that is initialized from a standard normal distribution will, on average, have a standard deviation very close to the square root of the number of input connections. For our example, with 512 dimensions, show that for 10,000 multiplications of $a$ and $x$, the empirical standard deviation is similar to the square root of the number of input connections. Use the unbiased version: $$ \\hat{std} = \\sqrt{\\frac{\\sum_{i=1}^{10000}\\frac{1}{N}\\sum_{j=1}^N y^2}{10000}}, $$ where $y=ax$ and $N$ is the number of input connections. **Print** the mean, std and the square root of the number of input connections.\n",
        "3. For the code from 1, normalize the weight initialization by the square root of the input connections. How does that change the outcome? **Print** the mean and std after the modification.\n",
        "4. Add a `tanh()` activation after each layer for the code from 1. **Print** the mean and std after the modification. Explain the result.\n",
        "5. Xavier initialization sets a layer’s weights to values chosen from a random uniform distribution that’s bounded between $$\\pm \\sqrt{\\frac{6}{n_i + n_{i+1}}}$$ where $n_i$ is the number of incoming network connections, or “fan-in,” to the layer, and $ n_{i+1}$ is the number of outgoing network connections from that layer, also known as the “fan-out”. Glorot and Bengio believed that Xavier weight initialization would maintain the variance of activations and back-propagated gradients all the way up or down the layers of a network and demonstrated that networks initialized with Xavier achieved substantially quicker convergence and higher accuracy. Implement **Xavier Uniform** as `xavier_init(fan_in, fan_out)`, a function that returns a tensor initialized according to **Xavier Uniform**. Use it on the simple network from 1 with `tanh` activation. **Print** the mean and std after the modification.\n",
        "6. If you try to replace the `tanh` activation with `relu` activation in section 5, you will see very different results. Xavier strives to acheive activation outputs of each layer to have a mean of 0 and a standard deviation around 1, on average. When using a ReLU activation, a single layer will, on average have standard deviation that’s very close to the square root of the number of input connections, **divided by the square root of two** ($\\sqrt{\\frac{512}{2}}$ in our example). **Kaiming He et. al.** proposed an initialization scheme that’s tailored for deep neural nets that use these kinds of asymmetric, non-linear activations. Implement **Kaiming Normal** as `kaiming_init(fan_in, fan_out)`, a function that returns a tensor initialized according to **Kaiming Normal** (use `fan_in` mode). Use it on the simple network from 1 with `relu` activation. **Print** the mean and std after the modification. What happens when you use Xavier with RelU activation?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZL2A1iIQOF1e",
        "outputId": "bd4bb708-1be1-46df-8019-572436f1cb8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(nan) tensor(nan)\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(512)\n",
        "for i in range(100):\n",
        "    a = torch.randn(512, 512)\n",
        "    x = a @ x\n",
        "print(x.mean(), x.std())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G29XbXyoOF1e"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 1 Code and Answers - The Importance of Activation and InitializationYour answers here\n",
        "\n",
        "1. At some point we hit numerical problems as the vector x components explode beyond the CPU max floating point percision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3lQL4qjOF1f",
        "outputId": "e209faec-2621-4f9a-bbf4-008ae499297e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x tensot is all NaN at layer 29\n",
            "tensor(nan) tensor(nan)\n"
          ]
        }
      ],
      "source": [
        "#1.1\n",
        "N=512\n",
        "x = torch.randn(N)\n",
        "for i in range(100):\n",
        "    a = torch.randn(N, N)\n",
        "    x = a @ x\n",
        "    #print(x.numpy())\n",
        "    if torch.isnan(x).all():\n",
        "        print(f\"x tensot is all NaN at layer {i}\")\n",
        "        #print(x.numpy())\n",
        "        break\n",
        "print(x.mean(), x.std())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IqmmQZrOF1f"
      },
      "source": [
        "2. We get that with 10000 calculations of empirical std, the mean (estimator) is about the same as the square root on the input connections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzh8yVu8OF1f",
        "outputId": "8349182e-4bcb-4a15-b8a1-4982af5f34c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean of stds:22.610655394172667\n",
            "std of stds:1.0025907195653077\n",
            "number of input connections square root:22.627416997969522\n"
          ]
        }
      ],
      "source": [
        "#1.2\n",
        "N=512\n",
        "num_of_runs=10000\n",
        "arr_of_stds=np.zeros((num_of_runs,))\n",
        "for i in range(num_of_runs):\n",
        "    x = torch.randn(N)\n",
        "    a = torch.randn(N, N)\n",
        "    y = a @ x\n",
        "    arr_of_stds[i] = torch.sqrt(torch.sum(y**2)/N).numpy()\n",
        "mean_of_stds = np.mean(arr_of_stds)\n",
        "std_of_stds = np.std(arr_of_stds)\n",
        "squar_root_of_N = np.sqrt(N)\n",
        "print(f\"mean of stds:{mean_of_stds}\")\n",
        "print(f\"std of stds:{std_of_stds}\")\n",
        "print(f\"number of input connections square root:{squar_root_of_N}\")\n",
        "                      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6H43XYiOF1g"
      },
      "source": [
        "3. As we saw in the lecture, once we divide each layer by the square root of the layer's input width, we can maintain the input mean and variance $u_{l-1} \\sim \\mathcal{N}(0,I)$ on the output of that layer $u_l$. Thus, the outputs of the NN don't explode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1ezxJzWOF1g",
        "outputId": "09e8f4d8-28db-4acf-f694-c77bd205d3eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x mean:-0.012621086090803146\n",
            "x std:0.7621493339538574\n"
          ]
        }
      ],
      "source": [
        "#1.3\n",
        "N=512\n",
        "x = torch.randn(N)\n",
        "for i in range(100):\n",
        "    a = torch.randn(N, N) / np.sqrt(N)\n",
        "    x = a @ x\n",
        "print(f\"x mean:{x.mean()}\\nx std:{x.std()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVutw1CaOF1h"
      },
      "source": [
        "4. Since tanh activation output is bounded by (-1,1), the output values can't explode and even w/o the normalization we maintain zero mean and variance one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyXIzRT8OF1k",
        "outputId": "82056828-a68a-4940-81e8-cff3403d0956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x mean:-0.03518223389983177\n",
            "x std:0.9775998592376709\n"
          ]
        }
      ],
      "source": [
        "#1.4\n",
        "N=512\n",
        "x = torch.randn(N)\n",
        "for i in range(100):\n",
        "    a = torch.randn(N, N) \n",
        "    x = torch.tanh(a @ x)\n",
        "print(f\"x mean:{x.mean()}\\nx std:{x.std()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihmZB4R1OF1k"
      },
      "source": [
        "5. With Xavier init We can see the mean is closer to 0 compared to previous section w/o it. The variance has changed and got smaller though, which can cause vanishing values at the last layers of the NN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjTXcstZOF1m",
        "outputId": "099c8c76-4a97-4b29-83ab-15e67b28f351"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x mean:-0.00306722242385149\n",
            "x std:0.06797510385513306\n"
          ]
        }
      ],
      "source": [
        "#1.5\n",
        "def xavier_init(fan_in, fan_out):\n",
        "    xavier_coef = np.sqrt(6/(fan_in + fan_out))\n",
        "    a = torch.rand(fan_out, fan_in)\n",
        "    a = xavier_coef * (2 * a - 1) \n",
        "    return a\n",
        "\n",
        "N=512\n",
        "x = torch.randn(N)\n",
        "for i in range(100):\n",
        "    a = xavier_init(N, N) \n",
        "    x = torch.tanh(a @ x)\n",
        "print(f\"x mean:{x.mean()}\\nx std:{x.std()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCgdVwvUOF1m"
      },
      "source": [
        "6. Using Relu activation instead of tanh with Xavier initialization, the mean and std are zeroed almost completely, thus the values at output will be effectively 0.  \n",
        "  With Kaiming He init, as we saw in the dry part Q4, for the Relu activation we can use any symmetric distribution and when we enforce the weights std to be  $\\sqrt{\\frac{2}{d_{l-1}}}$ we preserve the input mean and variance on output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8HI86pVOF1n",
        "outputId": "135d7d16-0abc-4022-d923-181a143edc42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relu with xavier init mean:3.6174410778934776e-16\n",
            "relu with xavier init std:5.593861699600155e-16\n",
            "relu with kaiming init mean:0.5281035900115967\n",
            "relu with kaiming init std:0.8080969452857971\n"
          ]
        }
      ],
      "source": [
        "#1.6\n",
        "N=512\n",
        "x = torch.randn(N)\n",
        "for i in range(100):\n",
        "    a = xavier_init(N, N) \n",
        "    x = torch.relu(a @ x)\n",
        "print(f\"relu with xavier init mean:{x.mean()}\\nrelu with xavier init std:{x.std()}\")\n",
        "\n",
        "def kaiming_init(fan_in, fan_out):\n",
        "    return torch.randn(fan_out, fan_in) * np.sqrt(2/fan_in)\n",
        "\n",
        "N=512\n",
        "x = torch.randn(N)\n",
        "num_of_layers = 100\n",
        "#arr_of_x=np.zeros((N,num_of_runs))\n",
        "for i in range(num_of_layers):\n",
        "    a = kaiming_init(N,N) \n",
        "    x = torch.relu(a @ x)\n",
        "    #arr_of_x[:,i] = x.numpy()\n",
        "\n",
        "print(f\"relu with kaiming init mean:{x.mean()}\\nrelu with kaiming init std:{x.std()}\")\n",
        "#print(f\"relu with kaiming init mean:{arr_of_x.mean()}\\nrelu with kaiming init std:{arr_of_x.std()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlSATRm2OF1n"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 2 - FashionMNIST Deep Classifer\n",
        "---\n",
        "In this task you are going to design and train your first neural network for classification.\n",
        "1. Load the FashionMNIST dataset `torchvision.datasets.FashionMNIST` and display 6 images with their labels from the dataset.\n",
        "2. Design a **MLP** to classify images from the FashionMNIST dataset. **You need to reach at least 85% accuracy on the test set, and 89% for a full grade**.\n",
        "    * You have a free choice of architecture, optimizer, learning scheduler, initialization, regularization and activations.\n",
        "    * In a Markdown block, write down the chosen architectures and all the hyper-parameters.\n",
        "    * **Plot** the loss curves (and any oter statistic you want) as a function of epochs/iterations.\n",
        "    * **Print** the test accuracy.\n",
        "3. Change the initialization of the linear layers and re-train the model. You can pick an initialization of your choosing from : https://pytorch.org/docs/stable/nn.init.html . See example below how to use. **Print** the change in accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vBsEvB25OF1n"
      },
      "outputs": [],
      "source": [
        "# example of weight initialization\n",
        "import torch.nn as nn\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, parmaeters):\n",
        "        super(MyModel, self).__init__()\n",
        "        # model definitions/blocks\n",
        "        # ...\n",
        "        # custom initialization\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                # pick initialzation: https://pytorch.org/docs/stable/nn.init.html\n",
        "                # examples\n",
        "                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                # nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu', a=math.sqrt(5))\n",
        "                # nn.init.normal_(m.weight, 0, 0.005)\n",
        "                # don't forget the bias term (m.bias)\n",
        "                pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ops on x\n",
        "        # ...\n",
        "        # output = f(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 2 Code and Answers - FashionMNIST Deep Classifer"
      ],
      "metadata": {
        "id": "h4mTzjp_CTmS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "MQwBVxNAOF1o",
        "outputId": "ea2e4120-6777-46c9-876f-c201e92732db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running calculations on:  cuda:0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#2.1\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# check if there is gpu avilable, if there is, use it\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.current_device()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"running calculations on: \", device)\n",
        "\n",
        "seed = 211\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "normalize = transforms.Normalize((0.5,), (0.5,))\n",
        "\n",
        "# define transforms\n",
        "valid_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "train_transform = transforms.Compose([\n",
        "    #transforms.RandomCrop(28, padding=4),\n",
        "    #transforms.RandomAffine(0, translate=(0.7,0.7)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "# load the dataset\n",
        "data_dir = './datasets/'\n",
        "train_dataset = datasets.FashionMNIST(\n",
        "    root=data_dir, train=True, transform=train_transform, download=True)\n",
        "valid_dataset = datasets.FashionMNIST(\n",
        "    root=data_dir, train=True, transform=valid_transform, download=True)\n",
        "test_dataset = datasets.FashionMNIST(\n",
        "    root=data_dir, train=False, transform=valid_transform, download=True)\n",
        "\n",
        "# visualize some images\n",
        "n_samples = 6\n",
        "sample_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=n_samples, shuffle=True)\n",
        "data_iter = iter(sample_loader)\n",
        "images, labels = data_iter.next()\n",
        "X = images.numpy()\n",
        "\n",
        "fig = plt.figure(figsize=(10 ,10))\n",
        "_, axes = plt.subplots(2, 3)\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(X[i, 0, :, :], interpolation='none', cmap='gray')\n",
        "        \n",
        "    xlabel = str(labels[i].numpy())\n",
        "\n",
        "    ax.set_title(\"Ground Truth: {}\".format(xlabel))    \n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_axis_off()\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.2\n",
        "\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "\n",
        "class MyMLP(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(MyMLP, self).__init__()\n",
        "        self.params = copy.deepcopy(params)\n",
        "        self.hidden_layer_sizes = self.params['layers_sizes'][:]\n",
        "        self.num_of_hidden_layers = len(self.hidden_layer_sizes)\n",
        "        assert self.num_of_hidden_layers > 0, \"number of hidden layers should be at least 1\"\n",
        "        self.input_size = self.params['input_size']\n",
        "\n",
        "        self.fc = nn.ModuleList()\n",
        "        prev_layer_size = self.input_size\n",
        "        for layer_index, current_hidden_layer_size in enumerate(self.hidden_layer_sizes):\n",
        "          #print(f\"Adding FC Linear layer {prev_layer_size}x{current_hidden_layer_size}\")\n",
        "          self.fc.append(nn.Linear(prev_layer_size, current_hidden_layer_size))\n",
        "          prev_layer_size = current_hidden_layer_size\n",
        "\n",
        "        if self.params['enable_dropout']:\n",
        "          print(f\"Dropout is enabled with {self.params['dropout']}\")\n",
        "          self.dropout = nn.Dropout(self.params['dropout'])\n",
        "        if self.params['weights_init'] != 'default':\n",
        "          self.init_weights(self.params['weights_init'])\n",
        "\n",
        "    def init_weights(self, init_type):\n",
        "        print('custom init')\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                # pick initialzation: https://pytorch.org/docs/stable/nn.init.html\n",
        "                # examples\n",
        "                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                # nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu', a=math.sqrt(5))\n",
        "                # nn.init.normal_(m.weight, 0, 0.005)\n",
        "                # don't forget the bias term (m.bias)\n",
        "                pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        #prev_layer_size = self.input_size\n",
        "        for layer_index, current_hidden_layer_size in enumerate(self.hidden_layer_sizes):\n",
        "          #print(f\"Apply FC layer {layer_index}: {prev_layer_size}x{current_hidden_layer_size}\")\n",
        "          #prev_layer_size = current_hidden_layer_size\n",
        "          x = self.fc[layer_index](x)\n",
        "          if self.params['enable_batchnorm']:\n",
        "            nn.BatchNorm1d(current_hidden_layer_size)\n",
        "          if (layer_index < (len(self.hidden_layer_sizes)-1)):\n",
        "            if self.params['activation'] == 'relu':\n",
        "              #print(f\"Apply relu activation\")\n",
        "              x = nn.functional.relu(x)\n",
        "            elif self.params['activation'] == 'tanh':\n",
        "              x = torch.tanh(x)\n",
        "            elif self.params['activation'] == 'lrelu':\n",
        "              x = nn.functional.leaky_relu(x)\n",
        "            elif self.params['activation'] == 'elu':\n",
        "              x = nn.functional.elu(x)\n",
        "            elif self.params['activation'] == 'gelu':\n",
        "              x = nn.functional.gelu(x)\n",
        "            else:\n",
        "              pass\n",
        "            if self.params['enable_dropout']:\n",
        "              #print(f\"Apply dropout activation\")\n",
        "              x = self.dropout(x)\n",
        "        #output = nn.functional.softmax(x, dim=1)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "qeDx-BEK4wg5"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training\n",
        "\n",
        "seed = 211\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# model arch hyper params\n",
        "model_hyper_params = {}\n",
        "hidden_layer_sizes = [512, 256, 128, 64, 10]\n",
        "input_size = 28*28\n",
        "model_hyper_params['input_size'] = input_size \n",
        "model_hyper_params['layers_sizes'] = hidden_layer_sizes\n",
        "model_hyper_params['activation'] = 'lrelu'\n",
        "model_hyper_params['weights_init'] = 'default'\n",
        "model_hyper_params['enable_dropout'] = True \n",
        "model_hyper_params['dropout'] = 0.2\n",
        "model_hyper_params['enable_batchnorm'] = True\n",
        "\n",
        "\n",
        "#training hyper parameters\n",
        "train_hyper_params = {}\n",
        "train_hyper_params['VALID_SIZE'] = 0.2\n",
        "train_hyper_params['BATCH_SIZE'] = 128\n",
        "train_hyper_params['LEARNING_RATE'] = 0.01\n",
        "train_hyper_params['SCHED_FACTOR'] = 0.5\n",
        "train_hyper_params['SCHED_PAT'] = 2\n",
        "train_hyper_params['NUM_OF_EPOCHS'] = 50\n",
        "train_hyper_params['OPTIMIZER'] = 'sgd'\n",
        "\n",
        "\n",
        "num_train = len(train_dataset)\n",
        "indices = list(range(num_train))\n",
        "split = int(np.floor(train_hyper_params['VALID_SIZE'] * num_train))\n",
        "\n",
        "np.random.shuffle(indices)\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=train_hyper_params['BATCH_SIZE'], sampler=train_sampler)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    valid_dataset, batch_size=train_hyper_params['BATCH_SIZE'], sampler=valid_sampler)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=train_hyper_params['BATCH_SIZE'])\n",
        "\n",
        "# loss criterion\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# model\n",
        "model = MyMLP(model_hyper_params).to(device)\n",
        "#model(images[0].to(device))\n",
        "# optimizer\n",
        "if train_hyper_params['OPTIMIZER'] == 'adam':\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=train_hyper_params['LEARNING_RATE'])\n",
        "elif train_hyper_params['OPTIMIZER'] == 'sgd':\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=train_hyper_params['LEARNING_RATE'], momentum=0.9, nesterov=True, weight_decay=1e-5)\n",
        "else:\n",
        "  assert False, \"optimizer was not defined\"\n",
        "\n",
        "#!rm accuracy_*\n",
        "\n",
        "model.train()\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=train_hyper_params['SCHED_FACTOR'], patience=train_hyper_params['SCHED_PAT'], verbose=True)\n",
        "\n",
        "epochs_train_losses = []\n",
        "epochs_valid_losses = []\n",
        "epochs_test_accuracy = []\n",
        "max_accuracy = 0\n",
        "\n",
        "for epoch in range(train_hyper_params['NUM_OF_EPOCHS']):\n",
        "  print(f'epoch {epoch}')\n",
        "\n",
        "  model.train()\n",
        "  epoch_train_losses = []\n",
        "\n",
        "  for images, labels in train_loader:\n",
        "    images = images.view(-1, input_size).to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # forward pass\n",
        "    predictions = model(images)\n",
        "\n",
        "    # loss\n",
        "    loss = criterion(predictions, labels)\n",
        "\n",
        "    # backward pass\n",
        "    optimizer.zero_grad() # clean the gradients from previous iteration\n",
        "    loss.backward() # autograd backward to calculate gradients\n",
        "    optimizer.step() # apply update to the weights\n",
        "    \n",
        "    epoch_train_losses.append(loss.item())\n",
        "\n",
        "  model.eval()\n",
        "  epoch_valid_losses = []\n",
        "\n",
        "  for images, labels in valid_loader:\n",
        "    images = images.view(-1, input_size).to(device)\n",
        "    labels = labels.to(device)\n",
        "    predictions = model(images)\n",
        "    loss = criterion(predictions, labels)\n",
        "    epoch_valid_losses.append(loss.item())\n",
        "\n",
        "  # Calculate Accuracy\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # Iterate through test dataset\n",
        "  for images, labels in test_loader:\n",
        "    # Send images and labels to device\n",
        "    images = images.view(-1, input_size).to(device)\n",
        "    labels = labels.to(device)\n",
        "    # Forward pass only to get logits/output\n",
        "    outputs = model(images)\n",
        "    # Get predictions from the maximum value\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    predicted = predicted.to(device)\n",
        "    # Total number of labels\n",
        "    total += labels.size(0)\n",
        "    # Total correct predictions\n",
        "    # Without .item(), it is a uint8 tensor which will not work when you pass this number to the scheduler\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "  epochs_train_losses.append(np.mean(epoch_train_losses))\n",
        "  epochs_valid_losses.append(np.mean(epoch_valid_losses))\n",
        "  epochs_test_accuracy.append(accuracy)\n",
        "\n",
        "  # Decay Learning Rate, pass validation accuracy for tracking at every epoch\n",
        "  print(f'epoch: {epoch} train_loss: {epochs_train_losses[-1]:.5f} '\n",
        "    f'validation loss:{epochs_valid_losses[-1]:.5f} test accuracy: {accuracy:.1f}%')\n",
        "\n",
        "  scheduler.step(accuracy) # accuracy is used to track down a plateau\n",
        "\n",
        "  if accuracy > max_accuracy:\n",
        "    checkpoint = {'model_hyper_params': model_hyper_params,\n",
        "                  'train_hyper_params': train_hyper_params,\n",
        "                  'state_dict': model.state_dict()}\n",
        "    filename = f'accuracy_{accuracy}_checkpoint.pth'\n",
        "    torch.save(checkpoint, filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujQVsA_high0",
        "outputId": "e1d1edec-01c2-4453-d85d-607b014460f1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropout is enabled with 0.2\n",
            "epoch 0\n",
            "epoch: 0 train_loss: 1.30146 validation loss:0.66167 test accuracy: 74.0%\n",
            "epoch 1\n",
            "epoch: 1 train_loss: 0.60520 validation loss:0.47130 test accuracy: 81.7%\n",
            "epoch 2\n",
            "epoch: 2 train_loss: 0.50702 validation loss:0.42449 test accuracy: 83.8%\n",
            "epoch 3\n",
            "epoch: 3 train_loss: 0.46678 validation loss:0.39988 test accuracy: 84.8%\n",
            "epoch 4\n",
            "epoch: 4 train_loss: 0.43386 validation loss:0.37753 test accuracy: 85.6%\n",
            "epoch 5\n",
            "epoch: 5 train_loss: 0.41306 validation loss:0.37294 test accuracy: 85.4%\n",
            "epoch 6\n",
            "epoch: 6 train_loss: 0.39553 validation loss:0.35912 test accuracy: 85.7%\n",
            "epoch 7\n",
            "epoch: 7 train_loss: 0.38422 validation loss:0.35864 test accuracy: 86.0%\n",
            "epoch 8\n",
            "epoch: 8 train_loss: 0.37043 validation loss:0.34599 test accuracy: 86.1%\n",
            "epoch 9\n",
            "epoch: 9 train_loss: 0.36003 validation loss:0.33628 test accuracy: 86.8%\n",
            "epoch 10\n",
            "epoch: 10 train_loss: 0.35279 validation loss:0.33739 test accuracy: 86.5%\n",
            "epoch 11\n",
            "epoch: 11 train_loss: 0.34531 validation loss:0.33158 test accuracy: 86.8%\n",
            "epoch 12\n",
            "epoch: 12 train_loss: 0.33821 validation loss:0.32687 test accuracy: 87.2%\n",
            "epoch 13\n",
            "epoch: 13 train_loss: 0.33189 validation loss:0.31894 test accuracy: 87.3%\n",
            "epoch 14\n",
            "epoch: 14 train_loss: 0.32350 validation loss:0.32032 test accuracy: 87.2%\n",
            "epoch 15\n",
            "epoch: 15 train_loss: 0.31732 validation loss:0.32825 test accuracy: 86.8%\n",
            "epoch 16\n",
            "epoch: 16 train_loss: 0.31371 validation loss:0.32324 test accuracy: 87.4%\n",
            "epoch 17\n",
            "epoch: 17 train_loss: 0.30671 validation loss:0.31512 test accuracy: 87.5%\n",
            "epoch 18\n",
            "epoch: 18 train_loss: 0.30311 validation loss:0.31555 test accuracy: 87.5%\n",
            "epoch 19\n",
            "epoch: 19 train_loss: 0.29518 validation loss:0.30576 test accuracy: 87.4%\n",
            "epoch 20\n",
            "epoch: 20 train_loss: 0.28981 validation loss:0.30481 test accuracy: 88.0%\n",
            "epoch 21\n",
            "epoch: 21 train_loss: 0.28884 validation loss:0.30354 test accuracy: 87.6%\n",
            "epoch 22\n",
            "epoch: 22 train_loss: 0.28362 validation loss:0.30027 test accuracy: 88.0%\n",
            "epoch 23\n",
            "epoch: 23 train_loss: 0.27838 validation loss:0.29724 test accuracy: 88.4%\n",
            "epoch 24\n",
            "epoch: 24 train_loss: 0.27674 validation loss:0.29310 test accuracy: 88.5%\n",
            "epoch 25\n",
            "epoch: 25 train_loss: 0.27052 validation loss:0.29528 test accuracy: 88.0%\n",
            "epoch 26\n",
            "epoch: 26 train_loss: 0.26743 validation loss:0.29606 test accuracy: 88.1%\n",
            "epoch 27\n",
            "epoch: 27 train_loss: 0.26215 validation loss:0.29106 test accuracy: 88.5%\n",
            "Epoch    28: reducing learning rate of group 0 to 5.0000e-03.\n",
            "epoch 28\n",
            "epoch: 28 train_loss: 0.23932 validation loss:0.28556 test accuracy: 88.8%\n",
            "epoch 29\n",
            "epoch: 29 train_loss: 0.23316 validation loss:0.28505 test accuracy: 88.8%\n",
            "epoch 30\n",
            "epoch: 30 train_loss: 0.22928 validation loss:0.28034 test accuracy: 89.0%\n",
            "epoch 31\n",
            "epoch: 31 train_loss: 0.22709 validation loss:0.28083 test accuracy: 88.8%\n",
            "epoch 32\n",
            "epoch: 32 train_loss: 0.22331 validation loss:0.28421 test accuracy: 89.0%\n",
            "epoch 33\n",
            "epoch: 33 train_loss: 0.22252 validation loss:0.27836 test accuracy: 88.9%\n",
            "epoch 34\n",
            "epoch: 34 train_loss: 0.21888 validation loss:0.28184 test accuracy: 88.8%\n",
            "epoch 35\n",
            "epoch: 35 train_loss: 0.21665 validation loss:0.27943 test accuracy: 88.9%\n",
            "Epoch    36: reducing learning rate of group 0 to 2.5000e-03.\n",
            "epoch 36\n",
            "epoch: 36 train_loss: 0.20053 validation loss:0.27620 test accuracy: 89.1%\n",
            "epoch 37\n",
            "epoch: 37 train_loss: 0.20007 validation loss:0.27484 test accuracy: 89.2%\n",
            "epoch 38\n",
            "epoch: 38 train_loss: 0.19926 validation loss:0.28013 test accuracy: 89.1%\n",
            "epoch 39\n",
            "epoch: 39 train_loss: 0.19543 validation loss:0.27888 test accuracy: 89.3%\n",
            "epoch 40\n",
            "epoch: 40 train_loss: 0.19214 validation loss:0.27543 test accuracy: 89.4%\n",
            "epoch 41\n",
            "epoch: 41 train_loss: 0.19282 validation loss:0.27542 test accuracy: 89.3%\n",
            "epoch 42\n",
            "epoch: 42 train_loss: 0.18967 validation loss:0.27938 test accuracy: 89.3%\n",
            "epoch 43\n",
            "epoch: 43 train_loss: 0.19064 validation loss:0.27764 test accuracy: 89.5%\n",
            "epoch 44\n",
            "epoch: 44 train_loss: 0.18606 validation loss:0.27672 test accuracy: 89.1%\n",
            "epoch 45\n",
            "epoch: 45 train_loss: 0.18765 validation loss:0.27885 test accuracy: 89.3%\n",
            "epoch 46\n",
            "epoch: 46 train_loss: 0.18463 validation loss:0.28149 test accuracy: 89.2%\n",
            "Epoch    47: reducing learning rate of group 0 to 1.2500e-03.\n",
            "epoch 47\n",
            "epoch: 47 train_loss: 0.17676 validation loss:0.28030 test accuracy: 89.3%\n",
            "epoch 48\n",
            "epoch: 48 train_loss: 0.17541 validation loss:0.27817 test accuracy: 89.7%\n",
            "epoch 49\n",
            "epoch: 49 train_loss: 0.17406 validation loss:0.27886 test accuracy: 89.6%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm accuracy_*\n",
        "plt.plot(epochs_train_losses, label='train losses')\n",
        "plt.plot(epochs_valid_losses, label='valid losses')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "\n",
        "# for torchviz on google collab\n",
        "#!pip install torchviz"
      ],
      "metadata": {
        "id": "QYMEGwFCrf1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "5705da35-68b7-495b-9c1b-61e30646f3bf"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe0defc6b90>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy\n",
        "filename = f'accuracy_{accuracy}_checkpoint.pth'\n",
        "checkpoint = torch.load(filename)\n",
        "print(f\"train_hyper_params {checkpoint['train_hyper_params']}\")\n",
        "print(f\"model_hyper_params {checkpoint['model_hyper_params']}\")\n",
        "model = load_state_dict(checkpoint['state_dict'])\n"
      ],
      "metadata": {
        "id": "nd2yx8f5d5_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAINGxl0OF1o"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 3 - Design a CNN\n",
        "---\n",
        "In this task you are going to design a deep convolutional neural network to classify house number digits from the **The Street View House Numbers (SVHN)** Dataset. \n",
        "\n",
        "SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It can be seen as similar in flavor to MNIST (e.g., the images are of small cropped digits), but incorporates an order of magnitude more labeled data (over 600,000 digit images) and comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images.\n",
        "\n",
        "* 10 classes, 1 for each digit. Digit '0' has label 0, '1' has label 1,...\n",
        "* 73257 digits for training, 26032 digits for testing, and 531131 additional, somewhat less difficult samples, to use as extra training data.\n",
        "\n",
        "<img src=\"http://ufldl.stanford.edu/housenumbers/32x32eg.png\" style=\"height:250px\">\n",
        "\n",
        "1. Load the SVHN dataset with PyTorch using `torchvision.datasets.SVHN(root, split='train', transform=None, target_transform=None, download=True)`, you can read more here: https://pytorch.org/docs/stable/torchvision/datasets.html#svhn. Display 5 images from the train set.\n",
        "2. Design a Convolutional Neural Network (CNN) to classify digits from the images.\n",
        "    * Describe the chosen architecture, how many layers? What activations did you choose? What are the filter sizes? Did you use fully-connected layers (if you did, explain their sizes)?\n",
        "    * What is the input dimension? What is the output dimension?\n",
        "    * Calculate the number of parameters (weights) in the network. **Print** this number.\n",
        "3. Train the classifier (preferably on a GPU - use Colab for this part if you don't have a GPU).\n",
        "    * Describe the the hyper-parameters of the model (batch size, epochs, learning rate....). How did you tune your model? Did you use a validation set to tune the model?\n",
        "    * What is the final accuracy on the test set? **Print** it.\n",
        "        * You need to reach at least 86% accuracy in this section, and 90% for a full grade.\n",
        "    * **Plot** the loss curves (and any other statistic you want) as a function of epochs/iterations.\n",
        "4. For the trained classifier, what is the accuracy on the test set when each test image is added a small noise $a=(0.05, 0.01, 0.005)$: $$ \\text{image} + a \\times \\mathcal{N}(0, 1) $$. **Print** the result for each value of $a$.\n",
        "5. Retrain the classifier, but this time use data augementation of your choosing. Briefly explain what augmentation you chose and how it works. Did the test accuracy improve? **Print** the result.\n",
        "    * You can use transformations available in `torchvision.transforms` as shown in the tutorial.\n",
        "    * You are welcome to use <a href=\"https://kornia.github.io/\">`kornia`</a> for the augmentations.\n",
        "    * **Plot** the loss curves (and any other statistic you want) as a function of epochs/iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAFN-QAXOF1o"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Your Code Here\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyAGHBuqOF1o"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
        "---\n",
        "* Icons made by <a href=\"https://www.flaticon.com/authors/becris\" title=\"Becris\">Becris</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\">www.flaticon.com</a>\n",
        "* Icons from <a href=\"https://icons8.com/\">Icons8.com</a> - https://icons8.com\n",
        "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "name": "ee046211_hw2_034462796_204034953.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}