{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzV9wsJ5pGhf"
   },
   "source": [
    "# <img src=\"https://img.icons8.com/bubbles/50/000000/mind-map.png\" style=\"height:50px;display:inline\"> EE 046211 - Technion - Deep Learning\n",
    "---\n",
    "\n",
    "## HW1 - Optimization and Automatic Differentiation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq2c8X93pGhh"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/clouds/96/000000/keyboard.png\" style=\"height:50px;display:inline\"> Keyboard Shortcuts\n",
    "---\n",
    "* Run current cell: **Ctrl + Enter**\n",
    "* Run current cell and move to the next: **Shift + Enter**\n",
    "* Show lines in a code cell: **Esc + L**\n",
    "* View function documentation: **Shift + Tab** inside the parenthesis or `help(name_of_module)`\n",
    "* New cell below: **Esc + B**\n",
    "* Delete cell: **Esc + D, D** (two D's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZZybn3NpGhh"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Students Information\n",
    "---\n",
    "* Fill in\n",
    "\n",
    "|Name         |Campus Email                     | ID       |\n",
    "|-------------|---------------------------------|----------|\n",
    "|Lior Friedman| liorf@campus.technion.ac.il     | 204034953|\n",
    "|Yair Nahum   | nahum.yair@campus.technion.ac.il| 034462796|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDK5zqhdpGhi"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
    "---\n",
    "* Maximal garde: 100.\n",
    "* Submission only in **pairs**. \n",
    "    * Please make sure you have registered your group in Moodle (there is a group creation component on the Moodle where you need to create your group and assign members).\n",
    "* **No handwritten submissions.** You can choose whether to answer in a Markdown cell in this notebook or attach a PDF with your answers.\n",
    "* <a style='color:red'> SAVE THE NOTEBOOKS WITH THE OUTPUT, CODE CELLS THAT WERE NOT RUN WILL NOT GET ANY POINTS! </a>\n",
    "* What you have to submit:\n",
    "    * If you have answered the questions in the notebook, you should submit this file only, with the name: `ee046211_hw1_id1_id2.ipynb`.\n",
    "    * If you answered the questionss in a different file you should submit a `.zip` file with the name `ee046211_hw1_id1_id2.zip` with content:\n",
    "        * `ee046211_hw1_id1_id2.ipynb` - the code tasks\n",
    "        * `ee046211_hw1_id1_id2.pdf` - answers to questions.\n",
    "    * No other file-types (`.py`, `.docx`...) will be accepted.\n",
    "* Submission on the course website (Moodle).\n",
    "* **Latex in Colab** - in some cases, Latex equations may no be rendered. To avoid this, make sure to not use *bullets* in your answers (\"* some text here with Latex equations\" -> \"some text here with Latex equations\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmSj_UufpGhi"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/online.png\" style=\"height:50px;display:inline\"> Working Online and Locally\n",
    "---\n",
    "* You can choose your working environment:\n",
    "    1. `Jupyter Notebook`, **locally** with <a href=\"https://www.anaconda.com/distribution/\">Anaconda</a> or **online** on <a href=\"https://colab.research.google.com/\">Google Colab</a>\n",
    "        * Colab also supports running code on GPU, so if you don't have one, Colab is the way to go. To enable GPU on Colab, in the menu: `Runtime`$\\rightarrow$ `Change Runtime Type` $\\rightarrow$`GPU`.\n",
    "    2. Python IDE such as <a href=\"https://www.jetbrains.com/pycharm/\">PyCharm</a> or <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a>.\n",
    "        * Both allow editing and running Jupyter Notebooks.\n",
    "\n",
    "* Please refer to `Setting Up the Working Environment.pdf` on the Moodle or our GitHub (https://github.com/taldatech/ee046211-deep-learning) to help you get everything installed.\n",
    "* If you need any technical assistance, please go to our Piazza forum (`hw1` folder) and describe your problem (preferably with images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlp1Fp4ppGhj"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "\n",
    "* [Part 1 - Theory](#-Part-1---Theory)\n",
    "    * [Q1 - Convergence of Gradient Descent](#-Question-1---Convergence-of-Gradient-Descent)\n",
    "    * [Q2 - 1D Quadratic Optimization](#-Question-2---1D-Quadratic-Optimization)\n",
    "    * [Q3 -Optimal Convergence Rate](#-Question-3---Optimal-Convergence-Rate)\n",
    "    * [Q4 - Autodiff 1](#-Question-4----Automatic-Differentiation)\n",
    "    * [Q5 - Autodiff 2](#-Question-5----Automatic-Differentiation-2)\n",
    "* [Part 2 - Code Assignments](#-Part-2---Code-Assignments)\n",
    "    * [Task 1 - The Beale Function](#-Task-1---The-Beale-Function)\n",
    "    * [Task 2 - Building an Optimizer - Nesterov Momentum](#-Task-2---Building-an-Optimizer---Nesterov-Momentum)\n",
    "    * [Task 3 - PyTorch Autograd](#-Task-3---PyTorch-Autograd)\n",
    "    * [Task 4 - Low Rank Matrix Factorization](#-Task-4---Low-Rank-Matrix-Factorization)\n",
    "* [Credits](#-Credits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKtSiQX_pGhj"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/cute-clipart/64/000000/ball-point-pen.png\" style=\"height:50px;display:inline\"> Part 1 - Theory\n",
    "---\n",
    "* You can choose whether to answser these straight in the notebook (Markdown + Latex) or use another editor (Word, LyX, Latex, Overleaf...) and submit an additional PDF file, **but no handwritten submissions**.\n",
    "* You can attach additional figures (drawings, graphs,...) in a separate PDF file, just make sure to refer to them in your answers.\n",
    "\n",
    "* $\\large\\LaTeX$ <a href=\"https://kapeli.com/cheat_sheets/LaTeX_Math_Symbols.docset/Contents/Resources/Documents/index\">Cheat-Sheet</a> (to write equations)\n",
    "    * <a href=\"http://tug.ctan.org/info/latex-refsheet/LaTeX_RefSheet.pdf\">Another Cheat-Sheet</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsqSFZG1pGhj"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 1 - Convergence of Gradient Descent\n",
    "---\n",
    "Recall from the lecture notes:\n",
    "\n",
    "* **Definition**: A function $f$ is $\\beta$-smooth if: $$ \\forall w_1, w_2 \\in \\mathbb{R}^d: ||\\nabla f(w_1) - \\nabla f(w_2)|| \\leq \\beta ||w_1 -w_2|| $$\n",
    "* **Lemma**: If $f$ is $\\beta$-smooth then $$ f(w_1) -f(w_2) -\\nabla f(w_2)^T (w_1-w_2) \\leq \\frac{\\beta}{2} ||w_1-w_2||^2 $$\n",
    "\n",
    "Prove the lemma.\n",
    "\n",
    "Hints:\n",
    "* Represent $f$ as an integral: $f(x) − f(y) = \\int_0^1 \\nabla f(y + t(x-y))^T(x-y) dt $\n",
    "* Make use of Cauchy-Schwarz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/fluency/48/000000/exclamation-mark.png\" style=\"height:50px;display:inline\">  Answer 1 - Convergence of Gradient Descent\n",
    "\n",
    "We subtract from both sides of the equation:  \n",
    "$$f(w_1) − f(w_2) = \\int_0^1 \\nabla f(w_2 + t(w_1-w_2))^T(w_1-w_2) dt $$\n",
    "The following expression:  \n",
    "$$\\nabla f(w_2)^T(w_1-w_2)$$\n",
    "$\\Rightarrow$\n",
    "$$f(w_1) − f(w_2) - \\nabla f(w_2)^T(w_1-w_2)= \\int_0^1 \\nabla f(w_2 + t(w_1-w_2))^T(w_1-w_2) dt - \\nabla f(w_2)^T(w_1-w_2) = \\int_0^1 (\\nabla f(w_2 + t(w_1-w_2)) - \\nabla f(w_2))^T(w_1-w_2) dt = \\int_0^1 < \\nabla f(w_2 + t(w_1-w_2)) - \\nabla f(w_2), w_1-w_2 > dt$$\n",
    "Using the facts that a scalar is less than its absoulute value (first inequality), the trainagle inequality for integrals (second inequality) and  Cauchy-Schwartz inequality (third inequality) we continue:\n",
    "$$\\int_0^1 < \\nabla f(w_2 + t(w_1-w_2)) - \\nabla f(w_2), w_1-w_2 > dt \\leq \\left| \\int_0^1 < \\nabla f(w_2 + t(w_1-w_2)) - \\nabla f(w_2), w_1-w_2 > dt \\right| \\leq \\int_0^1 \\left|< \\nabla f(w_2 + t(w_1-w_2)) - \\nabla f(w_2), w_1-w_2 >\\right| dt \\leq \\int_0^1 \\left\\lVert \\nabla f(w_2 + t(w_1-w_2)) - \\nabla f(w_2)\\right\\lVert \\left\\lVert w_1-w_2 \\right\\lVert dt$$\n",
    "Using the fact that the function is $\\beta$-smooth:\n",
    "$$\\int_0^1 \\left\\lVert \\nabla f(w_2 + t(w_1-w_2)) - \\nabla f(w_2)\\right\\lVert \\left\\lVert w_1-w_2 \\right\\lVert dt \\leq \\int_0^1 \\beta \\left\\lVert t(w_1-w_2)\\right\\lVert \\left\\lVert w_1-w_2 \\right\\lVert dt = \\beta \\left\\lVert w_1-w_2 \\right\\lVert^2  \\int_0^1 tdt = \\frac{\\beta}{2} \\left\\lVert w_1-w_2 \\right\\lVert^2$$\n",
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCb_MNJepGhk"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 2 - 1D Quadratic Optimization\n",
    "---\n",
    "We examine the following model: $$ f(w) = \\frac{1}{2}\\sum_{n=1}^N h_nw^2$$\n",
    "The SGD update with batch size $M=1$: $$ w(t) = w(t-1) -\\eta h_{n(t)}w(t-1) = (1-\\eta h_{n(t)})w(t-1) $$\n",
    "$n(t)$ sampled from $\\{1,...,N\\}$ **with replacement**.\n",
    "\n",
    "We define: $$ h \\triangleq \\mathbb{E}h_{n(t)} = \\frac{1}{N}\\sum_{n=1}^Nh_n $$ $$ \\rho \\triangleq Var(h_{n(t)})= \\frac{1}{N}\\sum_{n=1}^Nh_n^2 -h^2$$\n",
    "\n",
    "\n",
    "Show that:\n",
    "\n",
    "1. $\\mathbb{E}w(t) = (1-\\eta h)\\mathbb{E}w(t-1)$\n",
    "2. $\\mathbb{E}w^2(t) = ((1-\\eta h)^2 +\\eta^2 \\rho)\\mathbb{E}w^2(t-1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/fluency/48/000000/exclamation-mark.png\" style=\"height:50px;display:inline\">  Answer 2 - 1D Quadratic Optimization\n",
    "1. We assume independence between $h_{n(t)}$ and $w(t-1)$ as these are based on different times and the sampling is i.i.d with replacement.  Therefore, we can split expectancy multiplication between these terms.\n",
    "$$\\mathbb{E}[w(t)] = \\mathbb{E}[(1-\\eta h_{n(t)}) w(t-1)] = \\mathbb{E}[1-\\eta h_{n(t)}] \\mathbb{E}[w(t-1)] = (1-\\eta \\mathbb{E}[h_{n(t)}]) \\mathbb{E}[w(t-1)] = (1-\\eta h) \\mathbb{E}[w(t-1)]$$\n",
    "$\\blacksquare$  \n",
    "2.  Again we use the independence between the terms:\n",
    "$$(1)\\space\\space\\space\\mathbb{E}[w^2(t)] = \\mathbb{E}[(1-\\eta h_{n(t)})^2 w^2(t-1)] = \\mathbb{E}[(1-\\eta h_{n(t)})^2] \\mathbb{E}[w^2(t-1)]$$\n",
    "    We use the fact that for any random variable $X$:\n",
    "$$\\mathbb{E}[X^2] = \\mathbb{Var}[X] + \\mathbb{E}[X]^2$$\n",
    "    And calculate $\\mathbb{E}[(1-\\eta h_{n(t)})^2]$ as follows:  \n",
    "$$(2)\\space\\space\\space\\mathbb{E}[(1-\\eta h_{n(t)})^2] = \\mathbb{Var}[(1-\\eta h_{n(t)})] + \\mathbb{E}[1-\\eta h_{n(t)}]^2 = (-\\eta)^2 \\mathbb{Var}[h_{n(t)}] + ((1-\\eta \\mathbb{E}[h_{n(t)}]))^2 = \\eta^2 \\rho + (1-\\eta h)^2$$\n",
    "    We plug (2) into (1) and get the requested equation.  \n",
    "$\\blacksquare$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 3 - Optimal Convergence Rate\n",
    "---\n",
    "This question relates to slide ~26 in the Optimization lecture slides.\n",
    "\n",
    "For an objective function $f(w) = \\frac{1}{2}W^THW$ and $ H=X^TX=U\\Lambda U^T $ where $\\Lambda$ is the eigenvalue matrix with eigenvalues $\\lambda_1 \\leq \\lambda_2 \\leq...\\leq \\lambda_d$.\n",
    "\n",
    "The Gradient Descent step as defined in the lecture: $$ w(t) = w(t-1) -\\eta Hw(t-1). $$\n",
    "\n",
    "For convenience, use $z(0) = U^Tw(0), z(t)=U^Tw(t)$.\n",
    "\n",
    "Show that\n",
    "1. $$ f(w(t)) = \\frac{1}{2}\\sum_{i=1}^d (1-\\eta\\lambda_i)^{2t}\\lambda_iz_i^2(0)$$\n",
    "\n",
    "2. $$ \\mathrm{rate}(\\eta) = \\max (|1-\\eta\\lambda_{min}|, |1-\\eta\\lambda_{max}|) $$ (you can explain in words why it is true).\n",
    "3. $$ \\eta_{\\mathrm{optimal}} = arg\\min_{\\eta}\\mathrm{rate}(\\eta) = \\frac{2}{\\lambda_{max} + \\lambda_{min}} $$\n",
    "4. $$R_{\\mathrm{optimal}} = \\min_{\\eta}rate(\\eta) = \\frac{\\lambda_{max}/ \\lambda_{min} - 1}{\\lambda_{max} / \\lambda_{min} + 1} = \\kappa \\text{(condition number)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/fluency/48/000000/exclamation-mark.png\" style=\"height:50px;display:inline\">  Answer 3 - Optimal Convergence Rate\n",
    "\n",
    "1. We need to show that $$ f(w(t)) = \\frac{1}{2}\\sum_{i=1}^d (1-\\eta\\lambda_i)^{2t}\\lambda_iz_i^2(0)$$\n",
    "   We start by defining the function with $z(t)$:\n",
    "   $$ f(w(t)) = \\frac{1}{2}w^T(t)Hw(t) = \\frac{1}{2}w^T(t)U \\Lambda U^Tw(t) = \\frac{1}{2}z^T(t)U^TU \\Lambda U^TUz(t) = \\frac{1}{2}z^T(t)\\Lambda z(t)$$\n",
    "   Also we have:\n",
    "   $$ z(t) = U^Tw(t) = U^T(w(t-1) - \\eta H w(t-1)) = U^T(w(t-1) - \\eta U \\Lambda U^T w(t-1)) = U^T(U z(t-1) - \\eta U \\Lambda U^T U z(t-1)) = z(t-1) - \\eta \\Lambda z(t-1) = (I - \\eta \\Lambda) z(t-1)$$\n",
    "   Specifically, for each vector component $z_i(t) = (1 - \\eta \\lambda_i)z_i(t-1)$.  \n",
    "   To conclude from the 2 equations before, we get:\n",
    "   $$ f(w(t)) = \\frac{1}{2}z^T(t)\\Lambda z(t) = \\frac{1}{2}z^T(t-1)(I-\\eta \\Lambda) \\Lambda (I-\\eta \\Lambda)z(t-1) = ... = \\frac{1}{2}z^T(0)(I-\\eta \\Lambda)^t \\Lambda (I-\\eta \\Lambda)^tz(0)$$\n",
    "   Since the middle matrices are diagonal, it's clear that the product of these matrices is also diagonal with $\\lambda_i (1-\\eta \\lambda_i)^2$ on its diagonal and we get the requested identity.   \n",
    "   Basically what we did was to rotate the function elipses/eigen vectors (by multiplying with a unitary matrix) to get a simpler coordinates system in which the $z$ features are independent in each other. Then, the solution of the function is rather simple to analyze.  \n",
    "   $\\blacksquare$  \n",
    "2. As a neccessary optimality condition we demand $\\nabla f = 0$. Thus, if we calculate the derivative with respect to t (while treating $z(0)$ as constant) and compare to 0 we get:\n",
    "   $$\\nabla_t f(w(t)) = \\frac{1}{2}\\sum_{i=1}^d 2\\ln{(1-\\eta \\lambda_i)}(1-\\eta\\lambda_i)^{2t}\\lambda_i z_i^2(0)$$\n",
    "   Also, in this specific function the optimal point is when $w(t)=0$ (as $\\nabla f = Hw(t)$ must be 0 at that point).\n",
    "   For this expression to converge to zero as time grows, we need that $(1-\\eta\\lambda_i)^2 \\lt 1, \\forall \\lambda_i$ which is the same as $|1-\\eta\\lambda_i| \\lt 1, \\forall \\lambda_i$.  \n",
    "   The two eigenalues that set the tone of the convergence rate are thus $\\lambda_{min}$ and $\\lambda_{max}$ as for each feature $i$ it will converge if the \"slowest\" converging feature will converge. Meaining, we need to find the slowest feature to converge which is the eigenvalue with the $|1-\\eta\\lambda_i|$ closest to 1. This is why we maximize the expression $|1-\\eta\\lambda_i|$.\n",
    "   Also, we don't just say it is $|1-\\eta\\lambda_{min}|$ as the expression $1-\\eta\\lambda_{min}$ might be negative and $1-\\eta\\lambda_{max}$ will be the slowest to converge (more negative and closer to -1 in its absolute value).  \n",
    "   Thus, it is $\\mathrm{rate}(\\eta) = \\max (|1-\\eta\\lambda_{min}|, |1-\\eta\\lambda_{max}|)$  \n",
    "   BTW, for the GD to converge in a local environemnt of the optimal point, it is neccessary also that the Hessian (in our case H) will have $\\lambda_i > 0, \\forall i$ (that is PD matrix)  \n",
    "   $\\blacksquare$  \n",
    "3. From previous section we have $\\mathrm{rate}(\\eta) = \\max (|1-\\eta\\lambda_{min}|, |1-\\eta\\lambda_{max}|)$.\n",
    "   In case all eigenvalues are equal $\\lambda_{min} = \\lambda_{max}$, it is clear (as in the scalar case) that the minimum rate (0 - in one step of GD) is achieved at $\\eta = \\frac{2}{\\lambda_{max} + \\lambda_{min}} = \\frac{1}{\\lambda_{i}}$  \n",
    "   we need to calculate in case $\\lambda_{min} \\neq \\lambda_{max}$. When $\\eta = \\frac{1}{\\lambda_{min}}$ one expression in the maximization is zeroed and the other at $\\eta = \\frac{1}{\\lambda_{max}}$.  \n",
    "   The $\\eta$ that minimizes the rate should be in between. we now show why it's exactly $\\eta = \\frac{2}{\\lambda_{max} + \\lambda_{min}}$.  \n",
    "   Since $\\lambda_i > 0$, if $\\frac{1}{\\lambda_{max}} \\lt \\eta \\lt \\frac{2}{\\lambda_{max} + \\lambda_{min}} \\Rightarrow |1-\\eta\\lambda_{min}| = 1-\\eta\\lambda_{min} > \\frac{\\lambda_{max} - \\lambda_{min}}{\\lambda_{max} + \\lambda_{min}} = \\eta\\lambda_{max} - 1 = |1-\\eta\\lambda_{max}|$ and thus the $rate(\\eta) = |1-\\eta\\lambda_{min}| = 1-\\eta\\lambda_{min} > \\frac{\\lambda_{max} - \\lambda_{min}}{\\lambda_{max} + \\lambda_{min}}$ (the optimal if we choose $\\eta = \\frac{2}{\\lambda_{max} + \\lambda_{min}}$).  \n",
    "   The same argument stands if we select $\\frac{1}{\\lambda_{min}} \\gt \\eta \\gt \\frac{2}{\\lambda_{max} + \\lambda_{min}}$  \n",
    "   If we select $\\eta \\lt \\frac{1}{\\lambda_{max}}$, the rate will be $rate(\\eta) = |1-\\eta\\lambda_{min}| = 1-\\eta\\lambda_{min} \\gt \\frac{\\lambda_{max} - \\lambda_{min}}{\\lambda_{max}}$ from the same reasons, which is bigger than the optimal we can achieve $\\frac{\\lambda_{max} - \\lambda_{min}}{\\lambda_{max} + \\lambda_{min}}$.\n",
    "   The same argument stands if we select $\\eta \\gt \\frac{1}{\\lambda_{min}}$  \n",
    "   $\\blacksquare$  \n",
    "4. If we set the $\\eta_{optimal} = \\frac{2}{\\lambda_{max} + \\lambda_{min}}$ that we found in the previous section into the rate equation,  we get the optimal rate: $R_{optimal} = \\max(|1-\\eta\\lambda_{min}|,|1-\\eta\\lambda_{max}|) = \\max(1-\\eta\\lambda_{min},\\eta\\lambda_{max}-1)= \\frac{\\lambda_{max} - \\lambda_{min}}{\\lambda_{max} + \\lambda_{min}} = \\frac{\\lambda_{max}/\\lambda_{min}-1}{\\lambda_{max}/\\lambda_{min}+1}$  \n",
    "    $\\blacksquare$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVF9OJWWpGhl"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 4 -  Automatic Differentiation\n",
    "---\n",
    "\n",
    "Consider the scalar function: $$ f = \\exp(\\exp(x) + \\exp(x)^2) +\\sin(\\exp(x) + \\exp(x)^2) $$\n",
    "\n",
    "1. Write down the derivative w.r.t. $x$ explicitly, i.e., $\\frac{d f}{d x}$\n",
    "2. We define the following intermediate variables: $$ a = \\exp(x) $$ $$ b=a^2 $$ $$ c = a+b $$ $$ d=\\exp(c) $$ $$ e=\\sin(c) $$ $$ f=d+e $$ Draw a graph picturing the relationship between all variables (called the **computation graph**).\n",
    "3. Using the graph, write down the derivatives of the individual terms, working backwards to compute the derivative of $f$ (i.e., write down the derivatives $\\frac{df}{dd}, \\frac{df}{de}, ..., \\frac{df}{dx}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rydz9zROpGhl"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 5 -  Automatic Differentiation 2\n",
    "---\n",
    "Write down the chain rule in the dual numbers representation for the following: $$ f(g(h(x + \\epsilon x'))) $$ What is $ \\frac{df(x)}{dx} $?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7D-14iM7pGhm"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/officel/80/000000/code.png\" style=\"height:50px;display:inline\"> Part 2 - Code Assignments\n",
    "---\n",
    "* You must write your code in this notebook and save it with the output of aall of the code cells.\n",
    "* Additional text can be added in Markdown cells.\n",
    "* You can use any other IDE you like (PyCharm, VSCode...) to write/debug your code, but for the submission you must copy it to this notebook, run the code and save the notebook with the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bya0qUGYpGhm"
   },
   "outputs": [],
   "source": [
    "# imports for the practice (you can add more if you need)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import LogNorm\n",
    "from sklearn.datasets import load_iris\n",
    "seed = 211\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "# %matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HC-UE1cNpGhn"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 1 - The Beale Function\n",
    "---\n",
    "The Beale function is defined as follows: $$ f(x, y) = (1.5 - x + xy)^{2} + (2.25 - x + xy^{2})^{2} + (2.625 - x +xy^{3})^{2}$$\n",
    "\n",
    "1. What is the global minima of this function?\n",
    "2. Implement the Beale function: `beale_f(x,y)`.\n",
    "3. Implement a function, `beale_grads(x,y)` that returns the gradients of the Beale function.\n",
    "4. 3D plot the Beale function wit the global minima you found. Use Matplotlib's `ax.plot_surface(x_mesh, y_mesh, z, norm=LogNorm(), rstride=1, cstride=1, edgecolor='none', alpha=.8, cmap=plt.cm.jet)` for the function, and `ax.plot(x, y, f(x, y), 'r*', markersize=20)` for the minima.\n",
    "4. 2D plot the contours with `ax.contour(x_mesh, y_mesh, z, levels=np.logspace(-.5, 5, 35), norm=LogNorm(), cmap=plt.cm.jet)` and the minima with `ax.plot(x, y, 'r*', markersize=20)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsHtDWllpGhn"
   },
   "source": [
    "Your Answers Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukJi9-rapGhn"
   },
   "outputs": [],
   "source": [
    "# Set the manually calculated minima\n",
    "min_x = None\n",
    "min_y = None\n",
    "\n",
    "def beale_f(x, y):\n",
    "    value = None\n",
    "    \"\"\"\n",
    "    Your Code Here\n",
    "    \"\"\"\n",
    "    return value\n",
    "\n",
    "def beale_grads(x, y):\n",
    "    dx, dy = None, None\n",
    "    \"\"\"\n",
    "    Your Code Here\n",
    "    \"\"\"\n",
    "\n",
    "    grads = np.array([dx, dy])\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IepBb4_9pGho"
   },
   "outputs": [],
   "source": [
    "minima = np.array([min_x, min_y])\n",
    "beale_res = beale_f(*minima)\n",
    "grads_res = beale_grads(*minima)\n",
    "print(f\"minima (1x2 row vector shape): {minima}\")\n",
    "print(f\"beale_f output: {beale_res}\")\n",
    "print(f\"beale_grad output: {grads_res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZmHZe-9pGhp"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 2 - Building an Optimizer - Nesterov Momentum\n",
    "---\n",
    "In this task, you are going to implement the Nesterov Momentum optimizer. We are giving the skeleton of the code and the description of the methods, and you need to implement the optimizer.\n",
    "\n",
    "Recall the Nesterov Momentum update rule:\n",
    "$$ z^{k+1} = \\beta z^k -\\alpha \\nabla f(w^k +\\beta z^k) $$ $$ w^{k+1} = w^k + z^{k+1} $$\n",
    "\n",
    "1. Implement `class NesterovMomentumOptimizer()`. \n",
    "    * `function` is the Python function you want to optimize.\n",
    "    * `gradients` is the Python function that returns the gradients of `function`.\n",
    "    * `x_init` and `y_init` are the initialization points for the optimizer.\n",
    "    * Save the `path` of the optimizer (the minima points the optimizer visits during the optimization).\n",
    "    * Stopping criterion: change in minima `<1e-7`.\n",
    "    * **You can change the class however you wish, you can remove/add varaibles and methods as you wish**\n",
    "2. For ` x_init=0.7, y_init=1.4, learning_rate=0.01, momentum=0.9`, optimize the Beale function. Plot the results **with the path taken** (better do it on the 2D contour plot).\n",
    "3. Choose different initialization and learning rate and show the results as in 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1hBmFg3pGhp"
   },
   "outputs": [],
   "source": [
    "class NesterovMomentumOptimizer():\n",
    "    def __init__(self, function, gradients, x_init=None, y_init=None, learning_rate=0.01, momentum=0.9):\n",
    "        self.f = function\n",
    "        self.g = gradients\n",
    "        scale = 3.0\n",
    "        self.path = None\n",
    "        self.current_val = np.zeros([2])\n",
    "        if x_init is not None:\n",
    "            self.current_val[0] = x_init\n",
    "        else:\n",
    "            self.current_val[0] = np.random.uniform(low=-scale, high=scale)\n",
    "            print(\"x_init: {:.3f}\".format(self.curernt_val[0]))\n",
    "        if y_init is not None:\n",
    "            self.current_val[1] = y_init\n",
    "        else:\n",
    "            self.current_val[1] = np.random.uniform(low=-scale, high=scale)\n",
    "        print(\"x_init: {:.3f}\".format(self.current_val[0]))\n",
    "        print(\"y_init: {:.3f}\".format(self.current_val[1]))\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.velocity = np.zeros([2])\n",
    "    \n",
    "        # for accumulation of loss and path\n",
    "        self.z_history = []\n",
    "        self.x_history = []\n",
    "        self.y_history = []\n",
    "  \n",
    "  \n",
    "    def func(self, variables):\n",
    "        \"\"\"Beale function.\n",
    "\n",
    "        Args:\n",
    "          variables: input data, shape: 1-rank Tensor (vector) np.array\n",
    "            x: x-dimension of inputs\n",
    "            y: y-dimension of inputs\n",
    "\n",
    "        Returns:\n",
    "          z: Beale function value at (x, y)\n",
    "        \"\"\"\n",
    "        \n",
    "  \n",
    "    def gradients(self, variables):\n",
    "        \"\"\"Gradient of Beale function.\n",
    "\n",
    "        Args:\n",
    "          variables: input data, shape: 1-rank Tensor (vector) np.array\n",
    "            x: x-dimension of inputs\n",
    "            y: y-dimension of inputs\n",
    "\n",
    "        Returns:\n",
    "          grads: [dx, dy], shape: 1-rank Tensor (vector) np.array\n",
    "            dx: gradient of Beale function with respect to x-dimension of inputs\n",
    "            dy: gradient of Beale function with respect to y-dimension of inputs\n",
    "        \"\"\"\n",
    "       \n",
    "      \n",
    "    def weights_update(self, grads):\n",
    "        \"\"\"Weights update using Nesterov Momentum.\n",
    "\n",
    "          v' = gamma * v - lr * grads\n",
    "          w' = w + v\n",
    "        \"\"\"\n",
    "        \n",
    "    \n",
    "    def history_update(self, z, x, y):\n",
    "        \"\"\"Accumulate all interesting variables, z = function(x,y)\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    def train(self, max_steps):\n",
    "        \"\"\"\n",
    "        Optimize the function using Nesterov Momentum\n",
    "        \"\"\"\n",
    "\n",
    "    @property\n",
    "    def x(self):\n",
    "        return self.current_val[0]\n",
    "  \n",
    "    @property\n",
    "    def y(self):\n",
    "        return self.current_val[1]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mKouk-J0pGhq"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your Code Here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGPdFcsSpGhq"
   },
   "outputs": [],
   "source": [
    "opt = NesterovMomentumOptimizer(beale_f, beale_grads, x_init=0.7, y_init=1.4, learning_rate=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxq23lLJpGhq"
   },
   "outputs": [],
   "source": [
    "%time\n",
    "opt.train(1000)\n",
    "print(\"Global minima\")\n",
    "print(\"x*: {:.2f}  y*: {:.2f}\".format(minima[0], minima[1]))\n",
    "print(\"Solution using the gradient descent\")\n",
    "print(\"x: {:.4f}  y: {:.4f}\".format(opt.x, opt.y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGSxoUKppGhs"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 3 - PyTorch Autograd\n",
    "---\n",
    "For the function from the theory practice: $$ f = \\exp(\\exp(x) + \\exp(x)^2) +\\sin(\\exp(x) + \\exp(x)^2)  $$\n",
    "\n",
    "1. Implement it and its dervative (explicitly) using `torch`.\n",
    "2. Define a scalar tensor `x` and use `autograd` to calculate the derivative w.r.t $x$. Does the result correspond to the output of the function the calculates the derivative explicitly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qopWh-QupGhs"
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    f_val = None\n",
    "    \"\"\"\n",
    "    Your Code Here\n",
    "    \"\"\"\n",
    "    return f_val\n",
    "\n",
    "def derv_f(x):\n",
    "    derv_val = None\n",
    "    \"\"\"\n",
    "    Your Code Here\n",
    "    \"\"\"\n",
    "    return derv_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNO19wRspGhs"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(0.5, requires_grad=True)\n",
    "print(x)\n",
    "f_res = f(x)\n",
    "f_manual_grad = derv_f(x.detach()) \n",
    "\n",
    "\"\"\"\n",
    "Your Code Here\n",
    "\"\"\"\n",
    "# Calculate with torch autograd\n",
    "f_autograd = None\n",
    "\n",
    "\n",
    "print(f_manual_grad)\n",
    "print(f_autograd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jrof_SsJpGht"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 4 - Low Rank Matrix Factorization \n",
    "---\n",
    "Consider the following optimization problem: $$ \\min_{\\hat{U}, \\hat{V}}||A - \\hat{U}\\hat{V}||_F^{2} $$ Where $A \\in \\mathcal{R}^{m \\times n},\\hat{U} \\in \\mathcal{R}^{m \\times r}, \\hat{V} \\in \\mathcal{R}^{r \\times n} $ and $r < min(m,n)$ ($r$ is the rank of the matrix). $||\\cdot||_F^2$ denotes the Frobenius norm.\n",
    "\n",
    "1. Implement a function, `gd_factorize_ad(A, rank, num_epochs=1000, lr=0.01)`, that given a 2D tensor `A` and a `rank`, will calculate the low-rank factorization of `A` using **gradient decsent**. Compute and apply all the gradients of $\\hat{U}$ and of $\\hat{V}$ once per epoch. $\\hat{U}$ and $\\hat{V}$ should be initially created with uniform random values. Use PyTorch's `autograd` for the gradients.\n",
    "    * To compute the squared Frobenius norm loss (reconstruction loss), use `torch.nn.functional.mse loss with reduction=’sum’`.\n",
    "\n",
    "2. Use the provided `data` of the Iris dataset of 150 instances and 4 features. Apply `gd_factorize_ad` to compute the 2-rank matrix factorization of `data`. What is the reconstruction loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCFYO3QBpGht"
   },
   "outputs": [],
   "source": [
    "df = load_iris(as_frame=True).data # option 1\n",
    "# df = pd.read_csv('./iris.data', header=None) # option 2\n",
    "data = torch.tensor(df.iloc[:, [0, 1, 2, 3]].values)\n",
    "data = data - data.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sLIgji02pGht"
   },
   "outputs": [],
   "source": [
    "def gd_factorize_ad(A, rank, num_epochs=1000, lr=0.01):\n",
    "    # initialize\n",
    "    U = None\n",
    "    V = None\n",
    "    \n",
    "    \"\"\"\n",
    "    Your Code Here\n",
    "    \"\"\"\n",
    "    \n",
    "    # implement gradient descent\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        \"\"\"\n",
    "        Your Code Here\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if epoch % 5 == 0:\n",
    "            print(f'epoch: {epoch}, loss: {loss}')\n",
    "    return U, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmImbLvIpGht"
   },
   "outputs": [],
   "source": [
    "U, V = gd_factorize_ad(data.float(), rank=2, num_epochs=1000, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgZUKV2NpGhu"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "---\n",
    "* Icons made by <a href=\"https://www.flaticon.com/authors/becris\" title=\"Becris\">Becris</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\">www.flaticon.com</a>\n",
    "* Icons from <a href=\"https://icons8.com/\">Icons8.com</a> - https://icons8.com\n",
    "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ee046211_hw1_optimization_autograd_sol.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
